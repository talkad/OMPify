{"code": "for (i = 0; i < 64; i++) \n    { \n      z = bar (0, &x, 0); \n      r += bar (1, y, 0); \n    }", "pragma": "target teams distribute parallel for reduction(+:r) private(x y z)", "hash": "7cfb1d9581438da6bc8c59f6f9716fe708f603f01142706ec88616a7e5870250"}
{"code": "for (i = 0; i < 100; i++) \n    { \n      long long c = 0; \n      const long long d[] = { 1, 3, 5, 7, 9 }; \n      for (int j = 4; j >= 0; j--) \n         c = d[j] + b[i] * c; \n      a[i] += c; \n    }", "pragma": "target teams distribute parallel for simd ", "hash": "34f7622fb862d789168573040e776bff886f1b8cc38582be6bbca6843573bd79"}
{"code": "for(i=0; i<10; i++) CHECK(\"array test before mapped\", i, A[i], i);", "pragma": "target ", "hash": "76620010fd4f5544b101d107125a60ed2d8a6e609bffba0f94383de4929b2368"}
{"code": "for (int i = 0; i < n; ++i) { \n      if (a_h[i] != a[i]) { \n\tprintf(\"Error at n = %d, i = %d: host = %lf, device = %lf\\n\", n, i, a_h[i], a[i]); \n\treturn 1; \n      } \n    }", "pragma": "target ", "hash": "3206c6a1548e6bf089a6b8b776b9102a657300a9df960fbc52261613ff7aec8e"}
{"code": "for (int i = 0; i < ALLOC_SIZE; i++) \n    { \n      int expected = (BASE <= i && i < BASE + N) ? 1 : 0; \n      if (a[i] == expected) \n\tcontinue; \n \n      printf (\"Expected %d, got %d at base[%d]\\n\", expected, a[i], i - BASE); \n      abort (); \n    }", "pragma": "target ", "hash": "ae78420f620e4b17b5e0890a1ad14fca3256cffaab1bf8a5f45a373d97a95786"}
{"code": "for (int i = 0; i < 2; ++i) { \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n \n     \n \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n \n    sivar += i; \n \n    [&]() { \n       \n \n       \n \n \n      sivar += 4; \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n    }(); \n  }", "pragma": "teams distribute parallel for simd reduction(+: sivar) ", "hash": "005f174d86b61d94be3bf4cefcb0fea0155999d5576d7be9c7cdf2cbbf726793"}
{"code": "for (l = num_layers-2; l > 0; l--){ \n    delta = ((layers[l+1].w).transpose() * delta).cwiseProduct((layers[l].z).unaryExpr(phiprime));  \n \n \n    layers[l].w -= eta * (delta * (layers[l-1].a).transpose());   \n \n    layers[l].b -= eta * delta;  \n \n \n    if (dropout[l] != 1){ \n      layers[l].w -= (eta/lamda) * (layers[l].w).unaryExpr(regularization);  \n \n      layers[l].b -= (eta/lamda) * (layers[l].b).unaryExpr(regularization); \n    } \n  }", "pragma": "target teams distribute ", "hash": "01bfb5d3216ead86a139a25f23c24fafb96f2d5dd1b704b89ecb12faebddd1ea"}
{"code": "for (int i = 0; i < 64; i++) \n    l19 = i;", "pragma": "teams distribute private(l19)", "hash": "01e2a45169defdc6eb230f322100c06c7ed810031dcfb52e0f3058800d070644"}
{"code": "for( \n    int i = 0; i < n; i++) { \n    a[i] = g[0]; \n  }", "pragma": "target teams distribute parallel for simd ", "hash": "0215f8a9eb5645f1ff01b004e1d581f5fd8fd1abdd8d064c16e2b3189862fbbe"}
{"code": "for (int i = 0; i < argc; ++i) \n  L2: \n  foo();", "pragma": "target parallel for simd ", "hash": "023e11c4330ae7f10a619d3f3babed1c1379f937be034b4ae1e3d56ee60b0c09"}
{"code": "for (int i = offset; i < offset + length; i++) { \n      y[i] = C[i] * x[i]; \n    }", "pragma": "teams distribute parallel for simd ", "hash": "0302e64240e8d0fef333b341c72ccbe5f490220f8ed292a302b2b3e2fa4035b9"}
{"code": "for (int i = 0; i < 2; ++i) { \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n      g = 1; \n      g1 = 1; \n      svar = 3; \n      sfvar = 4.0; \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n      [&]() { \n\t \n \n\t \n \n\tg = 2; \n\tg1 = 2; \n\tsvar = 4; \n\tsfvar = 8.0; \n\t \n \n\t \n \n\t \n \n\t \n \n \n\t \n \n\t \n \n\t \n \n\t \n \n\t \n \n\t \n \n\t \n \n\t \n \n\t \n \n      }(); \n    }", "pragma": "teams distribute parallel for private(   g g1 sfvar svar)", "hash": "03cdbfc1e48015f8dbbbd1ff48c554f06d0bab8d546c3d5a5f1e5d381aafe38f"}
{"code": "for(int i = 0; i < n; i++) { \n    sum += var + to_var; \n  }", "pragma": "target ", "hash": "04c5e480335203dc10cee2cedd799163af2461d4480eae544a228f37e1b9626b"}
{"code": "for (int i = 0; i < 10; ++i) \n      a += 15;", "pragma": "target parallel for ", "hash": "053219dd83be7d45d99e02e07a50d7481fef9ac2aedb8888fe0c47cd8c0bbd64"}
{"code": "for(voxel=0; voxel<voxelNumber;++voxel){ \n             \n \n            if(mask[voxel]>-1){ \n                 \n \n                targetValue = (double)currentRefPtr[voxel]; \n                resultValue = (double)currentWarPtr[voxel]; \n                if(targetValue==targetValue && resultValue==resultValue){ \n                    diff = (targetValue-resultValue); \n                     \n \n                    if(jacobianDetImage!=NULL){ \n                        SSD += diff * diff * jacDetPtr[voxel]; \n                        n += jacDetPtr[voxel]; \n                    } \n                    else{ \n                        SSD += diff * diff; \n                        n += 1.0; \n                    } \n                } \n            } \n        }", "pragma": "parallel for reduction(+:ssd) reduction(+:n) private(   diff resultvalue targetvalue voxel)", "hash": "0556cc4fc0507829974bca10f22aa3d62c97c87ca982d04c24533f54d2caf45e"}
{"code": "for (int i = offset; i < offset + length; i++) { \n      y[i] = x[i] + a * y[i]; \n    }", "pragma": "teams distribute parallel for simd ", "hash": "08af11d85880a5584e53c5f062686bf348d6beb0b7bc234e9868b2fa941c8251"}
{"code": "for (int i = 0; i < 2; ++i) { \n    a[i] = x; \n  }", "pragma": "teams distribute parallel for ", "hash": "08d4e67700595beeb554426ecc036a4ff5be0d1aa814524ea23dbad8fe051a9a"}
{"code": "for(int i = 0 ; i < 100; i++) { \n    fn3(); \n  }", "pragma": "teams distribute parallel for ", "hash": "091ae47b967451084c1b863b3c7726486da66c5996b4e67add0c733c6a0d1e12"}
{"code": "for(int i = 0; i < n; i++) { \n    a[i] = 1; \n  }", "pragma": "target simd ", "hash": "097cdcdb990b15ddc3522f6c82667468564133fe72b70d1d431ab883b2807330"}
{"code": "for(int i = 0 ; i < 100; i++) { \n     \n \n     \n \n     \n \n \n     \n \n     \n \n     \n \n     \n \n     \n \n \n     \n \n     \n \n     \n \n     \n \n    fn5(); \n  }", "pragma": "target teams distribute parallel for simd ", "hash": "0bec5b7d402428f366fcb9c444560ab0e211c7d79a272b445d7e28b4b83c1b1c"}
{"code": "for (int i=0; i<100; i++) \n    ;", "pragma": "target teams distribute parallel for simd ", "hash": "0c06d140a87950c224538ab6ee64e23800ce347dd9b6e436a43019d2851b3dfa"}
{"code": "for (int i = 0; i < N; i++) { \n    y[i] = C[i] * x[i]; \n  }", "pragma": "target teams distribute parallel for simd ", "hash": "0d2c1f63a0e8c606b042a3c1d71a05ee44848b5d710313b7feac26778df84f0a"}
{"code": "for (int i = 0; i < 200; i++) \n    ++y;", "pragma": "teams distribute simd ", "hash": "0d53d49f8130c3a8f69ea44987f8c1411a6944569db488381270c514f91e20c0"}
{"code": "for (int i = 4; i < 12; i++) \n    argv[0][i] = argv[0][i] - argv[0][i-4];", "pragma": "teams distribute simd ", "hash": "0d806533961ef04b4d2362cb4663dd1ec1b9647ed15f83489e64aa515fa327dd"}
{"code": "for (int l = 1; l < end; l++){ \n    layers[l].z = layers[l].w * (layers[l-1].a) + layers[l].b; \n    layers[l].a = (layers[l].z).unaryExpr(phi); \n  }", "pragma": "target teams distribute ", "hash": "0dee749db8e07c487a1b61d1cb644b5c35e6a0efeef63d048a76dc3fa4850f7d"}
{"code": "for (int i = 0; i < 64; i++) \n    r28[1]++;", "pragma": "teams distribute simd reduction(+:r28[1:30]) ", "hash": "0e81f50f5ff225959b16c97aa3ddc1da5a885b733a494d1fd44c11b39ca8f2cb"}
{"code": "for (int i = 0; i < 64; i++) \n    r15[1]++;", "pragma": "target parallel for simd reduction(+:r15[1:17]) ", "hash": "0ff4a760e3d8f0dac045ba518bc7f0e925f3cd413fe85dcc56d6ec20b59b3a52"}
{"code": "for (int i = 0; i < 2; ++i) { \n \n     \n \n    g = 1; \n    g1 = 1; \n    sivar = 2; \n    [&]() { \n      g = 2; \n      g1 = 2; \n      sivar = 4; \n \n    }(); \n  }", "pragma": "target teams distribute simd private(  g g1 sivar)", "hash": "0ff8d9e2c0e65367e7e134bf5688c49508808d80e56009cdff2a2a2e783bf677"}
{"code": "for (j06 = 0; j06 < 64; j06++) \n    ;", "pragma": "target parallel for simd ", "hash": "1038f44f3fb6ef72799103ae86ee48a7dcd3c1893e7e12c1caa2ac4996ad6d4f"}
{"code": "for(int i = 0 ; i < 100; i++) { \n    fn1(); \n  }", "pragma": "teams distribute parallel for ", "hash": "1075c8928e6edf7f2b3c3447f550a07dc963ccb997ec9365a4a94e18f59d18c6"}
{"code": "for (j09 = 0; j09 < 64; j09++) \n    ;", "pragma": "target teams distribute simd ", "hash": "109584e500289838e1ae6266ec8ac58b9a5462b796a92343869da0c987df96bf"}
{"code": "for (int i = 0; i < 2; ++i) { \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n    sivar += i; \n \n    [&]() { \n       \n \n       \n \n \n      sivar += 4; \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n    }(); \n  }", "pragma": "target teams distribute reduction(+: sivar) ", "hash": "10abc12fa342996880f727b489b8b194eb0d6974942eb447304e43e180602a19"}
{"code": "for(int i = 0; i < n; i++) { \n    for(int j = 0; j < n; j++) { \n      c[i][j] = i + j; \n    } \n  }", "pragma": "target teams distribute parallel for ", "hash": "10c89a5c5a0f1bbac766a6414cbc12074bd17c313333ce54928c7c3a07ab7d18"}
{"code": "for (i = 0; i < N; i++) \n      zd_dev[i] = c*xd_dev[i];", "pragma": "teams distribute parallel for ", "hash": "136774d9f1a9647d3e4ba44ae90097c6b9d8ea5e0693fc6068c0a39626733d1c"}
{"code": "for(int i = 0 ; i < 100; i++) { \n \n \n    fn4(); \n  }", "pragma": "teams distribute parallel for ", "hash": "137f3a6ab0049188b830bcefe075ea50c5deacb88cf58bf14a0bcfe4fadccca5"}
{"code": "for (IT I = begin; I <= end; I += ST) {  \n \n      ++I; \n    }", "pragma": "teams distribute ", "hash": "13c9d8ed4170ad0528358626da38515bdcb9d1dbdc1b0a8e30d3e13868e29a6c"}
{"code": "for (int k = 0; k < 10; ++k) { \n  }", "pragma": "teams distribute simd private(i)", "hash": "14d813dfb6d5386cd811e9604fc4207e5c9d1a713123a24d65a5fba576265774"}
{"code": "for (GoodIter I(begin); I < end; ++I)  \n \n    ++I;", "pragma": "teams distribute ", "hash": "151ddf81f7b5bdd6fe46819ac82437a81084351a55fe41df3682da0bc2903168"}
{"code": "for (int i=0; i<200; i++) ++argc;", "pragma": "target teams distribute parallel for simd ", "hash": "15797d081b7cc71ac8669138c7815a6a5004c1e42b62ee542928109fff6d5ff4"}
{"code": "for(int i=0; i<100; i++) { \n    pos(i).x = i; \n    pos(i).y = i+1; \n  }", "pragma": "target teams distribute parallel for ", "hash": "159f67d2c930bf0a99c46e3453ac26bb0b0fdc33073c2d1ae3ceb6bc3ddbd1da"}
{"code": "for (j12a = 0; j12a < 64; j12a++) \n    for (j12b = 0; j12b < 4; j12b++) \n      ;", "pragma": "teams distribute simd private( j12a j12b)", "hash": "159f8a7e9316a2a243409e6cafa7dcdde228979d8b54d9f0e39c2d76631536ca"}
{"code": "for (int i = 0; i < 100; i++) { \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n    foo(); \n  }", "pragma": "teams distribute parallel for ", "hash": "15beb1bd16c9ae349adb953ff4be3a3bb827bc32909a87a7e40e89aaf3598002"}
{"code": "for (int k = 0; k < 10; ++k) \n    e += d + argc;", "pragma": "teams distribute parallel for simd reduction(+:c) reduction(max:e) private(argc b)", "hash": "16676078a1759727a975e4d62eda3d2a2747fb5dd1ab3381a14f2cfb5949d7b7"}
{"code": "for(int j = 0 ; j < 510 ; j += blockSize) { \n      int ub = (j+blockSize < 510) ? (j+blockSize) : 512; \n#pragma omp parallel for \n      for(int i = j ; i < ub; i++) { \n\tA[i] += B[i] + C[i]; \n      } \n    }", "pragma": "target teams distribute ", "hash": "1675d3cbd2f2b4a4fd80f2531a56698196d95034e6506d3775d190ff36e6ff30"}
{"code": "for (int i = 0 ; i < 500 ; i++) \n    { \n      A[i] += C[i];  \n \n    }", "pragma": "target teams distribute ", "hash": "16854d7f7d0dacfddda5184657ac38107b71c1bee72f46cb889e79a2dc03bd05"}
{"code": "for(int i = 0 ; i < N ; i++) { \n      p = 2; \n      q = 3; \n      A[i] += p; \n      B[i] += q; \n    }", "pragma": "target teams distribute private(p q)", "hash": "16b31aef6440ddfeec06954921f9a08f3284038134482816482a6c1f6e1a31bb"}
{"code": "for (int i = 0; i < 2; ++i) { \n     \n \n     \n \n     \n \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n    a[i] = x; \n \n     \n \n     \n \n     \n \n     \n \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n \n     \n \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n    [&]() { \n       \n \n       \n \n      a[i] = x; \n    }(); \n  }", "pragma": "teams distribute parallel for ", "hash": "16bb213a4cf8a21b0f672e7c336399009b0050a1e257cee5b511757f81d94732"}
{"code": "for (GoodIter I = begin; I >= end; --I)  \n \n    ++I;", "pragma": "teams distribute ", "hash": "1723264a94ae1d28881f11a3106adf9c6ca1f70252a148882c5f2b3639efadaf"}
{"code": "for (int i = 0; i < 2; ++i) { \n \n     \n \n \n    sivar += i; \n \n    [&]() { \n \n      sivar += 4; \n \n    }(); \n  }", "pragma": "target teams distribute simd reduction(+: sivar) ", "hash": "18707f44c5de8494715255d971d884642ec78b42f71676a083dfb120fdf0f079"}
{"code": "for (i = 0; i < N; i++) \n    zd_dev[i] = (SUNRabs(xd_dev[i]) >= c) ? ONE : ZERO;", "pragma": "teams distribute parallel for ", "hash": "18e776d3a548db5f1e865c331ae0a8be7954f522d8b4b7ac9530890acb734d9b"}
{"code": "for (int i = 0; i < 10; ++i) \n    ++r;", "pragma": "target parallel for ", "hash": "1ba2948a29d9f306d2e9aa0cf59308d2510d0ce9df71c00069f851da68293f9a"}
{"code": "for(int i = 0 ; i < 100; i++) { \n    gtid_test(); \n  }", "pragma": "teams distribute parallel for ", "hash": "1ba844b327146d910bed88db35b71bf362d43f5e4345217aa6c704cf63593088"}
{"code": "for (int i = 0 ; i < 123 ; i++) \n    { \n      A[i] += C[i];  \n \n    }", "pragma": "target teams distribute ", "hash": "1ba9e1d77991ff3d0af22839bb6a24e52c347486cbac82754b8cfd9539514cc0"}
{"code": "for (int i = 0; i < 64; i++) \n    ;", "pragma": "target teams reduction(+:r) private(f p)", "hash": "1c112033aa636717b017b50eb11706d4fe4fb4ff0f4dda32001056020b94c8d0"}
{"code": "for(int i = 0; i < n; i++) { \n    a[i] = 0; \n  }", "pragma": "teams distribute parallel for simd ", "hash": "1ca30c29578cd444e8d1b8134073f62cc54d9352d6917806fac96ae1a51c5e10"}
{"code": "for (int i = 0; i < 10; ++i) { \n      this->a = (double)b + 1.5; \n      c[1][1] = ++a; \n    }", "pragma": "target teams distribute simd ", "hash": "1d648ae007b7d8b08284db2496b2ec993f3a62d4f63f276a711fcd2ac0431c2a"}
{"code": "for(int i = 0; i < N; i++) \n      a[i] += b[i] + k*c[i] + l*z[i];", "pragma": "teams distribute parallel for ", "hash": "1e8c4f405b6ce6bd4d2b2955d4aa0ff11a162aee3c20d7d6e0fbad1ae87161a0"}
{"code": "for (l = num_layers-2; l > 0; l--){ \n    delta = gamma*delta; \n    delta += ((layers[l+1].w).transpose() * delta/gamma).cwiseProduct((layers[l].z).unaryExpr(phiprime));  \n \n \n    gradient = (delta * (layers[l-1].a).transpose()); \n \n    layers[l].w -= eta * gradient;   \n \n    layers[l].b -= eta * delta;  \n \n \n    if (dropout[l] != 1){ \n      layers[l].w -= (eta/lamda) * (layers[l].w).unaryExpr(regularization);  \n \n      layers[l].b -= (eta/lamda) * (layers[l].b).unaryExpr(regularization);  \n \n    } \n  }", "pragma": "target teams distribute private(gradient)", "hash": "1e8faaab5359ed3fa4e29ba90d9314ecf5ea7690ccab8015e4299e8aaace1d6a"}
{"code": "for(int i = 0; i < X; i++) { \n      for(int j = 0; j < Y; j++) { \n\ta[i][j] = (T)0; \n      } \n    }", "pragma": "teams distribute parallel for simd ", "hash": "2005047bd1883e60e21ca7e8e9e216f8870914afe55722ce19b8bc9efe59a1db"}
{"code": "for (j11a = 0; j11a < 64; j11a++) \n    for (j11b = 0; j11b < 4; j11b++) \n      ;", "pragma": "teams distribute parallel for simd private( j11a j11b)", "hash": "209b91f107e0fa3413ed53f4c497fb5208e635535adcde2edb7ccf819cc46a3a"}
{"code": "for (int i = 0; i < 10; ++i) \n      a += 16 + x;", "pragma": "target parallel for ", "hash": "20b93d5488f4ee6dbb7efe9ce6343ce616007b1e17643832eba091ad787eb34a"}
{"code": "for (i = 0; i < N; i++) \n    zd_dev[i] = xd_dev[i]+b;", "pragma": "teams distribute parallel for ", "hash": "2110d4763ad63010e33352f97335e10d1df770bf7084fa9b8477bda4dc7991fd"}
{"code": "for (int i = 0; i < 10; ++i) { \n    int &f = c[1][1][1]; \n    int &g = a; \n    int &h = b[1][1]; \n    int d = 15; \n    a = 5; \n    b[0][a] = 10; \n    c[0][0][a] = 11; \n    b[0][a] = c[0][0][a]; \n    bb |= b[0][a]; \n  }", "pragma": "target parallel for private( a b)", "hash": "216f3df1077fdadc0fc95d8ca2979001af2db175e32d954d3d95ee7eb70165b9"}
{"code": "for (i = 0; i < bufsize; i++) { \n        k = (int)omp_get_team_num() + bufsize - bufsize2; \n         \n \n         \n \n        unew[i] = (omp_get_team_num()+1) * omp_get_thread_num(); \n    }", "pragma": "teams distribute parallel for private(k)", "hash": "2178c75e9233ac7294ac9cbc91a51d5fb9522470e4a54fa7015b3c5f2ab59440"}
{"code": "for (int i = 0; i < argc; ++i) \n  L2: foo();", "pragma": "target teams distribute parallel for ", "hash": "21d4d13a56d7148cdb976d9c68176bf222137034f9e6e2d4e2a1131540c39e03"}
{"code": "for (int j08 = 0; j08 < 64; j08++) \n    ;", "pragma": "target teams distribute parallel for simd ", "hash": "21eedd7a834fd8be54a9ae38bdd6395300f0f58590aa8a6ca91b66cf496a1ed8"}
{"code": "for(int k=0; k<N; k++) { \n      a[k] = k; \n      n = k; \n    }", "pragma": "teams distribute simd private(n)", "hash": "21ef492dd14103df258b0af17d70db31b02fe77191e4ff12afdb9c2aed53c889"}
{"code": "for(int i = 0; i < M; i++) { \n    for(int j = 0; j < M; j++) { \n      k = M; \n      c[i][j] = i+j*f+k;       \n    } \n  }", "pragma": "target teams distribute simd private(f k)", "hash": "23b6aa18a39f5789dd48d982cc84192f02d7c124c882a780a7a746500c373a46"}
{"code": "for (int i = offset; i < offset + length; i++) { \n      vectorDotResults[d] += a[i] * b[i]; \n    }", "pragma": "teams distribute parallel for simd reduction(+:vectordotresults[d:1]) ", "hash": "24adc799c1e4f664b2a51bdcda8733f07ea8c3c41df41e2a5f2d5e9ced496217"}
{"code": "for (i = 0; i < argc; ++i) si = i + 1;", "pragma": "teams distribute simd private(si)", "hash": "24b50f9b8bf156a6ed753f638f155c2344f490e0d25439f2a17f7a31d1953433"}
{"code": "for (int i = 0; i < 64; i++) \n    r21++;", "pragma": "target teams distribute simd reduction(+:r21) ", "hash": "25b1ac97bb09545dfadc4916c5f4fd9a62fd0f1fafaf79fdc42954946e6fd2ea"}
{"code": "for (int i = 0; i < 2; ++i) { \n \n     \n \n \n \n    sivar += i; \n \n    [&]() { \n \n      sivar += 4; \n \n    }(); \n  }", "pragma": "teams distribute simd reduction(+: sivar) ", "hash": "27f1c91a442aa086b0d9fff4d3debfd4c543d11c13865dd6bf1f8696bb17c911"}
{"code": "for (int i = 0; i < 64; i++) \n    r18[1]++;", "pragma": "target teams distribute reduction(+:r18[1:20]) ", "hash": "280eb44490c6fb70dbc977969b6108b6cbeb54ed09f333000b237e98badb4210"}
{"code": "for (int i = 0; i < 10; i++);", "pragma": "target parallel reduction(+:o[0]) ", "hash": "2818df6b1f693ee3835c203718210c30514e34bc0ea57e24193a5f54c351d52a"}
{"code": "for (int n = 0; n < size; ++n)\r \n\t\t\tsinTable[n] = std::sin(2 * M_PI * n / size);", "pragma": "target teams distribute parallel for ", "hash": "284391db290dc041222a393d5eb0fcd4902c464857261273daf4ac158550540a"}
{"code": "for (i = 0; i < 10; i++) \n\tif (b) \n\t  bar (); \n\telse \n\t  baz ();", "pragma": "target simd ", "hash": "287f242d729e029c6c730132df191206b4bacba6f4929a81df82fbce175be1e3"}
{"code": "for (int k = 0; k < argc; ++k) { \n    ++k; \n    v += j; \n  }", "pragma": "target simd ", "hash": "288104e397fe219d46eb634db5bb67a6c84026adaae6e6859be40997cdc3c184"}
{"code": "for(i=0; i<P; i++) { \n     \n \n    #pragma omp target enter data map(alloc: p[i][0:C]) \n  }", "pragma": "target ", "hash": "28f3039d1894c557f205bc7a32065657a28ef15dd09a1529973304ab39a00852"}
{"code": "for (j09a = 0; j09a < 64; j09a++) \n    for (j09b = 0; j09b < 4; j09b++) \n      ;", "pragma": "target teams distribute simd private( j09a j09b)", "hash": "2977288739fc9aaad352af8d8227084d1fcfc95b7d0e5796dd71bdea7c2d83d0"}
{"code": "for(int i = 0; i < n; i++) { \n    aa[i] += 1; \n  }", "pragma": "target teams distribute simd ", "hash": "2b3628db30b3ba561f6ed7a8569104ef3d33f7fb3b5aedb5f6aa6a13e9b9e5ba"}
{"code": "for (int i = 0; i < N; i++) \n    if (a.ptr[i] != 1) \n      abort ();", "pragma": "target ", "hash": "2bb43ac0b4f6abfdb4f0fe873c5d77076fdd265b5f49bab4aac4a0d98a9ebb8e"}
{"code": "for (int i = 0; i < size; i++) \n    ;", "pragma": "target parallel reduction(+: a[:2]) ", "hash": "2c513075c7a1d6e948a5f418e96c4024e15ceb5a8506a888f8790aa2f5677611"}
{"code": "for (int i = 0; i < length; i++) { \n       \n \n      if (!roundup || ptr[i] != ptr[i + 1]) { \n        floatType tmp = (roundup ? y[offset + i] : 0); \n        #pragma omp simd reduction(+:tmp) \n        for (int j = ptr[i]; j < ptr[i + 1]; j++) { \n          tmp += value[j] * x[index[j]]; \n        } \n        y[offset + i] = tmp; \n      } \n    }", "pragma": "teams distribute parallel for ", "hash": "2d12db8c66c8e118e4006220e5f5d192b25f91c77b8c389e3adef0179cec7b51"}
{"code": "for(int i = 0 ; i < 100; i++) { \n \n \n    fn5(); \n  }", "pragma": "teams distribute parallel for ", "hash": "2d80f2737685f389fd1b07530443a8624608184d3cd61d8cd4d0fce53c24cf4c"}
{"code": "for (int i = 3; i < 32; i += 5) { \n  }", "pragma": "target simd ", "hash": "2ebfdaccda2fb7fb22931ca322ca4d27142702e8863908ee8de3525ba98923bc"}
{"code": "for (int i = 0; i < 64; i++) \n    l16 = i;", "pragma": "target teams distribute simd private(l16)", "hash": "2ef7bbf0797259c6e64dade296f2c664aad85802ce690b08cd0cf16a482f8d14"}
{"code": "for (int i = 0 ; i < M ; i++) \n      for (int j = 0 ; j < M ; j++) \n        for (int k = 0 ; k < M ; k++) \n          V[i*M*M+j*M+k] += Z[i*M*M+j*M+k];", "pragma": "teams distribute simd ", "hash": "2f4815d652b6d51d05d8d657ba920f12a76b9ed4c8a49cd6a5a31c9a76699c83"}
{"code": "for (int i = 0; i < 2; ++i) { \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n      g = 1; \n      g1 = 1; \n      svar = 3; \n      sfvar = 4.0; \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n      [&]() { \n\t \n \n\t \n \n\tg = 2; \n\tg1 = 2; \n\tsvar = 4; \n\tsfvar = 8.0; \n\t \n \n\t \n \n\t \n \n\t \n \n \n\t \n \n\t \n \n\t \n \n\t \n \n\t \n \n\t \n \n\t \n \n\t \n \n\t \n \n      }(); \n    }", "pragma": "teams distribute private(   g g1 sfvar svar)", "hash": "3021530db6bbe7beb72fcf69b123f8fb2270c2e401f3f9072f6b7c53a369881f"}
{"code": "for (int i = 0; i < 64; i++) \n    l17 = i;", "pragma": "target simd private(l17)", "hash": "3083fe3957ded408beaec2f8d367408225503be3945eeec479019782ecd308aa"}
{"code": "for (d = a; d < b; d++) \n    { \n      u[d] = v[d] + w[d]; \n      e = c + d * 5; \n    }", "pragma": "teams distribute parallel for private(   a b c d e)", "hash": "30fd245c176728af04589b896a99e552f7154d03ce99776561df0481639bd363"}
{"code": "for (int i = 0; i < 64; i++) \n    r27[1]++;", "pragma": "teams distribute parallel for simd reduction(+:r27[1:29]) ", "hash": "324b049b8e6c5ac94f84b1cb55bd7fa37e4142ecfc692ee5c823c6862986351e"}
{"code": "for (i = 0; i < 16; ++i)  \n \n \n \n    for (int j = 0; j < 16; ++j)", "pragma": "target parallel for private(i)", "hash": "324e9ee3f227f3af206c8daa5f99245bed2ad7cb1c67d763b80300879fba319a"}
{"code": "for (int i = 0; i < 100; ++i) { \n  }", "pragma": "target teams distribute parallel for ", "hash": "329772abebf062884750b52c3bba72860b5d0e097825e998d0cdf76e20fed8df"}
{"code": "for (int i = 0; i < 2; ++i) { \n    vec[i] = t_var; \n    s_arr[i] = var; \n  }", "pragma": "teams distribute simd private(   s_arr t_var var vec)", "hash": "32aa7b2b61b0d69f1dede33c42ad217a0e51e5b1700d713e4b83faba57ee09a4"}
{"code": "for (j08 = 0; j08 < 64; j08++) \n    ;", "pragma": "target teams distribute parallel for simd ", "hash": "33a3fe5ae64c686754d38e2d1c26ff2163e138a595887d3646b4be2852cbef31"}
{"code": "for (int i=0; i < N; ++i) \n    ordc = ordc || rcd[i];", "pragma": "target teams distribute parallel for simd reduction(ordc ||:) ", "hash": "33a857c8d4e7d9b23c8ee01cd0b701a4b4366f9088cfc205f1c44ad47f56e90d"}
{"code": "for (j07a = 0; j07a < 64; j07a++) \n    for (j07b = 0; j07b < 4; j07b++) \n      ;", "pragma": "target simd private( j07a j07b)", "hash": "33da39d49e0b228f8b66da5da8bd27a445ad025c3f81c5e67d3de67b39354c11"}
{"code": "for (i = 0; i < N; i++) { \n    if (idd_dev[i] > ZERO) { \n      sum += SUNSQR(xd_dev[i]*wd_dev[i]); \n    } \n  }", "pragma": "teams distribute parallel for reduction(+:sum) ", "hash": "34200beb0c8c181cdbb21a62212e88d6b1287d62c53be02673f8f0a90aed7bc3"}
{"code": "for (int i = 0 ; i < 512 ; i++) \n    { \n      A[i] += C[i];  \n \n    }", "pragma": "target teams distribute ", "hash": "349a8ef76b026d75f212cacfec99d178222c0be456cbeeb72aed2c003f3bc341"}
{"code": "for(int j = 0 ; j < 256 ; j += blockSize) { \n#pragma omp parallel for \n\tfor(int i = j ; i < j+blockSize; i++) { \n\t  A[i] += B[i] + C[i]; \n\t} \n      }", "pragma": "target teams distribute ", "hash": "3544b942a1c39389e28ca661b7f1c03e737124f1df1fe6e25a4bf310abf40d10"}
{"code": "for (auto i = 0; i < N; ++i) { \n    #pragma omp loop \n    for (auto j = 0; j < N; ++j) { \n      MTX[i][j] = 0; \n    } \n  }", "pragma": "target teams distribute parallel for ", "hash": "3549dbcba759dccdf6834499a495fa55ffffbaf1cf1ba7ecf0081ea05f99bbcd"}
{"code": "for (int i = 0; i < array_size; i++) \n  { \n    c[i] = a[i]; \n  }", "pragma": "target teams distribute parallel for simd ", "hash": "35d92ea07bc18b4b053c81dd3f4af58626843f19bb867cbb1c12c5e837f9febd"}
{"code": "for (int i=0; i < N; ++i) \n    orf = orf || rf[i];", "pragma": "target teams distribute parallel for reduction(orf ||:) ", "hash": "363fb1fa1c037ff9e63a40b1964adf69481b133e896b2e76f13fdb4d13a9fe43"}
{"code": "for (int i = 0; i < 64; i++) \n    r25[1]++;", "pragma": "teams distribute reduction(+:r25[1:27]) ", "hash": "36a20d5ed3f834c1993d4e5da26fce3f69dbdcbb0c2b2570be8c9540d569c40c"}
{"code": "for (int i = 0; i < 2; ++i) { \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n      g = 1; \n      g1 = 1; \n      svar = 3; \n      sfvar = 4.0; \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n      [&]() { \n         \n \n         \n \n        g = 2; \n        g1 = 2; \n        svar = 4; \n        sfvar = 8.0; \n         \n \n         \n \n         \n \n         \n \n \n         \n \n         \n \n         \n \n         \n \n         \n \n         \n \n         \n \n         \n \n         \n \n      }(); \n    }", "pragma": "teams distribute simd private(   g g1 sfvar svar)", "hash": "36fbbff92cb93336fe270dd8450ea39ca70ba543eba86163ef11efdc4613c07b"}
{"code": "for (int tms = 1 ; tms <= 256 ; tms *= 2) {  \n \n      for (int ths = 32 ; ths <= 1024 ; ths *= 2) {  \n \n\t  t++; \n#pragma omp target teams distribute parallel for simd schedule(dynamic) num_teams(tms) thread_limit(ths) \n\t    for (int i = 0; i < n; ++i) { \n\t      a[i] += b[i] + c[i]; \n\t    } \n      }  \n \n    }", "pragma": "target ", "hash": "374b51cdab3d3b85d13fd2c75275201706d7657da14ef566bd7068a1410c9057"}
{"code": "for (int ii = 0; ii < n; ii++) { \n       \n \n       \n \n       \n \n      vec[ii] = iter + ii + x; \n    }", "pragma": "target teams distribute parallel for ", "hash": "397a8c337035c9267c69419b05de5fb582eaf9cabec8da181b3b7652f68a3887"}
{"code": "for (int i = 0; i < 2; ++i) { \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n      g = 1; \n      g1 = 1; \n      svar = 3; \n      sfvar = 4.0; \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n      [&]() { \n         \n \n         \n \n        g = 2; \n        g1 = 2; \n        svar = 4; \n        sfvar = 8.0; \n         \n \n         \n \n         \n \n         \n \n \n         \n \n         \n \n         \n \n         \n \n         \n \n         \n \n         \n \n         \n \n         \n \n      }(); \n    }", "pragma": "teams distribute simd private(   g g1 sfvar svar)", "hash": "3a29bfbdaa09dfed5c38ee93d870cd83988721ad6e0bf73daf40a61a69a32853"}
{"code": "for (int i = 0; i < n; ++i) { \n\t\ta[i] += b[i] + c[i]; \n\t      }", "pragma": "target teams distribute parallel for simd ", "hash": "3abf3647867fae4726fc178cf188d039ddee6c508e5a0ba1477cf2dc2c7024c0"}
{"code": "for (i = 0; i < 10; ++i) \n    ;", "pragma": "target teams distribute parallel for simd ", "hash": "3bc618dc52b90edbe1b34a081c32251244a6aa0f505f3379826059b4f9077043"}
{"code": "for (int i = 0 ; i < M ; i++) \n      for (int j = 0 ; j < M ; j++) \n\tfor (int k = 0 ; k < M ; k++) \n\t  V[i*M*M+j*M+k] += Z[i*M*M+j*M+k];", "pragma": "target teams distribute ", "hash": "3be107c1019378b837ce59d05f26e55dd396f4ab15ce16d92bafbc76b044ef3f"}
{"code": "for (int i = 0 ; i < N ; i++) \n      for (int j = 0 ; j < N ; j++) \n\tS[i*N+j] += T[i*N+j] + U[i*N+j];", "pragma": "target teams distribute ", "hash": "3d604f3e530789ec301f11e5f8b4e359728473ae113b5646c1b4219398c0312a"}
{"code": "for (int i = 0; i < 10; ++i) { \n#pragma omp target update to(a)  \n \n  }", "pragma": "target parallel for ", "hash": "3d8cbf8d62a364b38f5470a8b4be97b79e46541e203d175fb7e4e8a7cecd07d7"}
{"code": "for (int i = 0; i < N; i++) \n\t  p[i] = i;", "pragma": "target ", "hash": "3dad7f9c946821e56870f3acd3e8600df266bce4a4cd1a3a03c905a4dfc3c97b"}
{"code": "for (int i = 0; i < 10; ++i) \n    a = i;", "pragma": "target teams distribute parallel for private(a)", "hash": "3def2126bbb26949ac90d9b9a2a0f68d7507693456d954800ecfe9025283d6a2"}
{"code": "for(int i = 0 ; i < 100; i++) {}", "pragma": "target teams distribute parallel for ", "hash": "3ef7116c896fecae0bc01b40851fd262324c629610ec9a1eede9a4d891e999a4"}
{"code": "for (i = 0; i < 32; i++) \n    for (j = 0; j < 32; j+=2) \n      ;", "pragma": "target teams distribute parallel for ", "hash": "3fd0f127d6ce752c19d62a5b8ea2d75d7d540ff574e3bb1681651960077081c9"}
{"code": "for (int n = 0; n < 100; ++n) {}", "pragma": "target ", "hash": "40dafd468124a7ab9ad5fc5a2b5ab486f7d5e6c6ba05f9054808314952d33669"}
{"code": "for (int j=0; j<100; j++) foo();", "pragma": "teams distribute parallel for simd reduction ", "hash": "447f361cb0a5b9161af38370afaf0671e95595dde5df5108052fe0f26177d39f"}
{"code": "for (int i = 0; i < 64; i++) \n    r18++;", "pragma": "target teams distribute reduction(+:r18) ", "hash": "4489ebeb25c7abd664e3ccdacad163163a27fa025e4332eb5eee9e4020644279"}
{"code": "for (int i = 0; i < N; i++) \n\ty[i] = i + 2;", "pragma": "target ", "hash": "452b89d4ddad9b2d18c4fa646373120e555771406ffe8f012bb08b10c2175453"}
{"code": "for (int i = 0; i < 10; ++i) { \n    int &f = c[1][1][1]; \n    int &g = a; \n    int &h = b[1][1]; \n    int d = 15; \n    a = 5; \n    b[0][a] = 10; \n    c[0][0][a] = 11; \n    b[0][a] = c[0][0][a]; \n    d = bb; \n  }", "pragma": "target parallel for private(a)", "hash": "45f289883958549d42a2706ed5f5af02a42747b77811f17562c02aa6a7bb85db"}
{"code": "for (int i = 0 ; i < N ; i++) \n      for (int j = 0 ; j < N ; j++) \n        S[i*N+j] += T[i*N+j] + U[i*N+j];", "pragma": "teams distribute simd ", "hash": "46f6bf06afe438237aaf01420fbc9c51149d10a39f6c1a02386fc7e9cd87f7d9"}
{"code": "for (int i = 0; i < 64; i++) \n    r23[1]++;", "pragma": "target simd reduction(+:r23[1:25]) ", "hash": "48006645e014a048d0b6e2c2d36b91e91f5f87f1ea370715c396a7bcab50e751"}
{"code": "for (int i = 0; i < 64; i++) \n    l21 = i;", "pragma": "teams distribute parallel for simd private(l21)", "hash": "485ff581ef7f1a69ec1c9da8fd6676b282ce523c58d82403303172cb9a0ee97a"}
{"code": "for (int k = 0; k < argc; ++k) { ++k; i += 4; }", "pragma": "target teams distribute parallel for simd ", "hash": "48a8f0af68f81a22d07554ec20afdb68c9f82c1d91a94ece52ac809479579aeb"}
{"code": "for (i = 0; i < 10; i++) \n\t{ \n\t  if (b) \n\t    bar (); \n\t  else \n\t    baz (); \n\t}", "pragma": "target simd ", "hash": "49001d1a5722ce854cbefb8640e06b726ac0c7beb6140ca07a043710c2f87bc6"}
{"code": "for (l = num_layers-2; l > 0; l--){ \n \n    msD = gamma * msD + (1.-gamma)*delta.squaredNorm(); \n \n    delta = ((layers[l+1].w).transpose() * delta); \n    delta = delta.cwiseProduct((layers[l].z).unaryExpr(phiprime));  \n \n \n    gradient = (delta * (layers[l-1].a).transpose()); \n \n    msW = gamma * msW + (1.-gamma)*gradient.squaredNorm(); \n \n    lr = sqrt(msD + epsilon)/sqrt(msW + epsilon); \n \n    layers[l].w -= lr * gradient;   \n \n    layers[l].b -= lr * delta;  \n \n \n    if (dropout[l] != 1){ \n      layers[l].w -= (lr/lamda) * (layers[l].w).unaryExpr(regularization);  \n \n      layers[l].b -= (lr/lamda) * (layers[l].b).unaryExpr(regularization); \n    } \n  }", "pragma": "target teams distribute ", "hash": "49e88089fb6b94b0787f89b9ac26a749179b74020d240ea0780b8a31bd54db7f"}
{"code": "for(int j = 0 ; j < 256 ; j += blockSize) { \n      for(int i = j ; i < j+blockSize; i++) { \n        A[i] += B[i] + C[i]; \n      } \n    }", "pragma": "teams distribute simd ", "hash": "4a6ab4ac265687c686998dbc2dc5d2c85f0b269545abea9326dcad7c9eddfad3"}
{"code": "for (i = 0; i < 64; i++) \n    p[2]++;", "pragma": "teams distribute parallel for reduction(+: p[2:2]) ", "hash": "4ac8726338899ab52692c8eed91bf3ee37a7a5220412650a62e0b699ffa2112b"}
{"code": "for (i = 0; i < N; i++) \n    zd_dev[i] = c*(xd_dev[i]+yd_dev[i]);", "pragma": "teams distribute parallel for ", "hash": "4b36460aa89bdd931f045942063fa010f2413a7e75edfe137a0206d3e5cea110"}
{"code": "for (int i = 0; i < 100; i++) \n    foo();", "pragma": "teams distribute parallel for simd ", "hash": "4b613a52540eff09e9b9e0c61ab6e5b7703cb74c0feb8b119b43e7147780ad14"}
{"code": "for (int i = 0; i < length; i++) { \n       \n \n      if (!roundup || lengthA[i] > 0) { \n        floatType tmp = (roundup ? y[offset + i] : 0); \n        #pragma unroll 1 \n        for (int j = 0; j < lengthA[i]; j++) { \n          int k = j * length + i; \n          tmp += data[k] * x[index[k]]; \n        } \n        y[offset + i] = tmp; \n      } \n    }", "pragma": "teams distribute parallel for simd ", "hash": "4bfe77b72a0cf8da02c6e6181a50bbcaae178757422e4d601e2c33a7c798429c"}
{"code": "for (int i = 0; i < 2; ++i) { \n     \n \n     \n \n     \n \n     \n \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n \n    g = 1; \n    g1 = 1; \n    sivar = 2; \n     \n \n     \n \n     \n \n     \n \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n    [&]() { \n       \n \n       \n \n      g = 2; \n      g1 = 2; \n      sivar = 4; \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n    }(); \n  }", "pragma": "target teams distribute parallel for private(  g g1 sivar)", "hash": "4c6c3ddb01228a5a5df0eadb90a478fc380f369f031dc886f4a4cfa92f0e2c75"}
{"code": "for (i = 0; i < 64; i++) \n    s[0]++;", "pragma": "teams distribute parallel for reduction(+: s) ", "hash": "4c83841930e9ca9e61c95c3067b7922df18439f260d56c08adc2cc7dcc6130c0"}
{"code": "for (d1 = a1; d1 < b1; d1++) \n    for (d2 = a2; d2 < b2; d2++) \n      u[d1 * 32 + d2] = v[d1 * 32 + d2] + w[d1 * 32 + d2];", "pragma": "teams distribute parallel for private(    a1 a2 b1 b2 d1 d2)", "hash": "4c9752ef5c573444c25e2d55722f20b8e7becad34701b3f5989b1eb2fa0e640d"}
{"code": "for (i = 0; i < argc; ++i) \n      foo();", "pragma": "target parallel for ", "hash": "4d18f8eedb091bd469a7666641dca6f720508a0c7c9f2569493e369842a146ec"}
{"code": "for (int i = 0; i < 64; i++) \n    r25++;", "pragma": "teams distribute reduction(+:r25) ", "hash": "4d49163f28fc9e0ce1865c78ec0049c9b191004d89e02097a13a4b1ee0d512f7"}
{"code": "for (i = 0; i < N; i++) { \n    sum += SUNSQR(xd_dev[i]*wd_dev[i]); \n  }", "pragma": "teams distribute parallel for reduction(+:sum) ", "hash": "4d8f4565a31634bffeaff4fb5168a943d4dd8d180c42c883abc341b1a71fb308"}
{"code": "for (int i=0; i<100; i++) foo();", "pragma": "teams distribute ", "hash": "4dd7cb0387c0cd2cfbe1603c2a6f44b6b87b2b77537d12d61f1bb119f5c39245"}
{"code": "for (int i=0; i < N; ++i) \n    ord = ord || rcd[i];", "pragma": "target teams distribute parallel for simd reduction(ord ||:) ", "hash": "4e0d8e71b24e3dbe765faeb361447ccf3f663b05298bf14302a636112e048f8d"}
{"code": "for (int i=0; i < N; ++i) \n    ori = ori || rci[i];", "pragma": "teams distribute parallel for reduction(ori ||:) ", "hash": "4e43912c293ad0281a915cff803b7844442cffb9c2b3b1eca577bebfafda4162"}
{"code": "for (j11 = 0; j11 < 64; j11++) \n    ;", "pragma": "teams distribute parallel for simd ", "hash": "4ebd1eaee267c16727044288a86062e2498bb8d75b57872228e41c76fb85ec08"}
{"code": "for (int i = 0; i < 10; ++i) { \n    a += 1; \n  }", "pragma": "target teams distribute ", "hash": "4f31dfcf8f3a58dccecafaaefa063f11f662740059ae58de71dc0ad2f98931ee"}
{"code": "for (int i = 0; i < 2; ++i) { \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n      g1 = 1; \n      svar = 3; \n      sfvar = 4.0; \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n      [&]() { \n         \n \n         \n \n        g = 2; \n        g1 = 2; \n        svar = 4; \n        sfvar = 8.0; \n         \n \n         \n \n         \n \n         \n \n \n         \n \n         \n \n         \n \n         \n \n         \n \n         \n \n         \n \n         \n \n         \n \n      }(); \n    }", "pragma": "target teams distribute parallel for simd private(   g g1 sfvar svar)", "hash": "50de4cfb164990eb8e3a8ee5eb640115be8b232afd2e1edf2a0913c51b70baf7"}
{"code": "for (int i = 0; i < n; ++i) { \n\t    a[i] += b[i] + c[i]; \n\t  }", "pragma": "target teams distribute parallel for simd ", "hash": "531f72916aeed2521b786bea681c898534bca1d9121a41f90a02226119b7aa00"}
{"code": "for(int i = 0 ; i < 100; i++) { \n \n \n    fn6(); \n  }", "pragma": "teams distribute parallel for ", "hash": "534b656e91688e2ba2e9d5a6dbc75a31b852451f3628687641781217465a9267"}
{"code": "for(int i = 0 ; i < 100; i++) { \n     \n \n     \n \n     \n \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n \n    fn4(); \n  }", "pragma": "target teams distribute parallel for simd ", "hash": "53588e451419d0886430ddfd56eff23920d7ddc36a422d28fe3e5cd09e22a6ab"}
{"code": "for (begin = GoodIter(0); begin < end; ++begin)  \n \n    ++begin;", "pragma": "teams distribute ", "hash": "5458dfbb2b51b48a3b8301cc58f5dfb78add2b4b00e366d150e83e2e8bf7bb7f"}
{"code": "for (i = 0; i < N; i++) \n    zd_dev[i] = SUNRabs(xd_dev[i]);", "pragma": "teams distribute parallel for ", "hash": "54b0a803dd90c92e8286173f5d8dd9800c92282911cc1e07a77a1dc6904c7358"}
{"code": "for (int i=0; i < N; ++i) \n    ors = ors || rcs[i];", "pragma": "teams distribute parallel for simd reduction(ors ||:) ", "hash": "55c6f17fba1857127eeeccf6aa29033421b738b0ee2681cafeb01ce3ac229098"}
{"code": "for (int i=0; i < N; ++i) \n    andl = andl && rcc[i];", "pragma": "teams distribute parallel for simd reduction(&&: andl) ", "hash": "570b3d85a967086ab144dbb16fd6e8ca3766e446aeff48968097173b77127353"}
{"code": "for(voxel=0; voxel<voxelNumber;voxel++){ \n            if(mask[voxel]>-1){ \n                targetValue = currentRefPtr[voxel]; \n                resultValue = currentWarPtr[voxel]; \n                gradX=0; \n                gradY=0; \n                gradZ=0; \n                if(targetValue==targetValue && resultValue==resultValue){ \n                    common = - 2.0 * (targetValue - resultValue)/maxSD; \n                    gradX = (DTYPE)(common * currentGradPtrX[voxel]); \n                    gradY = (DTYPE)(common * currentGradPtrY[voxel]); \n                    if(referenceImage->nz>1) \n                        gradZ = (DTYPE)(common * currentGradPtrZ[voxel]); \n                    if(jacobianDetImage!=NULL){ \n                        JacDetValue = jacDetPtr[voxel]; \n                        gradX *= JacDetValue; \n                        gradY *= JacDetValue; \n                        if(referenceImage->nz>1) \n                            gradZ *= JacDetValue; \n                    } \n                    ssdGradPtrX[voxel] += gradX; \n                    ssdGradPtrY[voxel] += gradY; \n                    if(referenceImage->nz>1) \n                        ssdGradPtrZ[voxel] += gradZ; \n                } \n            } \n        }", "pragma": "parallel for private(       common gradx grady gradz jacdetvalue resultvalue targetvalue voxel)", "hash": "579d9025fca72ab2f8608c9e7578b1f362763dc5f752d788a2e4fabe41b8d4fd"}
{"code": "for (int i=0; i < 2; ++i) \n    a = 2;", "pragma": "teams distribute parallel for simd ", "hash": "58ca403d0cc20faca828b3884829118d0a78f9a3183e9d2dbd08ed9da04d4c55"}
{"code": "for(int i = 0; i < X; i++) { \n      for(int j = 0; j < Y; j++) { \n        a[i][j] = (T)0; \n      } \n    }", "pragma": "target teams distribute parallel for simd ", "hash": "5933373b44a079dc6e9f15d57ec992b1cfc302ec4aaa523f93db4b199538ef90"}
{"code": "for (i = 0; i < 10; i++) \n    { \n      #pragma omp cancel parallel   \n \n      #pragma omp cancel for    \n \n      #pragma omp cancel sections   \n \n      #pragma omp cancel taskgroup   \n \n      #pragma omp cancellation point parallel  \n \n      #pragma omp cancellation point for  \n \n      #pragma omp cancellation point sections  \n \n      #pragma omp cancellation point taskgroup  \n \n    }", "pragma": "target teams distribute ", "hash": "593fce7cf31b8d9d85a836e67d7669f941ca364873f117dc47da141676513c68"}
{"code": "for (int i = 12; i < 24; i++) \n\ty[i] = i + 4;", "pragma": "target ", "hash": "5b4fa4c39244eaf7d36aed076517a538291eb4c3c8ac32e6a3f451da0a2afdc9"}
{"code": "for (int i = 0; i < 2; ++i) { \n \n     \n \n     \n \n    g = 1; \n    g1 = 1; \n    sivar = 2; \n \n     \n \n     \n \n \n     \n \n    [&]() { \n      g = 2; \n      g1 = 2; \n      sivar = 4; \n \n    }(); \n  }", "pragma": "target teams distribute parallel for simd private(  g g1 sivar)", "hash": "5baa9f6c92e96982577b196101daf5c93f9b56b372b283d8b2292d09d72eccea"}
{"code": "for (j = 0; j < argc; ++j) foo();", "pragma": "target teams distribute simd private(i)", "hash": "5be42a67b9fb85cfc3dd0df5b3c7cfce146548236a29477be1f11c27776a4831"}
{"code": "for (j12 = 0; j12 < 64; j12++) \n    ;", "pragma": "teams distribute simd ", "hash": "5c4436a4a426abe3e31867c9d962479234022882a56fa1efdac184060083134f"}
{"code": "for (int k = 0; k < argc; ++k) { \n    i = k; \n    v += i; \n  }", "pragma": "teams distribute simd private(i)", "hash": "5c9ce77accdf38e10bff107464d3cd1f98f1e61661b978c8bf3eff7f46750837"}
{"code": "for(int i = 0 ; i < omp_get_num_teams() ; i++) \n    lastpriv = omp_get_team_num();", "pragma": "teams distribute simd private(lastpriv)", "hash": "5cfe1d90bdc31b28f36402658143a1e0db2731e47c4466779884a1ec4750d71a"}
{"code": "for(int i = 0 ; i < 100; i++) { \n     \n \n     \n \n     \n \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n \n     \n \n     \n \n     \n \n     \n \n    fn6(); \n  }", "pragma": "teams distribute parallel for simd ", "hash": "5d82a93a1e39f585dcc30d0cd916746593fdea2e0b79cf58fdfffc1cb8467392"}
{"code": "for (int x = 0; x < 10; ++x) foo();", "pragma": "teams distribute simd private(i)", "hash": "5e0e98efbc48a3f6437b6eb5956631e08e3059aadf5ca115dd5560db7fa272e2"}
{"code": "for (int i = 0; i < 2; ++i) { \n \n     \n \n    a[i] = x; \n \n \n     \n \n \n \n    [&]() { \n      a[i] = x; \n    }(); \n  }", "pragma": "teams distribute parallel for ", "hash": "5f3691c5fa5eb27b166c36607f86b15f6f848e029bc764f40d503d0a920e353f"}
{"code": "for (unsigned long long it = 2000; it >= 600; it -= 400) { \n      this->a = (double)b + 1.5; \n      c[1][1] = ++a; \n    }", "pragma": "target parallel for ", "hash": "5f80aedec3dbb15e42face4fe0f53f45ce6e064a795c63bfe5b56f0cadf755e2"}
{"code": "for (int i = 0; i < 2; ++i) { \n \n     \n \n \n \n \n     \n \n     \n \n \n \n    sivar += i; \n \n    [&]() { \n \n      sivar += 4; \n \n    }(); \n  }", "pragma": "target teams distribute parallel for reduction(+: sivar) ", "hash": "606368749d857e4bd226fb87620c443e6fc7205d5042a1c0ebd60c28de87290c"}
{"code": "for (i = 0; i < N; i++) \n      yd_dev[i] += xd_dev[i];", "pragma": "teams distribute parallel for ", "hash": "60c98cd0a2c6f56df19d15e98af51e17c573eda3f7893d5b1c914f5553f6dd71"}
{"code": "for (int i = 0; i < N; i++) { \n    arr[i] = callable(arr[i]); \n  }", "pragma": "target ", "hash": "617a60f2e40deeb06ec977206217c9f1e990b65f2ec354e607755f4c3631c987"}
{"code": "for (int i = 3; i < 32; i += 5) { \n#pragma omp cancel for \n#pragma omp cancellation point for \n  }", "pragma": "target parallel for ", "hash": "625bfd5ad4733f6a7de6d8ebfe44c55fc7e8919653371811150b06ab191e6aba"}
{"code": "for (i = 0; i < N; i++) \n    zd_dev[i] = xd_dev[i]-yd_dev[i];", "pragma": "teams distribute parallel for ", "hash": "62d24f64c65b59da7b232f18d3bcafd34352dc776581badefca51e788e6305fe"}
{"code": "for(int i = 0 ; i < 100; i++) { \n    fn2(); \n  }", "pragma": "teams distribute parallel for ", "hash": "6310df4c83608acb3b336af38be6b74e2031d63d7c8bb042663e5d7355fe7dc1"}
{"code": "for (int j09 = 0; j09 < 64; j09++) \n    ;", "pragma": "target teams distribute simd ", "hash": "638f4053cc4d3563cdb5813d9bb536bbdd5dc7fb40c4f571d32dbdb602d209ea"}
{"code": "for (int i = 0; i < 2; ++i) { \n       \n \n \n       \n \n      g = 1; \n      g1 = 1; \n      svar = 3; \n      sfvar = 4.0; \n \n \n \n \n       \n \n \n       \n \n \n \n \n \n \n      [&]() { \n\tg = 2; \n\tg1 = 2; \n\tsvar = 4; \n\tsfvar = 8.0; \n \n      }(); \n    }", "pragma": "teams distribute parallel for private(   g g1 sfvar svar)", "hash": "65f34edd17e8e8970902d33edee00420248798372273ca33f6ba04944a09c634"}
{"code": "for (j06a = 0; j06a < 64; j06a++) \n    for (j06b = 0; j06b < 4; j06b++) \n      ;", "pragma": "target parallel for simd private( j06a j06b)", "hash": "66746a0091e156a43128073b5999354e2ff438f1a0724f4d41a6de67d4d08937"}
{"code": "for(int k=0; k<N; k++) { \n      l = 2*k; \n      a[k] = l; \n    }", "pragma": "teams distribute simd ", "hash": "66d850fab9b2bcb57534207c5a9522a02f5e7dcc9888cc217e415c2d0ff9f53f"}
{"code": "for (i = 0; i < N; i++) \n    yd_dev[i] += a*xd_dev[i];", "pragma": "teams distribute parallel for ", "hash": "67143b994d637b5f49515aa052610f7c733549c3ec1a1b93a41497dc03a8072a"}
{"code": "for (int i = 0; i < N; i++) { \n    arr[i] = 1; \n  }", "pragma": "target simd ", "hash": "67b458670fb20dfeec95e49b77fedb0a75d7288bf29afd47a26b1ab3b637039b"}
{"code": "for (int I = 0; I < 2; ++I) { \n    ThreadLimitL1 = omp_get_thread_limit(); \n#pragma omp parallel reduction(unique32 : ThreadLimitL2) \n    { ThreadLimitL2 = omp_get_thread_limit(); } \n  }", "pragma": "target parallel for reduction(                                                                        : threadlimitl1 threadlimitl2 unique32) ", "hash": "68460cd0a646f22b20589ba22b4d8aa72d8808a985841b266ded0e8ae63cfdd3"}
{"code": "for(int i = 0; i < 10; i++) { \n    b[i] += 1; \n  }", "pragma": "target teams distribute simd ", "hash": "69a60411d554bcacbd225aeea33ae14c2cb978323c335622f34d5092ed772558"}
{"code": "for (int i=0; i < N; ++i) \n    andfc = andfc && rcf[i];", "pragma": "target parallel for simd reduction(&&: andfc) ", "hash": "69b72a07f200742c7f7309927873b45f7ce8c7fab0029a540e5bfe1302239ce2"}
{"code": "for (int i=0; i<200; i++) foo();", "pragma": "target teams distribute parallel for simd ", "hash": "69d07e3960c06b52253b0295f0c4e7f31f96d893fde850663a69a357a0f2a32d"}
{"code": "for (i = 0; i < N; i++) \n    zd_dev[i] = c*(xd_dev[i]-yd_dev[i]);", "pragma": "teams distribute parallel for ", "hash": "6a3f9326ebe463ba96c0d6da39b790b2c256d2ae2eab6c39c57d4016ca7b9077"}
{"code": "for (int i = 0; i < argc; ++i) { \n        foo(); \n        break;  \n \n        continue; \n      }", "pragma": "target parallel for simd ", "hash": "6ae198a5f7ed6763b41547902adf55bf86fea6d6c302a269ddc0f8bc40b8886e"}
{"code": "for (int i = 0; i < 2; ++i) { \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n      g = 1; \n      g1 = 1; \n      svar = 3; \n      sfvar = 4.0; \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n      [&]() { \n\t \n \n\t \n \n\tg = 2; \n\tg1 = 2; \n\tsvar = 4; \n\tsfvar = 8.0; \n\t \n \n\t \n \n\t \n \n\t \n \n \n\t \n \n\t \n \n\t \n \n\t \n \n\t \n \n\t \n \n\t \n \n\t \n \n\t \n \n      }(); \n    }", "pragma": "teams distribute parallel for private(   g g1 sfvar svar)", "hash": "6d33ff33ab6287c29055c0dcaa13ff45bdc37629b5e51c3994afd4cb8aae4302"}
{"code": "for (int i = 24; i < N; i++) \n\ty[i] = i + 3;", "pragma": "target ", "hash": "6dd28287d18c12ec623f4063759fea7655e23824a08a6de1cec96426822ba83d"}
{"code": "for (int i = 0; i < 10; ++i) \n      a += 14;", "pragma": "target parallel for ", "hash": "6df03992d9bc71f7b140ab595a449cc21fd5eeeee263c6e17c062c18290c2a3d"}
{"code": "for (i = 0; i < N; i++) \n    zd_dev[i] = xd_dev[i]*yd_dev[i];", "pragma": "teams distribute parallel for ", "hash": "6df9941d80356d2acf7c1975bd21889fb5306abb0abb383f9b012c6d464176c8"}
{"code": "for (int i = 0; i < array_size; i++) \n  { \n    c[i] = a[i] + b[i]; \n  }", "pragma": "target teams distribute parallel for simd ", "hash": "6e6b4ee204fde65e4534b99d77f18ea8bf5613ffc0106fd6f737431f9f3af499"}
{"code": "for (int i= 0; i < argc; ++i) \n    a = foo(&i) + foo(&a) + foo(&argc);", "pragma": "target teams distribute parallel for ", "hash": "6e8f9b1f2f19885ffe54a2ffecaed465168d2530b26811231526e91a3bb2bb85"}
{"code": "for (i = 0; i < N; i++) { \n    md_dev[i] = ZERO; \n    if (cd_dev[i] == ZERO) continue; \n    if (cd_dev[i] > ONEPT5 || cd_dev[i] < -ONEPT5) { \n      if ( xd_dev[i]*cd_dev[i] <= ZERO) { temp = ZERO; md_dev[i] = ONE; } \n      continue; \n    } \n    if ( cd_dev[i] > HALF || cd_dev[i] < -HALF) { \n      if (xd_dev[i]*cd_dev[i] < ZERO ) { temp = ZERO; md_dev[i] = ONE; } \n    } \n  }", "pragma": "teams distribute parallel for reduction(min:temp) ", "hash": "6ecf0501360847345ccee071012236c9bed68de18b4bafc4fa50b60abd0f81a7"}
{"code": "for (int i = 0; i < argc; ++i) { \n    goto L1;  \n \n    argc++; \n  }", "pragma": "target parallel for simd ", "hash": "6efc3c3e56437ee9c90373957066e410260c115aa77f0e8ff49f1505486ed78f"}
{"code": "for (int i=0; i < N; ++i) \n    anddc = anddc && rcd[i];", "pragma": "target parallel reduction(&&: anddc) ", "hash": "6f7fd664a9c67e89a54955bb504223746e195a43e725c93d7ab4788673652e0a"}
{"code": "for (short it = 6; it <= 20; it-=-4) { \n    a += 1; \n    aa += 1; \n  }", "pragma": "target simd ", "hash": "70d9381091c215fae9cb97fe6f767bb654d13ce3be307f198f5fbc9f6e4f8d90"}
{"code": "for (int i = 0; i < 2; ++i) \n    for (int j = 0; j < 2; ++j) \n      for (int j = 0; j < 2; ++j) \n        for (int j = 0; j < 2; ++j) \n          for (int j = 0; j < 2; ++j) \n            foo();", "pragma": "target parallel for simd reduction(+ : h) private(   argc b c d d f)", "hash": "71d1c221d4d7a00bf8e152bbfa337230e604481d6e6f5d5d8370f1c823376293"}
{"code": "for (int k = 0; k < argc; ++k) { i = k; v += i; }", "pragma": "target teams distribute parallel for simd ", "hash": "71d8faa65084ece9d41b7337a2078120c6aebf424410e8135be4e84dd60cb0fb"}
{"code": "for (int i = 0; i < 10; ++i) \n    ++argc;", "pragma": "target parallel for simd ", "hash": "734a0c35f785dd33d475cf0f11ddc09010578222eb03bb77059b5c13fdf29336"}
{"code": "for (i = 0; i < N; i++) \n    zd_dev[i] = -xd_dev[i];", "pragma": "teams distribute parallel for ", "hash": "735950dba9d233d5f63ef8661e6939749c99041b3ec0aa337036e298c95b6a0e"}
{"code": "for (GoodIter I(0); I < end; ++I)  \n \n    ++I;", "pragma": "teams distribute ", "hash": "736ffc6c3782b4da0071e2e4ff220cef6b2decd0de2a9f7c97882cb2cb546819"}
{"code": "for (int i = 0; i < 10; ++i) { \n    int &f = c[1][1][1]; \n    int &g = a; \n    int &h = b[1][1]; \n    int d = 15; \n    a = 5; \n    b[0][a] = 10; \n    c[0][0][a] = 11; \n    b[0][a] = c[0][0][a]; \n    bb = b[0][a]; \n  }", "pragma": "target parallel for ", "hash": "738e459cb6f798ca1207f56122812269c4b06ab385f4087d4f580065a9ab3e40"}
{"code": "for (int i = 0; i < N; i++) { \n    floatType tmp = 0; \n    #pragma unroll 1 \n    for (int j = 0; j < length[i]; j++) { \n      int k = j * N + i; \n      tmp += data[k] * x[index[k]]; \n    } \n    y[i] = tmp; \n  }", "pragma": "target teams distribute parallel for simd ", "hash": "7420758a489055c8d830eaabed67aecacec1ceca27e244e123ba0eae4069a04b"}
{"code": "for (int i = 0; i < size; i++) \n    output[0] += input[i];", "pragma": "target teams distribute parallel for reduction(+: output[0]) ", "hash": "74336f4cb7829340b39f29afcaaaeaa8c369e6d4c87c67894bb8941ca7be283e"}
{"code": "for (int i = a; i < n; ++i) { \n    a += 1; \n    aa += 1; \n    aaa += 1; \n    b[2] += 1; \n  }", "pragma": "target teams distribute simd ", "hash": "745004ce9f430454428a98be9ad09cb1e8ba5c9bd1c7b61e2bfa66e3e9949f60"}
{"code": "for (int i = 0; i < 2; ++i) { \n \n     \n \n \n    g = 1; \n    g1 = 1; \n    sivar = 2; \n \n     \n \n    [&]() { \n      g = 2; \n      g1 = 2; \n      sivar = 4; \n \n    }(); \n  }", "pragma": "teams distribute parallel for simd private(  g g1 sivar)", "hash": "747e34b6f92cbfe002990b8cf872baf96d06468db700ba20c957413b0998c7f5"}
{"code": "for (j07 = 0; j07 < 64; j07++) \n    ;", "pragma": "target simd ", "hash": "7499ea909c55dbe760ed47f23c5576b0b54901b40c3758a91b7325058566fa38"}
{"code": "for (int i = 0; i < 2; ++i) { \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n    g = 1; \n    g1 = 1; \n    sivar = 2; \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n    [&]() { \n       \n \n       \n \n      g = 2; \n      g1 = 2; \n      sivar = 4; \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n    }(); \n  }", "pragma": "target teams distribute simd private(  g g1 sivar)", "hash": "74eb826fccff52da5398075fcd0b78ecf3d9d1a100a9a37b361aec981e45578e"}
{"code": "for (int I = 0; I < 2; ++I) { \n    MaxThreadsL2 = omp_get_max_threads(); \n  }", "pragma": "target parallel for reduction(: maxthreadsl2 unique) ", "hash": "754a640b7a9467201f22f4d3f26a7748ac73462e9658b2eba26d6c6da4c61bba"}
{"code": "for (T i = 0; i < 2; ++i) {}", "pragma": "target parallel for simd reduction( +:c arr1[argc]) reduction( arr[:n][0:10] max:e) private(argc argv b)", "hash": "75817424e05abdda0cc48b4c9a88eedaf074467eed6253002deb524c8390d781"}
{"code": "for (i = 0; i < 64; i++) \n    { \n      d[i] = a; \n      #pragma omp scan exclusive (a)  \n \n      a += c[i]; \n    }", "pragma": "target parallel for reduction( +: a inscan) ", "hash": "76090428ae921bdde2bc902d7ead3ca497e5ae4b53bbd54159d5983fcfa44236"}
{"code": "for (int j07 = 0; j07 < 64; j07++) \n    ;", "pragma": "target simd ", "hash": "772cf29cc247403b88a21ed809028657af2d4d4bc5b2a59d0bb954ab4f06c5e6"}
{"code": "for (i = 0; i < 64; i++) \n    p[0]++;", "pragma": "teams distribute parallel for reduction(+: p[:2]) ", "hash": "781765fcfe0eb4a0c87fc7fabab37448a76f33d71b702f5cac61d680068302ff"}
{"code": "for(i = 0; i < X; i++) { \n      a[i] = (T)0; \n    }", "pragma": "teams distribute parallel for simd ", "hash": "78a8e81c7ffb470645a2aa11a37d6953743259656a431d15fad2f20efe0161cb"}
{"code": "for(int i = 0; i < X; i++) { \n      a[i] = (T)0; \n    }", "pragma": "teams distribute parallel for simd ", "hash": "78c4f07e0675274f2ed3a39bb6dcd88c6bd604e355581752c99291626b9fb955"}
{"code": "for (j=0; j<N; j++) { \n      zd_dev[j] = c[0] * xd_dev[j]; \n    }", "pragma": "teams distribute parallel for ", "hash": "7a380ddb4735a1048acaa0e08e1a5a2271614ff679967d55bc6a001fe8905d03"}
{"code": "for (i = 0; i < N; i++) { \n    if (xd_dev[i] == ZERO) \n      val = ONE; \n    else \n      zd_dev[i] = ONE/xd_dev[i]; \n  }", "pragma": "teams distribute parallel for reduction(max:val) ", "hash": "7b0a9cb270f161fac1a0e5af20e3805ff6e79eae2796b4e8f829574daaf68d76"}
{"code": "for (int i1 = 0; i1 < 16; ++i1) \n    for (int i2 = 0; i2 < 16; ++i2) \n      for (int i3 = 0; i3 < 16; ++i3) \n        for (int i4 = 0; i4 < 16; ++i4) \n          foo();", "pragma": "target parallel for ", "hash": "7b1f3f9d8af7d1c4e975a4907fad1ec98c183d939a17568df17b3098d67bd8a9"}
{"code": "for (int i = 0; i < 10; ++i) { \n    a += 1; \n    b[2] += 1.0; \n    bn[3] += 1.0; \n    c[1][2] += 1.0; \n    cn[1][3] += 1.0; \n    d.X += 1; \n    d.Y += 1; \n  }", "pragma": "target teams distribute ", "hash": "7b2d2e5f903b5a6f5c57431b6753afbc45ad7fe657606a5525f6e78bbbbb1a28"}
{"code": "for (unsigned char it = 'z'; it >= 'a'; it+=-1) { \n    a += 1; \n    b[2] += 1.0; \n    bn[3] += 1.0; \n    c[1][2] += 1.0; \n    cn[1][3] += 1.0; \n    d.X += 1; \n    d.Y += 1; \n  }", "pragma": "target simd ", "hash": "7b76f367bbe5b2654a4a7664bb334a1f57f17642e4f8223e3df038e2d8c7b4a5"}
{"code": "for(int k=0; k<N; k++) { \n      n = k; \n      a[k] = n; \n    }", "pragma": "teams distribute simd private(n)", "hash": "7cf065b500f8bf453f58be8a186dbc4f4e82a621c99f991bde4fca77c5d973e9"}
{"code": "for(int k=0; k<N/4; k++) \n      for(int l=0; l<4; l++) \n        a[k*4+l] = k*4+l;", "pragma": "teams distribute simd ", "hash": "7df96ff7fcc3be6097b8fa58684f3652b2021a1185b52addd4cf21ea011fe2ae"}
{"code": "for (int i=0; i < N; ++i) \n    ands = ands && rci[i];", "pragma": "teams distribute parallel for simd reduction(&&: ands) ", "hash": "80bc93b64cbf3b261b78463f7804538550589bd0d310a7fc4d2105014c2c8782"}
{"code": "for (IT I = begin; I < end; I = I + ST) {  \n \n      ++I; \n    }", "pragma": "teams distribute ", "hash": "80d877cd59241e3bd5f2cbc8d4aafa447bde666b2b6ca1d0a1c8583dafece125"}
{"code": "for (int i = 0; i < 200; i++) { \n    ++x; \n    ++y; \n  }", "pragma": "target teams distribute ", "hash": "80faa256525a187d0d7c013e96396eae3d1839abd563b75b926864ec82c1b233"}
{"code": "for(k=0; k<100; k++) { \n    if (k > 1){ \n      a[k] = a[k-2] + 2; \n    } \n    else{ \n      a[k] = k; \n    } \n  }", "pragma": "target parallel for simd ", "hash": "81158a528529880dc544174dcfefe63be4b4a37b94c7c6bb9455662f829525dd"}
{"code": "for (int i = 0; i < argc; ++i) { \n    return 1;  \n \n  }", "pragma": "target parallel for simd ", "hash": "8117b03b6c12c21e0558bac41db4e8ef94d29a45119a3507fe667499d28e8196"}
{"code": "for (int i = 0; i < 10; ++i) \n      a += 4;", "pragma": "target parallel for ", "hash": "81c68480d45fe358ba2288539ebadd1ec1bb669fcbe70dcde53fa10335f72e50"}
{"code": "for (sii = 0; sii < 10; sii++) \n    c[sii] = a[sii];", "pragma": "target teams distribute simd ", "hash": "81d4392e2d4bb9e567634806a8643716cf77c2b3bee13e2618e4cf706b1b9477"}
{"code": "for (int i = 0; i < N; i++) \n    if (a.ptr[i] != 2) \n      abort ();", "pragma": "target ", "hash": "822a8fae1d4d1a20c148d1ff70c60a9c0114f96b36112cbbe814357fd558dcff"}
{"code": "for (long long i = -10; i < 10; i += 3) { \n    a += 1; \n    aa += 1; \n    b[2] += 1; \n  }", "pragma": "target parallel for ", "hash": "83ce4ce17cd34b0cf066eec8d5d33a323b997141f0ee0439b6de5eb8f22ca490"}
{"code": "for (int i = 0; i < count; ++i) ptr[i] = 0;", "pragma": "target teams distribute parallel for ", "hash": "840085ba1e1cd774952f7f2be463ffb26a06171c0997016f7d96c61e17f228a1"}
{"code": "for (i = 0; i < 10; i++) \n\t{ { \n\t  for (j = 0; j < 10; j++) \n\t    if (b) \n\t      bar (); \n\t    else \n\t      baz (); \n\t} }", "pragma": "target simd ", "hash": "85f8ca5244d96444c4a77116126b2481e07677ef2f97c0c0c0e5ebcc73673fa3"}
{"code": "for (int i = 0; i < 2; ++i) { \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n      g = 1; \n      g1 = 1; \n      svar = 3; \n      sfvar = 4.0; \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n      [&]() { \n\t \n \n\t \n \n\tg = 2; \n\tg1 = 2; \n\tsvar = 4; \n\tsfvar = 8.0; \n\t \n \n\t \n \n\t \n \n\t \n \n \n\t \n \n\t \n \n\t \n \n\t \n \n\t \n \n\t \n \n\t \n \n\t \n \n\t \n \n      }(); \n    }", "pragma": "teams distribute private(   g g1 sfvar svar)", "hash": "85fd450a3dbd84062907558d9f905005f094bdd5c2eebce154e2e09cd425f6fa"}
{"code": "for (int i = 0; i < n; i++) { \n    aa[i] += 1; \n  }", "pragma": "target simd ", "hash": "8614c1e222b8bc56cbbb2a1bb387975087f8e52c696c022e7d8cb62163808229"}
{"code": "for (int i=0; i < N; ++i) \n    andi = andi && rcs[i];", "pragma": "teams distribute parallel for reduction(&&: andi) ", "hash": "86cfaa514abb46418f896845b6ded17e5dae9642f5af790ce577df8aa010c57e"}
{"code": "for (int i = 0; i < n; ++i) { \n\t      a[i] += b[i] + c[i]; \n\t    }", "pragma": "target teams distribute parallel for simd ", "hash": "87f9f71ddf1255e517ae7aeb41daa4d912aafaf6fbd46e9b176dbba04a2d58db"}
{"code": "for (int i = 0; i < 2; ++i) { \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n    g = 1; \n    g1 = 1; \n    sivar = 2; \n     \n \n     \n \n     \n \n     \n \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n    [&]() { \n       \n \n       \n \n      g = 2; \n      g1 = 2; \n      sivar = 4; \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n    }(); \n  }", "pragma": "target teams distribute parallel for simd private(  g g1 sivar)", "hash": "895a97611d43383e7f748760b4ff6efdb1aa4caa1f92df40f0420bf380e4f4ea"}
{"code": "for (int i = offset; i < offset + length; i++) { \n      y[i] += a * x[i]; \n    }", "pragma": "teams distribute parallel for simd ", "hash": "89c1303b9492c0eb80d8ead3b82397d36e44064062db2f06b5bab9cd50bad5c4"}
{"code": "for (int i = 10; i < 10 + 4; i++) \n\t  if (p[i] != 997 * i) \n\t    ok = 0;", "pragma": "target ", "hash": "8d1f3a444ecc7d158cb4aa3d5fd5c3b4ce7da8fa7fc87805b1c31448cbfb7925"}
{"code": "for(int i = 0; i < M; i++) { \n    for(int j = 0; j < M; j++) { \n      k = M; \n      c[i][j] = i+j*f+k; \n    } \n  }", "pragma": "target teams distribute parallel for simd private(f k)", "hash": "8d5c948805c32f102b1567b36d6e4b9f12c2ea9c0e477eecb0066bb2a02f7909"}
{"code": "for (int k = 0; k < 10; ++k) \n    e += d + argc + arr[k];", "pragma": "teams distribute parallel for simd ", "hash": "8d6f611a012c53ea3faea76e2b17bdc32398eb00b9560c8f267fd2b7fb6633d1"}
{"code": "for (int i = 0; i < 10; i++) { \n    b.c.a += ++c[i].a; \n  }", "pragma": "target ", "hash": "8f7772bfabdbdcfcf558e792186515d30e1aab84ed306e06e9ed24165b406120"}
{"code": "for (int d = 0; d < getNumberOfDevices(); d++) { \n    for (int src = 0; src < getNumberOfDevices(); src++) { \n      if (src == d) { \n         \n \n        continue; \n      } \n      int offset = workDistribution->offsets[src]; \n      int length = workDistribution->lengths[src]; \n \n      #pragma omp target update nowait device(d) to(x[offset:length])                                         depend(in: x[offset:length]) \n    } \n  }", "pragma": "target ", "hash": "8fe443b21931f368583dda22e37a33fbf8bb90b7187748798741acb17c05744f"}
{"code": "for (int j = 0; j< N; j++) { \n    num_threads[j] = omp_get_num_threads(); \n    num_teams[j] = omp_get_num_teams(); \n  }", "pragma": "target teams distribute parallel for ", "hash": "900f192db17b383df1d73d0116be029287b202c0e2da9dcb9fa1dcccb9b79146"}
{"code": "for (int i = 0; i < 10; i++) { \n    c[i] = a[i] + b[i]; \n    try {  \n \n      for (int j = 0; j < 10; ++j) { \n        if (a[i] > b[j]) \n          throw a[i];  \n \n      } \n      throw a[i];  \n \n    } catch (float f) { \n      if (f > 0.1) \n        throw a[i];  \n \n      return;  \n \n    } \n    switch (i) { \n    case 1: \n      b[i]++; \n      break; \n    default: \n      break; \n    } \n    for (int j = 0; j < 10; j++) { \n      if (c[i] > 10) \n        throw c[i];  \n \n    } \n  }", "pragma": "target teams distribute simd ", "hash": "91a1a92839c655e583f812aecfec26053f8cf53e5a9b442d40e9411fa173b87d"}
{"code": "for (int i=0 ; i<globalWI; i++) {}", "pragma": "target teams distribute ", "hash": "91c1a97f0b5a685e657362eec9a83ffe25919762786f39c30835a5b587ead6aa"}
{"code": "for(int i = 0 ; i < omp_get_num_teams() ; i++) { \n    lastpriv[0] = omp_get_team_num(); \n  }", "pragma": "target teams distribute private(lastpriv)", "hash": "92bc38ed4dd89ca9aff827df01bff6b980d882aca3aa6cea3b7675c1f37b85b0"}
{"code": "for (i = 0; i < 10; ++i) \n  { \n#pragma omp parallel \n    ++i; \n  }", "pragma": "target teams distribute ", "hash": "937ba22122fd4d00d6addac8e57bf0f8a83ed9081ba059b6c8367f6c80c44bcd"}
{"code": "for (i = 0; i < 16; ++i)  \n \n    for (int j = 0; j < 16; ++j)", "pragma": "target teams distribute parallel for simd private(i)", "hash": "93958843e6d74f3acb266baec5861094af32fdea5e718627b1806aa31ef10567"}
{"code": "for (int i = 0; i < 64; i++) \n    r15++;", "pragma": "target parallel for simd reduction(+:r15) ", "hash": "93dcdff40a7a512a0eb35211b5e9f879b1f5bf7424737e697cbeec713a510ade"}
{"code": "for (i = 0; i < N; i++) \n      yd_dev[i] -= xd_dev[i];", "pragma": "teams distribute parallel for ", "hash": "9457dcaeda01b0ce548da80b0e1f65b0749cf4919eb090cbf855943524e7120d"}
{"code": "for (int i = 0; i < 10; ++i) \n      a += 17 + x;", "pragma": "target parallel for ", "hash": "949c7350f26e9e427cd9564c3163b0c379c064f3050d4dcfe4ec5122ad867c40"}
{"code": "for (int i=0; i < N; ++i) \n    orl = orl || rcl[i];", "pragma": "teams distribute parallel for simd reduction(orl ||:) ", "hash": "9501ec3e469942b53b03c0ba0bd0894baeece6e806210aed86a256c1d53cdd08"}
{"code": "for (int i=0; i < N; ++i) \n    andf = andf && rf[i];", "pragma": "target parallel reduction(&&: andf) ", "hash": "953e26333caf2264b19e98e67d5ee301f0c83e3a10dfb3e98ad1ebb08d8c9a5b"}
{"code": "for (i = 0; i < 10; i++) \n\t{ { \n\t  for (j = 0; j < 10; j++) \n\t    if (b) \n\t      bar (); \n\t  } \n\t}", "pragma": "target simd ", "hash": "95f0bc8f150578283d3db8c1d013a4baa4563ff8ef628251d7ede3cfd490ac5f"}
{"code": "for (int i = 0; i < 1; i++) \n    j = v;", "pragma": "target simd private(j v)", "hash": "95fd59c2628989a684cac833f8abfe556191094c57f0512392b7c18aa61f29e6"}
{"code": "for (int k = 0; k < a.a; ++k) \n      ++a.a;", "pragma": "teams distribute reduction(+:c) reduction(max:e) private(argv b)", "hash": "9639156779f4259eab161a39d861db901305d521b0810876a6ac827c37ceb61a"}
{"code": "for(int i = 0; i < n; i++) { \n    a[i] = 0; \n    #pragma omp cancel for \n  }", "pragma": "teams distribute parallel for ", "hash": "9642ca874d34d90906965cecec7ac92f362a1f06ab2d0f5efec5c4085802f050"}
{"code": "for (int i = ST; i < N; i++) \n \n    argv[0][i] = argv[0][i] - argv[0][i-ST];", "pragma": "teams distribute ", "hash": "96667cf052d7118b842ebeb420ca1682c7a9c461c963a8b0022a78acd26279ce"}
{"code": "for(int j = 0 ; j < 256 ; j += blockSize) { \n      #pragma omp parallel for \n      for(int i = j ; i < j+blockSize; i++) { \n        A[i] += B[i] + C[i]; \n      } \n    }", "pragma": "teams distribute ", "hash": "97508d4bff47a08058c8eb50efbcf850779ed26f128a11bde40bd293ce7c2028"}
{"code": "for (int i = 0; i < argc; ++i) { \n    ++x; \n    ++y; \n  }", "pragma": "target teams distribute parallel for simd ", "hash": "97770801542e47c9d18591c1cdd737068402c31623c99360b7f9ab829cf64aac"}
{"code": "for (int i = ST; i < N; i++) \n     argv[0][i] = argv[0][i] - argv[0][i-ST];", "pragma": "teams distribute simd ", "hash": "97d49caa323e1a8c722e234e588146b933a5329a369aa16ac0f5adf408a9c9bd"}
{"code": "for (int i = 0; i < array_size; i++) \n  { \n    b[i] = scalar * c[i]; \n  }", "pragma": "target teams distribute parallel for simd ", "hash": "97f016ee6c7b11f2b7627da98c8dd171fcde88c34a1a62e2a090120cd010986d"}
{"code": "for (int i = 0; i < 2; ++i) foo();", "pragma": "teams distribute private(a)", "hash": "9957c06e595cba8cd8851fb03afac8f0d9c285ae606a141a25c9bd8009f72924"}
{"code": "for (int i = 0; i < 2; ++i) { \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n      g = 1; \n      g1 = 1; \n      svar = 3; \n      sfvar = 4.0; \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n      [&]() { \n         \n \n         \n \n        g = 2; \n        g1 = 2; \n        svar = 4; \n        sfvar = 8.0; \n         \n \n         \n \n         \n \n         \n \n \n         \n \n         \n \n         \n \n         \n \n         \n \n         \n \n         \n \n         \n \n         \n \n      }(); \n    }", "pragma": "target teams distribute private(   g g1 sfvar svar)", "hash": "9b1a82483a686a634143e81c5752355ed2e36cd1e393ee2987170dc4321f8be7"}
{"code": "for(i = 0; i < n; i++) { \n    a[i] = 0; \n  }", "pragma": "teams distribute simd ", "hash": "9b29c1ef808a98b54b8dbd62fda95fef83efebca352e95e7199d9de0ccd6de9f"}
{"code": "for (int i = ST; i < N; i++)  \n    argv[0][i] = argv[0][i] - argv[0][i-ST];", "pragma": "teams distribute simd ", "hash": "9b81050b719a210346ca6b30d1a61df218b8daf491bff1406aaedd6144c1b576"}
{"code": "for (T i = 0; i < 2; ++i) \n    a = 2;", "pragma": "target parallel for simd ", "hash": "9c2d5626bbaf3bc89619ae4d42e9f84fb3b8fcf29968a5abf75df7c15cfa0015"}
{"code": "for (begin = end; begin < end; ++begin)  \n \n    ++begin;", "pragma": "teams distribute ", "hash": "9dea363c5407c2614a8bb3b43876e7102663516a4082a57f8a1a28752076aa92"}
{"code": "for (int k = 0; k < 10; ++k) \n      e += d + argc;", "pragma": "teams distribute parallel for simd reduction(+:c) reduction(max:e) private(argc b)", "hash": "9e648867777f7f567167552c9e5abf4bee577b57e92bdcaeabe8861d085bd227"}
{"code": "for(int i = 0; i < n; i++) { \n    a[i] = (T)0; \n  }", "pragma": "teams distribute parallel for simd ", "hash": "a060de87e73c6a7f9460a0890d25a00bc64968e3006eebfc6e016b1469ca9dea"}
{"code": "for (int i = 0; i < 10; ++i) { \n    aa += 1; \n  }", "pragma": "target teams distribute ", "hash": "a13e5163ea4b7cce8502198b4abaf182b520d9fe690b3d85f98bf557a8efcf02"}
{"code": "for (i = 0; i < N; i++) \n    zd_dev[i] = ONE/xd_dev[i];", "pragma": "teams distribute parallel for ", "hash": "a14d8f246680c7a45fbc6b6b737953b673ec336ba8080c7ae76c01db69dd50b5"}
{"code": "for (int i = 0; i < 2; ++i) { \n    vec[i] = t_var; \n    s_arr[i] = var; \n    sivar += i; \n  }", "pragma": "target teams distribute parallel for simd private(    s_arr sivar t_var var vec)", "hash": "a18d8d3ec9b27cb56ab137e2b56619d5009c427eba9de74b57f8704879fc3d14"}
{"code": "for (i = 0; i < bufsize; i++) { \n        k = (int)omp_get_team_num(); \n         \n \n         \n \n        unew[i] += (omp_get_team_num()+1) * omp_get_thread_num(); \n    }", "pragma": "teams distribute parallel for private(k)", "hash": "a233298b374c997a70f3f9333f30da25119f8092449ec29f8ea2decd27277126"}
{"code": "for (k = 0; k < s.a.a; ++k) \n      ++s.a.a;", "pragma": "teams distribute parallel for simd private(a this->a)", "hash": "a2b53353439da2be4b16d1ac93dd39a72010df5c1186c6c0704775ece4b56693"}
{"code": "for (int i = ST; i < N; i++) \n    argv[0][i] = argv[0][i] - argv[0][i-ST];", "pragma": "teams distribute simd ", "hash": "a5e1dc179ec36dc5464034ee35cfab36bcf86d9733a577b028883bba9c6a0e86"}
{"code": "for (i = 0; i < N; i++) { \n    sum += xd_dev[i]*yd_dev[i]; \n  }", "pragma": "teams distribute parallel for reduction(+:sum) ", "hash": "a677f8fa3c48c9af12da8e1b952c9e226b8a10e98803939c043a090ce1ea9f74"}
{"code": "for(int i = 0; i < M; i++) { \n    for(int j = 0; j < M; j++) { \n      k = M; \n      c[i][j] = i + j * f + k; \n    } \n  }", "pragma": "target teams distribute simd private(f k)", "hash": "a6782ca8e01f04a1d880b55082a5d8c8f9d51fc63f9f443e8e3febe3f68650fc"}
{"code": "for (int i = 0; i < array_size; i++) \n  { \n    a[i] = b[i] + scalar * c[i]; \n  }", "pragma": "target teams distribute parallel for simd ", "hash": "a6df738dab58083a3ca8978c268a2f217482bdf2da5a0b1a1d76b9914938fb98"}
{"code": "for (int i = 0; i < 64; i++) \n    r23++;", "pragma": "target simd reduction(+:r23) ", "hash": "a884f0e0653babdf06ebd5716759aba06c33b76641044d327d7e9cf72e616129"}
{"code": "for (int i = 0; i < 2; ++i) { \n     \n \n     \n \n     \n \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n    g = 1; \n    g1 = 1; \n    sivar = 2; \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n    [&]() { \n       \n \n       \n \n      g = 2; \n      g1 = 2; \n      sivar = 4; \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n    }(); \n  }", "pragma": "teams distribute simd private(  g g1 sivar)", "hash": "a8d531e3bd642227af12d8e73dd8327921b49dabe89dd70314a9e80d2aab7076"}
{"code": "for (int i=0; i < N; ++i) \n    andd = andd && rcd[i];", "pragma": "target parallel for reduction(&&: andd) ", "hash": "a8eb599d9d93c092e803ae28d7278b00a589c618c19daf897c0f7f51db9ce535"}
{"code": "for (int i = 0; i < 100; i++) { \n    Arg = 0; \n  }", "pragma": "target teams distribute parallel for simd ", "hash": "a957e63b038ed1b0e5ced4d21430c1be7d34557fa949eccf2cb6e869270a6e15"}
{"code": "for (int j11 = 0; j11 < 64; j11++) \n    ;", "pragma": "teams distribute parallel for simd ", "hash": "aa3fa154f81f743c81f6ca7bb2d1ea0e4d5ad993f6356aa7863528fc53ea3820"}
{"code": "for (int i = 0; i < 2; ++i) {}", "pragma": "target parallel for simd reduction( +:c arr1[argc]) reduction( arr[:5][0:10] max:e) private(argc argv b)", "hash": "aaf45de9484757d8c17be6812d0cffd51abd16afa1226a0f63cc3fc2044276f3"}
{"code": "for (int i = 0; i < 10; ++i) \n    argc = x;", "pragma": "target teams distribute ", "hash": "ab183a1d18cfedb468a59e1327308d4766f0a529b866749ea40bc5b2688631bb"}
{"code": "for (i = 0; i < 10; i++) \n\t{ \n\t  if (b) \n\t    bar (); \n\t}", "pragma": "target simd ", "hash": "ab62b116d247ae669e43653e9608ab1a939e3732e88f23701c1721f2ce8aabd2"}
{"code": "for (i = 0; i < N; i++) \n    zd_dev[i] = xd_dev[i]/yd_dev[i];", "pragma": "teams distribute parallel for ", "hash": "abec46183716d339443f9640ee25963b0d44f4afd994f5aab4ca1159db3752df"}
{"code": "for (int i = 10; i > 1; i--) { \n    a += 1; \n  }", "pragma": "target simd ", "hash": "acca9861a862fa4311ba1d2ad65118f20caa13c029c5bcb0889eb6a9ea7e0d23"}
{"code": "for (int i = 0; i < 10; i++) { \n    c[i] = a[i] + b[i]; \n    try { \n      for (int j = 0; j < 10; ++j) { \n        if (a[i] > b[j]) \n          throw a[i]; \n      } \n      throw a[i]; \n    } catch (float f) { \n      if (f > 0.1) \n        throw a[i]; \n      return;  \n \n    } \n    switch (i) { \n    case 1: \n      b[i]++; \n      break; \n    default: \n      break; \n    } \n    for (int j = 0; j < 10; j++) { \n      if (c[i] > 10) \n        throw c[i]; \n    } \n  }", "pragma": "target parallel for ", "hash": "ad5d6a60184db06d0a949e4e499608799b06b7150409c3c0999a62945ed66d8d"}
{"code": "for (i = 0; i < N; i++) zd_dev[i] = c;", "pragma": "teams distribute parallel for ", "hash": "ad7c5fed59b38e330e200fa5348f1ce619d2af9f74e1195612d5199a1838f7a9"}
{"code": "for(int i = 0; i < n; i++) { \n    a[i] = g[0]; \n  }", "pragma": "target teams distribute parallel for simd ", "hash": "ad839bd8e39b85e7587ec80efebda0963baec1214b72d906bb6fccc2cda35962"}
{"code": "for (int j = 0; j < N; j++) \n\tarray[i][j] = i * 10 + j;", "pragma": "target ", "hash": "ade42b204a89ceb2e56c90d267dc616e70c0d6943b9461b4acb14bd733269c8a"}
{"code": "for(int j = 0 ; j < 510 ; j += blockSize) { \n      int ub = (j+blockSize < 510) ? (j+blockSize) : 512; \n      for(int i = j ; i < ub; i++) { \n        A[i] += B[i] + C[i]; \n      } \n    }", "pragma": "teams distribute simd ", "hash": "ae08e8d1602a3e29b7555776b1fe80f654f16c2d99d31bcc63ac565855950611"}
{"code": "for (int i = 0; i < argc; i++) { \n    ++x; \n    ++y; \n  }", "pragma": "target parallel for simd ", "hash": "aed0b0aceda7c938ec0afba4ec9a536da7a03e5455de21e12e499acd72b4f2e3"}
{"code": "for (int i = 0; i < 10; ++i) \n    ++a;", "pragma": "target teams distribute parallel for simd ", "hash": "af1f697c9709bb957b19bd0ef8a3db0d8aeee4b0bc8078319727ee9c4f385ea0"}
{"code": "for(int i = 0; i < X; i++) { \n      a[i] = (T)b; \n    }", "pragma": "teams distribute simd ", "hash": "af4bece7a2617649e3ed66b99004ed357fb819e3fb1ed6174902e9eb47525195"}
{"code": "for (int i = 0; i < 22; i++) \n        int a_var;", "pragma": "target teams distribute parallel for ", "hash": "af5f261f6336efbd0ca0bbe999ae647034ff0339afda8ba2a6bb3c211604250e"}
{"code": "for (int i = 0; i < 10; ++i) \n    ++fp1;", "pragma": "target private(fp)", "hash": "afcd2e2f71fc1be210a7319d85f351ef19274bf0802acb622d8cb02b218d000d"}
{"code": "for (int i = 0; i < 10; ++i) \n        ++a;", "pragma": "target teams distribute parallel for simd ", "hash": "b1840e135cd1d37dc9774128c62faff5577e04cafddcc02e85b479b545391494"}
{"code": "for (i = 0; i < N; i++) \n    zd_dev[i] = (a*xd_dev[i])-yd_dev[i];", "pragma": "teams distribute parallel for ", "hash": "b1d41d89efb127ed4c507ae7478e2c3d542e29c748002111de986f33064f57ce"}
{"code": "for (int i = 0; i < 100; ++i) { \n    body(i); \n  }", "pragma": "target teams distribute parallel for ", "hash": "b1eb2b73e2c78aacf657df3dc6ddb0d2ccbaa79819ffe027659d9ca176e71b39"}
{"code": "for (int i = 0; i < 10; ++i) { \n  }", "pragma": "target parallel for simd ", "hash": "b24813e03cd0b794279a67d9f758feb8aa6b032da32d1006ad8b64a601eafd08"}
{"code": "for (int i = 0; i < 10; ++i) ;", "pragma": "target teams distribute parallel for simd ", "hash": "b444a561e1e3981e2bd47a89c6fd2854d39ad8086bc4024ad5c7b5534ef6282b"}
{"code": "for (int i = 0; i < 2; ++i) { \n       \n \n       \n \n \n \n \n      g1 = 1; \n      svar = 3; \n      sfvar = 4.0; \n \n \n \n       \n \n       \n \n \n \n \n \n \n \n \n      [&]() { \n        g = 2; \n        g1 = 2; \n        svar = 4; \n        sfvar = 8.0; \n \n      }(); \n    }", "pragma": "target teams distribute parallel for simd private(   g g1 sfvar svar)", "hash": "b594eb542c933c488c4ade1e11636a06087ed1870d9d31327d026fcf678cc020"}
{"code": "for (i = 0; i < N; i++) \n    zd_dev[i] = (a*xd_dev[i])+yd_dev[i];", "pragma": "teams distribute parallel for ", "hash": "b5e13db001647f98f6b235a91a148042af72f7a7b15e5c24130be41d32b4c2e7"}
{"code": "for (j=0; j<NN; j++) \n\t    c[j] = a[j];", "pragma": "target teams distribute parallel for ", "hash": "b619e114196000bcc9680f76de5ffdecd6564f286e3d89c13ffafe579bf6619b"}
{"code": "for(int i = 0 ; i < 100; i++) { \n   \n \n   \n \n   \n \n   \n \n   \n \n   \n \n   \n \n   \n \n   \n \n   \n \n   \n \n   \n \n   \n \n    gtid_test(); \n  }", "pragma": "target teams distribute parallel for simd ", "hash": "b64ce4967f0850c45eb66c22fa46ba0e205baabd9ed2b16da107019b8c6c14e9"}
{"code": "for (int i = 0; i < 64; i++) \n    l05++;", "pragma": "target parallel for ", "hash": "b6b207f9713c92a6f147f85d22d1db868213475bfffd632731708fa17456a81a"}
{"code": "for (int i = 0; i < 200; i++) \n    ++x;", "pragma": "teams distribute simd ", "hash": "b7152e97e01d2c1aac37a6a58b94996875059f9a2a86dd4aee96e95c2063093f"}
{"code": "for (int i = 0; i < 64; i++) \n    l06++;", "pragma": "target parallel for simd ", "hash": "b7dde4676ef7f8a925c23cc8a8229ebc3f4a58f1b9281a1110d01c5bd5adf953"}
{"code": "for (int i = 0; i < N; i++) \n    a.ptr[i] += 1;", "pragma": "target ", "hash": "b8adbdab8f449ffba885567b0ff64c1264adb06b0def90bc05f597ae553e3ed0"}
{"code": "for (int i = 0; i < N; i++) \n\ty[i] = i;", "pragma": "target ", "hash": "b901734e2ef7a950dd9e43da32d6410054d1099a5b4f17e80756a6f12e95b342"}
{"code": "for(int i = 0 ; i < N ; i++) { \n      t.s->a[i] = i; \n    }", "pragma": "teams distribute parallel for ", "hash": "b95338848b8992095b6133a3e383c117aaaef8083f99a2d43bd71cad1ded8867"}
{"code": "for (i = 0; i < argc; ++i) { \n    ++x; \n    ++y; \n  }", "pragma": "target parallel for ", "hash": "ba5bd37f0fc56b89fea9e9d5c61f9a6c8474cfe33af7eade3ff68b4c53435fd2"}
{"code": "for (int i = 0; i < 2; ++i) { \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n \n     \n \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n \n     \n \n      \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n \n    sivar += i; \n \n    [&]() { \n       \n \n       \n \n \n      sivar += 4; \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n    }(); \n  }", "pragma": "target teams distribute parallel for reduction(+: sivar) ", "hash": "babf9d14b92f527afa41a465547a649b27da3eb0a32d910bc0c423e48871ba25"}
{"code": "for (int i = 0; i < N; i++) { \n    floatType tmp = 0; \n    #pragma omp simd reduction(+:tmp) \n    for (int j = ptr[i]; j < ptr[i + 1]; j++) { \n      tmp += value[j] * x[index[j]]; \n    } \n    y[i] = tmp; \n  }", "pragma": "target teams distribute parallel for ", "hash": "bb1b8a14a11fce8ccc956d4f35cd3ab954587b910ab130482bbe1d829b58dd71"}
{"code": "for (int i = 0; i < 2; ++i) { \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n      g1 = 1; \n      svar = 3; \n      sfvar = 4.0; \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n      [&]() { \n         \n \n         \n \n        g = 2; \n        g1 = 2; \n        svar = 4; \n        sfvar = 8.0; \n         \n \n         \n \n         \n \n         \n \n \n         \n \n         \n \n         \n \n         \n \n         \n \n         \n \n         \n \n         \n \n         \n \n      }(); \n    }", "pragma": "target teams distribute parallel for simd private(   g g1 sfvar svar)", "hash": "bbd6430247a6b7c2ff3173bbaabe590a1fd0a5c97dc21d3578470a2594369020"}
{"code": "for(k=0;k<size_z;k++) { \n        for(j=0;j<size_y;j++) { \n            for(i=0;i<size_x;i++) { \n             \n \n\t            itarget = i-nshift[l2D_int];  \n \n\t            while (itarget <  0)  { \n                    itarget += nx; \n                } \n\t            while (itarget >= nx+0) { \n                    itarget -= nx; \n                } \n\t            ltarget = l-i+itarget; \n\t            Pres[l] = q[ltarget]; \n            } \n        } \n    }", "pragma": "parallel for private(i itarget j k ltarget)", "hash": "bc59bcc077eb98576c610501246f33652b94b9ccdf89f60a8e2723a78f293ed2"}
{"code": "for (int i = 0; i < 2; ++i) { \n     \n \n     \n \n     \n \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n    g = 1; \n    g1 = 1; \n    sivar = 2; \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n    [&]() { \n       \n \n       \n \n      g = 2; \n      g1 = 2; \n      sivar = 4; \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n    }(); \n  }", "pragma": "target teams distribute simd private(  g g1 sivar)", "hash": "bc856d40e681dcd9ac90076e77c4733353904880f65e0ae1a44f3eda16b95504"}
{"code": "for(int i = 0 ; i < 100; i++) { \n     \n \n     \n \n     \n \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n \n     \n \n     \n \n     \n \n     \n \n    fn6(); \n  }", "pragma": "target teams distribute parallel for simd ", "hash": "bd170bfde7bdfdd88f35b8d7db7966ff23e4646e8c49f3c9e606cc0646a2ee02"}
{"code": "for (int i = 0; i < 2; ++i) { \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n        \n       \n \n       \n \n       \n \n       \n \n     \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n      g1 = 1; \n      svar = 3; \n      sfvar = 4.0; \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n        \n       \n \n       \n \n       \n \n       \n \n     \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n      [&]() { \n\t \n \n\t \n \n\tg = 2; \n\tg1 = 2; \n\tsvar = 4; \n\tsfvar = 8.0; \n\t \n \n\t \n \n\t \n \n\t \n \n \n\t \n \n\t \n \n\t \n \n\t \n \n\t \n \n\t \n \n\t \n \n\t \n \n\t \n \n      }(); \n    }", "pragma": "target teams distribute parallel for private(   g g1 sfvar svar)", "hash": "be10b78875acd16a1b70c15021f7ef79ec6c9bf1b5e6eb82fe845c2c3de51347"}
{"code": "for (int j = 0; j < N; ++j) { \n      tmp[j] += j; \n    }", "pragma": "target teams distribute parallel for ", "hash": "be35d33c227cc0dcb173fedd6d233cf813c752a9d9e2ce7f13eec779f67bbf3e"}
{"code": "for (int J = 0; J < 10; ++J) { \n#pragma omp critical \n    { \n      isHost = (isHost < 0 || isHost == 0) ? omp_is_initial_device() : isHost; \n      ParallelLevel1 = (ParallelLevel1 < 0 || ParallelLevel1 == 1) \n                           ? omp_get_level() \n                           : ParallelLevel1; \n    } \n    if (omp_get_thread_num() > 5) { \n      int L2; \n#pragma omp parallel for schedule(dynamic) lastprivate(L2) reduction(+: Count) \n      for (int I = 0; I < 10; ++I) { \n        L2 = omp_get_level(); \n        Count += omp_get_level();  \n \n      } \n#pragma omp critical \n      ParallelLevel2 = \n          (ParallelLevel2 < 0 || ParallelLevel2 == 2) ? L2 : ParallelLevel2; \n    } else { \n      Count += omp_get_level();  \n \n    } \n  }", "pragma": "target parallel for reduction(+: count) ", "hash": "bfc5e703157d68104db9e46f42cf8f715453e467eec6549b34ef9b82d4d47114"}
{"code": "for (int i = 0; i < 200; i++) { \n    ++x;  \n \n    ++y;  \n \n  }", "pragma": "teams distribute parallel for ", "hash": "c183034c7cd9f7690aa8be670d7c0ad2da4735b3fafe10133f91227a990ed5a3"}
{"code": "for (i = 0; i < 16; ++i) \n    ;", "pragma": "target parallel for ", "hash": "c3353b8f78db7cef0d8cc4e6f6548476d8ce911c3fb83d40afb26926d6867a39"}
{"code": "for (int k = 0; k < argc; ++k) ++k;", "pragma": "target teams distribute parallel for simd ", "hash": "c3acb47d0f3c8cb7d2654249a86afa1c391eaebd09e97780059a377182a1b188"}
{"code": "for (i = 0; i < bufsize; i++) { \n        k = (int)omp_get_team_num(); \n         \n \n         \n \n        unew[i] = (omp_get_team_num()+1) * omp_get_thread_num(); \n    }", "pragma": "teams distribute parallel for private(k)", "hash": "c429100dd82055a6b76fb4c450797ccf1d4b6c68992e6bdcafc6937e28a39aa8"}
{"code": "for (T i = ST; i < N; i++) \n    argv[0][i] = argv[0][i] - argv[0][i-ST];", "pragma": "teams distribute simd ", "hash": "c4851c0e59c3e8a64ec5113dadb79061e7770d6c36e3c16bd2af44a3770592c9"}
{"code": "for (unsigned long long it = 2000; it >= 600; it-=400) { \n    aa += 1; \n  }", "pragma": "target simd ", "hash": "c5539c3dc56fcc4af7e57166a6b40abefb60a79eede009ca4c81d0be21a79a2c"}
{"code": "for(int k=0; k<N; k++) { \n    a[k] = k; \n    n = k; \n  }", "pragma": "target parallel for simd private(n)", "hash": "c96ff4cc8b9ef114237e77705b0a5484f3ea39ec38e709880704a344df4c152e"}
{"code": "for(int k=0; k<N; k++) \n      b[k] = k;", "pragma": "teams distribute simd ", "hash": "c983b5e59f9030c9bb69ea41bac2c6938a3a1de082f37b1713fe0279edabed56"}
{"code": "for (int i = 0; i < 10; i++) \n    a[i] = i;", "pragma": "target ", "hash": "ca4a0429a564d3e4ff90504c8e2359f23db7060a5a4322b1c4059c82ccd79233"}
{"code": "for (Row=0; Row<m_Height; Row++) {\r \n    Temp = Row*m_Width;\r \n    for (Col=0; Col<m_Width; Col++) {\r \n      Sum0  = 0.0f;\r \n      Sum1  = 0.0f;\r \n      Sum2  = 0.0f;\r \n      Count = 0;\r \n\r \n      Index     = Temp + Col;\r \n      Radius    = MaxRadius * Mask[Index];\r \n      IntRadius = ceil(Radius);\r \n      if (IntRadius == 0) continue;\r \n\r \n      for(i = -IntRadius; i <= IntRadius; i++) {\r \n        NewRow = Row+i;\r \n        NewRow = NewRow < 0? -NewRow : NewRow > Height1? Height1_2-NewRow : NewRow ;\r \n        NewRow *= m_Width;\r \n        for(j = -IntRadius; j <= IntRadius; j++) {\r \n          if (Dist[abs(i)][abs(j)] < Radius) {\r \n            NewCol = Col+j;\r \n            NewCol = NewCol < 0? -NewCol : NewCol > Width1? Width1_2-NewCol : NewCol ;\r \n\r \n            Count++;\r \n            PtrSource = Source->m_Image[NewRow + NewCol];\r \n            Sum0     += PtrSource[0];\r \n            Sum1     += PtrSource[1];\r \n            Sum2     += PtrSource[2];\r \n          }\r \n        }\r \n      }\r \n\r \n      PtrTarget    = m_Image[Index];\r \n      PtrTarget[0] = Sum0 / Count;\r \n      PtrTarget[1] = Sum1 / Count;\r \n      PtrTarget[2] = Sum2 / Count;\r \n    }\r \n  }", "pragma": "parallel for private(               col count i index intradius j newcol newrow ptrsource ptrtarget radius row sum0 sum1 sum2 temp)", "hash": "ca8d24521f70d6fcc69cd092fcaea2d4ee8e294b6803405bd41b87002b7a5e43"}
{"code": "for (int j06 = 0; j06 < 64; j06++) \n    ;", "pragma": "target parallel for simd ", "hash": "caa08349c49d2a39f0347ffad072581a2cecaa2751ffc9f68f45879db2bef869"}
{"code": "for (int i = 0; i < 2; ++i) { \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n      g = 1; \n      g1 = 1; \n      svar = 3; \n      sfvar = 4.0; \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n      [&]() { \n\t \n \n\t \n \n\tg = 2; \n\tg1 = 2; \n\tsvar = 4; \n\tsfvar = 8.0; \n\t \n \n\t \n \n\t \n \n\t \n \n \n\t \n \n\t \n \n\t \n \n\t \n \n\t \n \n\t \n \n\t \n \n\t \n \n\t \n \n      }(); \n    }", "pragma": "teams distribute parallel for simd private(   g g1 sfvar svar)", "hash": "cb920b3c7b30fb18c7116cef244e4362b95395ba77b425e14537ee199300f75b"}
{"code": "for (int n = 0; n < 100; ++n) { \n  }", "pragma": "target parallel for simd ", "hash": "cbe5e389b764f4088b3bd7bb696db6bd653858e16a0700f965d52ccd0a18ccb2"}
{"code": "for(int i = 0; i < 1000; i++) {}", "pragma": "target teams distribute parallel for simd ", "hash": "ce09d992e7111e1204963d3fa71508efd92e787b3081542948d1e875eb8c26cf"}
{"code": "for (int i = 0; i < *plocal; ++i) { \n    static int local1; \n    *plocal = global; \n    local1 = global; \n  }", "pragma": "target parallel for simd ", "hash": "ce5fabe6aaffd5f757778df26bb23df18f46f5e1af28452d9a208e8deed5613c"}
{"code": "for (int i = 0; i < 64; i++) \n    l07++;", "pragma": "target simd ", "hash": "ce6e789797a68b2cab14f609acbcdc2f3acab0a32d3a75f1bc077f31ddd5bba8"}
{"code": "for (int j12 = 0; j12 < 64; j12++) \n    ;", "pragma": "teams distribute simd ", "hash": "ce7dafeb6b21001f09350d73b5c45eaffb896f3babe02e95ae641d246d3c5a96"}
{"code": "for (GoodIter I = begin; I >= end; I = I - 1)  \n \n    ++I;", "pragma": "teams distribute ", "hash": "cebf32c7a6d89eb92ffd4fa017a21fbd096a7f4fd44d129b9d275593df891da5"}
{"code": "for (j08a = 0; j08a < 64; j08a++) \n    for (j08b = 0; j08b < 4; j08b++) \n      ;", "pragma": "target teams distribute parallel for simd private( j08a j08b)", "hash": "ceeff9c2f557c4f8f41f17b6e30205d7c23b090d3e2f7610cee5182daa32a514"}
{"code": "for (long long i = 0; i < 10; ++i);", "pragma": "target parallel for reduction( +: b[0:2][2:4][1] task) ", "hash": "d0379477a383c2267c557f8e87ed294c22a01873977042fb09540cd04ede1f80"}
{"code": "for (int i = 0; i < 2; ++i) { \n \n     \n \n     \n \n    g = 1; \n    g1 = 1; \n    sivar = 2; \n    [&]() { \n      g = 2; \n      g1 = 2; \n      sivar = 4; \n \n    }(); \n  }", "pragma": "teams distribute simd private(  g g1 sivar)", "hash": "d1155d338c8a99dfe533fbc067afa02e37b9ebc7ff730e9c64f28a2cc66c0614"}
{"code": "for (i = 0; i < argc; ++i) foo();", "pragma": "target teams distribute simd ", "hash": "d21c512b3f80ff6dc24cf3dea7bb9fe2de32fea6d438bff69f8682e816e623e5"}
{"code": "for (int i = 0; i < 2; ++i) { \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n      g = 1; \n      g1 = 1; \n      svar = 3; \n      sfvar = 4.0; \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n      [&]() { \n\t \n \n\t \n \n\tg = 2; \n\tg1 = 2; \n\tsvar = 4; \n\tsfvar = 8.0; \n\t \n \n\t \n \n\t \n \n\t \n \n \n\t \n \n\t \n \n\t \n \n\t \n \n\t \n \n\t \n \n\t \n \n\t \n \n\t \n \n      }(); \n    }", "pragma": "teams distribute parallel for simd private(   g g1 sfvar svar)", "hash": "d32bf211a260f74c6c6fa85f7dbc54da509dc385344688910d1a8ec9a8eef79e"}
{"code": "for (i = 0; i < 10; i++) \n\tfor (j = 0; j < 10; j++) \n\t  if (b) \n\t    bar (); \n\t  else \n\t    baz ();", "pragma": "target simd ", "hash": "d4a8102af61a4d4714b3f3db4e4d895548e916e8763a4608e83cd853f2ae4266"}
{"code": "for (int i = 0; i < 10; ++i) { \n    a += 1; \n    aa += 1; \n  }", "pragma": "target teams distribute ", "hash": "d655142e3107710a7c6ebfcb57d7eecdfa6a8b061ee4d1c36acfe77f56ad5180"}
{"code": "for (int i = 0; i < 10; i++) \n    a[i] = 0;", "pragma": "target ", "hash": "d684babbd7e94c603a67955b07a0d035ea67dca7ebcbbb8a2b457d62a3a39931"}
{"code": "for(int i = 0; i < n; i++) \n    a[i] = v[i];", "pragma": "target teams distribute parallel for ", "hash": "d6aed8e20b3207234c7cd55ae79450aee7184a06e219dc55ac57e84c58538bd3"}
{"code": "for (int i = 0; i < 2; ++i) { \n    sivar += i; \n  }", "pragma": "teams distribute simd reduction(+: sivar) ", "hash": "d6f3d483f0f6d20d40528d85b963a43f94f4a6bbd83c95009160b185c35294a9"}
{"code": "for (int i = 0; i < 10; ++i) \n      a += 12;", "pragma": "target parallel for ", "hash": "d7b330116ba6f9ee3e3f72dc8ee6b7179e164427216752994ca93b69a977e523"}
{"code": "for (int tms = 1 ; tms <= 256 ; tms *= 2) {  \n \n      for (int ths = 32 ; ths <= 1024 ; ths *= 2) {  \n \n\t  t++; \n#pragma omp target \n#pragma omp teams num_teams(tms) thread_limit(ths) \n\t  { \n#pragma omp distribute parallel for schedule(dynamic) \n\t    for (int i = 0; i < n; ++i) { \n\t      a[i] += b[i] + c[i]; \n\t    } \n\t  } \n      }  \n \n    }", "pragma": "target ", "hash": "d7c63a856b9ac67bf45b2f7b00b5e3a1d76d67ac1fdaf5a0b7c293b8acce2695"}
{"code": "for (int i = 0; i < 64; i++) \n    r20[1]++;", "pragma": "target teams distribute parallel for simd reduction(+:r20[1:22]) ", "hash": "d8a5e645db7f97c533cd3e2df1fc8708d739321894f1522d6ade25dc58fe99c2"}
{"code": "for (int i = 0; i < 64; i++) \n    r28++;", "pragma": "teams distribute simd reduction(+:r28) ", "hash": "d8af22612a812eca097491e78f639b214212d43f85ad190f03c8ef5840527e6a"}
{"code": "for (int i = 0; i < 2; ++i) { \n    t_var += (T) i; \n  }", "pragma": "target teams distribute simd reduction(+: t_var) ", "hash": "d8f969c860456d1a41da6d1db7c28dce00a80bb358881281ece19f535f2ef534"}
{"code": "for (int i = 0; i < 10; ++i) { \n    a += 1; \n    aa += 1; \n    b[2] += 1; \n  }", "pragma": "target teams distribute simd ", "hash": "d90867857b9ee72cd1fa5e0b26e602ae095c3a7e162a7975a88505112de95ed1"}
{"code": "for(int i = 0; i < n; i++) { \n    for(int j = 0; j < m; j++) { \n      a[i][j] = (T)0; \n    } \n  }", "pragma": "target teams distribute parallel for simd ", "hash": "d93ebe3c605102b1abbd19c7b78a895b7447214848418393bcc8f1fdced832a2"}
{"code": "for (int ii=0; ii<10; ii++) \n      ;", "pragma": "target simd ", "hash": "da6e429eac21a898ad98d1024991a6b0a984d709a7b73c272801d9902edd0f1d"}
{"code": "for (T i = 0; i < N; ++i) {}", "pragma": "target parallel for simd ", "hash": "db22fb3aab5da10d085096c8ed823b5524ff3bca13f5356b48102cd72f3cf75e"}
{"code": "for (int i = 0; i < 100; i++) { \n    foo(); \n  }", "pragma": "teams distribute parallel for simd ", "hash": "db4007c2fa3cbfee98745582364d725220ea81f336156f8cfd93fc48b7c69cfd"}
{"code": "for (unsigned i=100; i<10; i+=10) { \n    a += 1; \n    aa += 1; \n    aaa += 1; \n    b[2] += 1; \n  }", "pragma": "target parallel for ", "hash": "dc86217a6c35e47919289de903fad58eff9b1a771493c45c366e0ecc009ed043"}
{"code": "for (int i = 0; i < 2; ++i) { \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n \n     \n \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n \n     \n \n      \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n \n    sivar += i; \n \n    [&]() { \n       \n \n       \n \n \n      sivar += 4; \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n    }(); \n  }", "pragma": "teams distribute parallel for reduction(+: sivar) ", "hash": "de6908f801fa9d814b1edfaebe73fea10f7f004eb5336e980f811efe882c0114"}
{"code": "for (j=0; j<N; j++) \n        zd_dev[j] *= c[0];", "pragma": "teams distribute parallel for ", "hash": "df2893456696550a4136e65194d0a10cc4327e25b36ffc20ac9a146d18fafe5b"}
{"code": "for (int i = 0; i < 64; i++) \n    r21[1]++;", "pragma": "target teams distribute simd reduction(+:r21[1:23]) ", "hash": "df9db7d9d7bc0f3249715a3f5dc740b7123fe003fa0afa039800c51f57576a2c"}
{"code": "for (int i=0; i < N; ++i) \n    orfc = orfc || rcf[i];", "pragma": "target teams distribute parallel for reduction(orfc ||:) ", "hash": "e1404aba96a367e3c4e352906015d5fc1667587e637eb29b004cb5e9c5b5c9b8"}
{"code": "for (int i = 0; i < 2; ++i) { \n       \n \n \n       \n \n      g = 1; \n      g1 = 1; \n      svar = 3; \n      sfvar = 4.0; \n \n \n      [&]() { \n        g = 2; \n        g1 = 2; \n        svar = 4; \n        sfvar = 8.0; \n \n      }(); \n    }", "pragma": "target teams distribute simd private(   g g1 sfvar svar)", "hash": "e145b61c2462f9081389ba9827cc1332187b9babdc82f26d40cdd3d81c92de74"}
{"code": "for (i = 0; i < argc; ++i) \n    foo();", "pragma": "target teams distribute parallel for ", "hash": "e1cfc2e94cd00626262705a06afb899afa4ffc4afd71b9801b798bb18117841f"}
{"code": "for (int i = 0; i < 2; ++i) { \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n      g = 1; \n      g1 = 1; \n      svar = 3; \n      sfvar = 4.0; \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n      [&]() { \n         \n \n         \n \n        g = 2; \n        g1 = 2; \n        svar = 4; \n        sfvar = 8.0; \n         \n \n         \n \n         \n \n         \n \n \n         \n \n         \n \n         \n \n         \n \n         \n \n         \n \n         \n \n         \n \n         \n \n      }(); \n    }", "pragma": "target teams distribute simd private(   g g1 sfvar svar)", "hash": "e240e21c2126d681ea72082c91dd9d88aab4fb243dafd4961f526fb134a77960"}
{"code": "for (int i=0; i < N; ++i) \n    andc = andc && rcl[i];", "pragma": "teams distribute parallel for reduction(&&: andc) ", "hash": "e3669c952d6b7f2d8ac81a8f675bb94049e3e6871b6c21a81904cdb25213c9a4"}
{"code": "for (int tms = 1 ; tms <= 256 ; tms *= 2) {  \n \n      for (int ths = 32 ; ths <= 1024 ; ths *= 2) {  \n \n\t  t++; \n#pragma omp target \n#pragma omp teams num_teams(tms) thread_limit(ths) \n\t  { \n#pragma omp distribute parallel for simd schedule(dynamic) \n\t    for (int i = 0; i < n; ++i) { \n\t      a[i] += b[i] + c[i]; \n\t    } \n\t  } \n      }  \n \n    }", "pragma": "target ", "hash": "e3ac574d8e01f987ad098a23f2483447a71dbb54229acabe60f6ed306a59e5ec"}
{"code": "for (int i = 0; i < 100; i++) { \n     \n \n     \n \n     \n \n    foo(); \n  }", "pragma": "teams distribute parallel for ", "hash": "e3c9414a727e8225a7513cb3a48814b9495e0435c4add020ad29e9047a95a957"}
{"code": "for(int i = 0 ; i < 128 ; i++) {  \n \n      p += 3.0;   \n \n      q += 7.0; \n      A[i] += p; \n      B[i] += q; \n    }", "pragma": "teams distribute simd private( p q)", "hash": "e44bba25a1d6daa3e60aefcc19a99196a8d852bafaff9dbe77eab6bed05cf3a3"}
{"code": "for (i = 0; i<N; i++) \n    sum += SUNRabs(xd_dev[i]);", "pragma": "teams distribute parallel for reduction(+:sum) ", "hash": "e46e99e4d1854e62ea9a81eae623fec63ed24a44dc6824be01dc7bd412816d28"}
{"code": "for (int i=0; i < N; ++i) \n    orc = orc || rcc[i];", "pragma": "teams distribute parallel for reduction(orc ||:) ", "hash": "e553f4b0adfae018d9e6dad526ad8232f1aa42b200d1815e7bfedfca77e56333"}
{"code": "for (int i = offset; i < offset + length; i++) { \n      dst[i] = src[i]; \n    }", "pragma": "teams distribute parallel for simd ", "hash": "e55f4007626753ce1abf52831503861f3f2e1ebe0511db89c9e3a21245a8aa70"}
{"code": "for (int i = start; i < end; ++i) { \n    body(i); \n  }", "pragma": "target teams distribute parallel for ", "hash": "e5b46eb9d2246f94ae139c2a4215050230bd5ed3bc0ff3dbc00e3b9019966a2e"}
{"code": "for (i = 0; i < N; i++) \n    zd_dev[i] = xd_dev[i]+yd_dev[i];", "pragma": "teams distribute parallel for ", "hash": "e5d313bd51e17d088a9018bd55f1316afe901b53c007ce6addfd9a6947629a22"}
{"code": "for (int i = 0; i < 10; ++i) \n    ;", "pragma": "target teams distribute simd ", "hash": "e5ea9646e8c9e27ec9ed2a0c8e2580f0da7f443582193ef91f2d6d0f5efd284c"}
{"code": "for (i = 0; i < bufsize; i++) { \n        k = (int)omp_get_team_num() + bufsize - bufsize2; \n         \n \n         \n \n        unew[i] = (omp_get_team_num()+1) * omp_get_thread_num() + k - k; \n    }", "pragma": "teams distribute parallel for private(k)", "hash": "e5fe2b82791792471eaf6433bd0fa3886f5f4eb4565b73f4456236b86460a242"}
{"code": "for(int j = 0 ; j < 510 ; j += blockSize) { \n      int ub = (j+blockSize < 510) ? (j+blockSize) : 512; \n      #pragma omp parallel for \n      for(int i = j ; i < ub; i++) { \n        A[i] += B[i] + C[i]; \n      } \n    }", "pragma": "teams distribute ", "hash": "e6181ab94c442fbb93d494b673cc893cd5b879b270840a01fb468710fee20b3b"}
{"code": "for (int i = 0; i < 64; i++) \n    r20++;", "pragma": "target teams distribute parallel for simd reduction(+:r20) ", "hash": "e6ca5000f29227d9c2b61f5baadc8e5d94b63d6304aa7ac0c5ce4d7e8680f8ec"}
{"code": "for (int i = 0; i < global; ++i) { \n    global += 1; \n  }", "pragma": "target parallel for simd private(global)", "hash": "e7d4bfddcfe9e73b819080a38658aeaf24c25ebfe2c729aecc1f4b8ff91e5326"}
{"code": "for (int i = 0; i < 100; i++) { \n    ind2 += LEN; \n  }", "pragma": "target teams distribute parallel for simd ", "hash": "e7fb2da38c72982ec1a125b454ed204ba6b26b59aa2a42ab3bc212e53aa795bb"}
{"code": "for (int i = 0; i < 2; ++i) { \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n      g1 = 1; \n      svar = 3; \n      sfvar = 4.0; \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n \n      [&]() { \n         \n \n         \n \n        g = 2; \n        g1 = 2; \n        svar = 4; \n        sfvar = 8.0; \n         \n \n         \n \n         \n \n         \n \n \n         \n \n         \n \n         \n \n         \n \n         \n \n         \n \n         \n \n         \n \n         \n \n      }(); \n    }", "pragma": "target teams distribute parallel for private(   g g1 sfvar svar)", "hash": "e95fd6b61f0939c077bae3e4708737d9263dde305e60badd555af6033b195f5b"}
{"code": "for (int i = 0; i < 10; ++i) \n      a += 8;", "pragma": "target parallel for ", "hash": "ea10778804374a4821184c68b03e1a4176020550f9355d11d34ebc31543c61e3"}
{"code": "for(int k=0; k<N; k++) \n    b[k] = k;", "pragma": "target parallel for simd ", "hash": "ebb93c8bfcfd3182f060f492512f12e9041fa942525e7c999b33f44ac6681e7e"}
{"code": "for (i = 0; i < N; i++) \n    xd_dev[i] *= a;", "pragma": "teams distribute parallel for ", "hash": "ed62062995143e4667066f48f3bfce7e09cf1530b13575b49b2a3d9d32680634"}
{"code": "for (int i = 0; i < 16; ++i) { \n#pragma omp cancel for \n    ; \n  }", "pragma": "teams distribute parallel for ", "hash": "edf9babe912b489c0e0353edf40b55f6f8e469c9fa773eb7fdd9ee78cc004aee"}
{"code": "for (i = 0; i < 64; i++) \n    s[2]++;", "pragma": "teams distribute parallel for reduction(+: s[2:2]) ", "hash": "ee43f3e7195faefb20afae1a2108139947f8db8a7844b21e9f953b21bbb2fde2"}
{"code": "for (i = 0; i < argc; ++i)   \n \n    foo();", "pragma": "target parallel for simd ", "hash": "ee587950d2add6b405bb712dd633968415f1d4e13a9f922f1c11787025f4e01f"}
{"code": "for (int i = 0; i < 64; i++) \n    l22 = i;", "pragma": "teams distribute simd private(l22)", "hash": "eee4c8c662407a6305e5771bd16020e4bac25755037d74f36564fe46d2163c4a"}
{"code": "for(int i = 0; i < n; i++) { \n    for(int j = 0; j < m; j++) { \n      a[i][j] = 0; \n    } \n  }", "pragma": "target teams distribute parallel for simd ", "hash": "eef4d6043ea7c7887b704ed1ff536d5fdf26fac5795bcbbbdd69c67b0d6b6858"}
{"code": "for (int i = 0; i < 2; ++i) { \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n    g = 1; \n    g1 = 1; \n    sivar = 2; \n     \n \n     \n \n     \n \n     \n \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n    [&]() { \n       \n \n       \n \n      g = 2; \n      g1 = 2; \n      sivar = 4; \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n    }(); \n  }", "pragma": "teams distribute parallel for private(  g g1 sivar)", "hash": "ef9909695613b86498784a2b1a5ce66f202bca2e069bffbaf363a973924682a3"}
{"code": "for (int k = 0; k < argc; ++k) { \n      ++k; \n      i += 4; \n    }", "pragma": "target parallel for simd ", "hash": "f02fb8eef3ddedcd3c153b6ee425175c924af176dceee2a3e081eff101de2ff3"}
{"code": "for (int i = 0; i < 64; i++) \n    l12 = i;", "pragma": "target parallel for simd private(l12)", "hash": "f0d6506ca3fb27da0d7208fba4341210c7dad15ec23f1b72b5cd8df3e002304f"}
{"code": "for (int i = 0; i < 2; ++i) { \n \n     \n \n \n    g = 1; \n    g1 = 1; \n    sivar = 2; \n    [&]() { \n      g = 2; \n      g1 = 2; \n      sivar = 4; \n \n    }(); \n  }", "pragma": "teams distribute simd private(  g g1 sivar)", "hash": "f0ec39aff901e8bb288d37b66c62122a503112c67a2c5d1e97177f2b2d52ad03"}
{"code": "for (int k = 0; k < 10; ++k) { \n#pragma omp parallel private(i) \n    foo(); \n  }", "pragma": "target teams private(i)", "hash": "f30b7b846ddcc88a49207930157506285c2fa06ed10cce2960135a0bf80cf937"}
{"code": "for (int i = 0; i < 2; ++i) { \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n    sivar += i; \n \n    [&]() { \n       \n \n       \n \n \n      sivar += 4; \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n    }(); \n  }", "pragma": "teams distribute simd reduction(+: sivar) ", "hash": "f3f4c9adbce830d56c312f09e0f39ff5cffe247f890c801bce3a4476b79dab6d"}
{"code": "for (GoodIter I(nullptr); I < end; ++I)  \n \n    ++I;", "pragma": "teams distribute ", "hash": "f42f43ba8525257a8a27009304e2c7bd5133f8aa8a77a70c79538f2077e02193"}
{"code": "for (int i=0; i<1000; ++i) { \n    #pragma omp loop \n    for (int j=0; j<100; j++) { \n      aaa[i] += i + j; \n    } \n  }", "pragma": "target teams ", "hash": "f460f24130d91af8d6d3090f9bb8256bea503e812ce067ec5eb683e63e388154"}
{"code": "for (int i = 0; i < 10; ++i) \n      a += 18 + x;", "pragma": "target parallel for ", "hash": "f4a533a7c0efc4935d83ea6297ea498ca8ace664de85200fdf7a5d350516b621"}
{"code": "for (int i = 0; i < 2; ++i) { \n       \n \n \n       \n \n      g = 1; \n      g1 = 1; \n      svar = 3; \n      sfvar = 4.0; \n \n \n      [&]() { \n\tg = 2; \n\tg1 = 2; \n\tsvar = 4; \n\tsfvar = 8.0; \n \n      }(); \n    }", "pragma": "teams distribute private(   g g1 sfvar svar)", "hash": "f4c14ae04abf981ce6ca689b328edb1837f658366bb66e545c6e2c1c9e3dfe01"}
{"code": "for (a = 0; a < 10; ++a) { \n    a += 1; \n  }", "pragma": "target teams distribute simd ", "hash": "f4e7f96f5f83d3f95dbe8a0ecf7f25f0542aefd4da17a77096ef7bc902902801"}
{"code": "for (int i = N - 1; i > -1; i -= s) \n    a[BASE + i] = 1;", "pragma": "target simd ", "hash": "f5367cdeb50a930e12bec74d9f0ae4bb5365121ed9fc3847e1dc504a18c97d6b"}
{"code": "for (int k = 0; k < argc; ++k) { ++k; v += j; }", "pragma": "target teams distribute parallel for simd ", "hash": "f655e3ed0edf10016896c764262a8241483bc97220716573f5503a7554a79ad4"}
{"code": "for (GoodIter I = begin; I < end; ++I)  \n \n    ++I;", "pragma": "teams distribute ", "hash": "f70f02ead90eecfb99ad7fabad44a0dd08812585a2233cef91440f9f72a40a82"}
{"code": "for(int i = 0; i < n; i++) { \n    a[i] = 1; \n    l = i; \n  }", "pragma": "target teams distribute simd private(l)", "hash": "f7e54e0a7382c821c764cacbdccaca3937f6b80c810e591ff31d3d56b6257213"}
{"code": "for (int i = 0; i < 2; ++i) { \n     \n \n     \n \n     \n \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n \n    g = 1; \n    g1 = 1; \n    sivar = 2; \n     \n \n     \n \n     \n \n     \n \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n     \n \n    [&]() { \n       \n \n       \n \n      g = 2; \n      g1 = 2; \n      sivar = 4; \n       \n \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n       \n \n    }(); \n  }", "pragma": "teams distribute parallel for simd private(  g g1 sivar)", "hash": "f812de7095ed3a674a28e802dbadf0f6ca370fb9cbbaaded45c2b7077fe52027"}
{"code": "for (IT I = begin; I < end; ++I) {  \n \n      ++I; \n    }", "pragma": "teams distribute ", "hash": "f87dda675ed3931aec73be211a9cac673264033467cb01e2b6b1c798d5b55c01"}
{"code": "for(int k=0; k<100; k++) { \n      if (k > 1){ \n        a[k] = a[k-2] + 2; \n      } \n      else{ \n        a[k] = k; \n      } \n    }", "pragma": "teams distribute simd ", "hash": "f8dfb1f6a3ac45e4414376bf933da3a9455c4eede017925d170691b1bdbdcbff"}
{"code": "for (i = 0; i < 16; ++i) \n \n \n    for (int j = 0; j < 16; ++j)", "pragma": "target parallel for private(i)", "hash": "f91e238662aa99a538b96904f0db738fbfacd7ebbba1c092f44d73dd55aaf210"}
{"code": "for (int k = 0; k < a.a; ++k) { \n      ++this->a.a; \n#pragma omp cancel for \n    }", "pragma": "target teams distribute parallel for private(a t::a this->a)", "hash": "fa721aa5ac8fb356977244c898ece9fd09a4f6d201c85feed63c4773d7b10917"}
{"code": "for (int i = 0; i < 10; ++i) \n      a += 13;", "pragma": "target parallel for ", "hash": "faaa975aef904c333c2474240e1c7e61cb6db5817cca551b8ca29d8c4ea50ee8"}
{"code": "for (int i = 0; i < 64; i++) \n    l15 = i;", "pragma": "target teams distribute parallel for simd private(l15)", "hash": "fafdf2dc2d13796ad5b9dce1b4574fda7844da6c3bc254c872e2e250fc17c4e2"}
{"code": "for (int i = 0; i < 64; i++) \n    r27++;", "pragma": "teams distribute parallel for simd reduction(+:r27) ", "hash": "fb663104bc14b0906f08520cd5f1435ce855dacade188952058b246a96bb758b"}
{"code": "for (int i = 0; i < 64; i++) \n    l13 = i;", "pragma": "target teams distribute private(l13)", "hash": "fbb6e699777c298b36ad64de36e0725f2b84ec045e8b649940f350ee9cb0cc32"}
{"code": "for (int i = 0; i < 2; ++i) { \n       \n \n \n       \n \n      g = 1; \n      g1 = 1; \n      svar = 3; \n      sfvar = 4.0; \n       \n \n \n \n \n       \n \n \n       \n \n \n \n       \n \n \n \n \n \n      [&]() { \n\tg = 2; \n\tg1 = 2; \n\tsvar = 4; \n\tsfvar = 8.0; \n \n      }(); \n    }", "pragma": "teams distribute parallel for simd private(   g g1 sfvar svar)", "hash": "fc3cd020b2f31daa49c53f63ce51603aa5db91d0ee2964aa21a821a6589c8c17"}
{"code": "for (i = 0; i < N; i++) \n    zd_dev[i] = (a*xd_dev[i])+(b*yd_dev[i]);", "pragma": "teams distribute parallel for ", "hash": "fd4a2bc3c49a3c9d074e1b6899c4636fdfa5c36c9bd05f0777dbf04df4721e8d"}
{"code": "for (int kk=0; kk<20; kk++) \n    ;", "pragma": "target simd ", "hash": "febc84fed126ee3fc2c1f62b0d0812952b8ce06a8bbdbaa1b920099e8902b27f"}
{"code": "for (int i = 0; i < argc; ++i) \n    foo();", "pragma": "target parallel for simd ", "hash": "feff307462b953078d939e2c843c70fbf70d20e29804a1dd49d139cfe173626e"}
{"code": "for (i = 0; i < N; i++) \n    zd_dev[i] = xd_dev[i];", "pragma": "teams distribute parallel for ", "hash": "fffd523903f300dfd1c8a52a13f76934003dcfe72a49d135a5b2a2bd61cab05e"}
{"code": "for (i = 0; i < 36; i++) { \n        printf(\"team %d thread %d doing iteration %d and x is %d num_threads %d num_teams is %d\\n\", \n               (int) omp_get_team_num(), (int) omp_get_thread_num(), i, x,  \n               (int) omp_get_num_threads(), (int) omp_get_num_teams()); \n    }", "pragma": "target teams distribute ", "hash": "b9ab05ccac2565fa0394be0686898643ed63833bcc5e664548aa998ed2a25f30"}
{"code": "for (size_t i = 0; i < resultReader.getSize(); ++i) { \n                progress.updateProgress(); \n                char *data = resultReader.getData(i, thread_idx); \n                while (*data != '\\0') { \n                    Util::parseKey(data, key); \n                    unsigned int dbKey = std::strtoul(key, NULL, 10); \n                    maxTargetId = std::max(maxTargetId, dbKey); \n                    data = Util::skipLine(data); \n                } \n            }", "pragma": "for reduction(max:maxtargetid) ", "hash": "45fdb6869f87a553a6cbc9b347276e9dd9039dac6a1c48468067a52ad4117f26"}
{"code": "for(i = 0; i < (int)training_instances_number; i++) \n    { \n        training_index = training_indices[i]; \n \n         \n \n \n        inputs = data_set_pointer->get_instance(training_index, inputs_indices); \n \n         \n \n \n        outputs = multilayer_perceptron_pointer->calculate_outputs(inputs, parameters); \n \n         \n \n \n        targets = data_set_pointer->get_instance(training_index, targets_indices); \n \n         \n \n \n        if(targets[0] == 1.0) \n        { \n            error = positives_w*outputs.calculate_sum_squared_error(targets); \n        } \n        else if(targets[0] == 0.0) \n        { \n            error = negatives_w*outputs.calculate_sum_squared_error(targets); \n        } \n        else \n        { \n            std::ostringstream buffer; \n \n            buffer << \"OpenNN Exception: WeightedSquaredError class.\\n\" \n                   << \"double calculate_error(const Vector<double>&) const method.\\n\" \n                   << \"Target is neither a positive nor a negative.\\n\"; \n \n            throw std::logic_error(buffer.str()); \n        } \n \n        sum_squared_error += error; \n \n    }", "pragma": "parallel for reduction(+:sum_squared_error) private(     error i inputs outputs targets training_index)", "hash": "3bec2b9a41fc4ad4bc7b377dcf85be41572e801e407edf2ee9191c709cb291ab"}
{"code": "for(i = 0; i < (int)training_instances_number; i++) \n    { \n        training_index = training_indices[i]; \n \n        \n \n \n       inputs = data_set_pointer->get_instance(training_index, inputs_indices); \n \n        \n \n \n       outputs = multilayer_perceptron_pointer->calculate_outputs(inputs, parameters); \n \n        \n \n \n       targets = data_set_pointer->get_instance(training_index, targets_indices); \n \n        \n \n \n       for(size_t j = 0; j < outputs_number; j++) \n       { \n           if(outputs[j] == 0.0) \n           { \n               outputs[j] = 1.0e-6; \n           } \n           else if(outputs[j] == 1) \n           { \n               outputs[j] = 0.99999; \n           } \n \n           cross_entropy_error -= (targets[j]*log(outputs[j]) + (1.0 - targets[j])*log(1.0 - outputs[j])); \n       } \n    }", "pragma": "parallel for reduction(+ : cross_entropy_error) private(    i inputs outputs targets training_index)", "hash": "29d9d3150ab27220e9688788da9528d584ff7da2a9f7b33ce8907a0a667a73b3"}
{"code": "for(i = 0; i < (int)training_instances_number; i++) \n   { \n       training_index = training_indices[i]; \n \n       \n \n \n      inputs = data_set_pointer->get_instance(training_index, inputs_indices); \n \n       \n \n \n      outputs = multilayer_perceptron_pointer->calculate_outputs(inputs, parameters); \n \n       \n \n \n      targets = data_set_pointer->get_instance(training_index, targets_indices); \n \n       \n \n \n      sum_squared_error += outputs.calculate_sum_squared_error(targets); \n \n       \n \n \n      normalization_coefficient += targets.calculate_sum_squared_error(training_target_data_mean); \n   }", "pragma": "parallel for reduction( + : normalization_coefficient sum_squared_error) private(    i inputs outputs targets training_index)", "hash": "4a5ed20ce4d8d9b52ebc51f90ea2b91a08c119459cf6da80047395abeecd8952"}
{"code": "for (i = 0; i < 10; i++) \n\tfor (j = 0; j < 10; j++) \n\t  { \n\t    r = r + 1; \n\t    p = q; \n\t    dosomething (a, n, p + q); \n     #pragma omp ordered \n\t      p = q; \n\t    s = i * 10 + j; \n\t  }", "pragma": "target teams distribute parallel for reduction(+: r) private(p q s)", "hash": "2aef9b7123b8827aaaba370895b63bbe5842211bfa10aa6b4bc89c76432ef4d9"}
{"code": "for (i = 0; i < 10; i++) \n\t{ \n\t  for (j = 0; j < 10; j++) \n\t    { \n\t      r = r + 1; \n\t      p = q; \n\t      dosomething (a, n, p + q); \n\t    } \n   #pragma omp ordered \n\t    p = q; \n\t  s = i * 10; \n\t}", "pragma": "target teams distribute parallel for reduction(+: r) private(p q s)", "hash": "e55052d43839c1bed70c88799ccbeb1cd4323f0fd6ef227b810a064ffb15362f"}
{"code": "for(int i = 0; i < (int)inputs_number; i++) \n   { \n       for(int j = 0; j < (int)targets_number; j++) \n       { \n           input_index = input_indices[i]; \n \n           input_variable = data.arrange_column(input_index); \n \n           target_index = target_indices[j]; \n \n           target_variable = data.arrange_column(target_index); \n \n           linear_correlations(i,j) = input_variable.calculate_linear_correlation(target_variable); \n       } \n   }", "pragma": "parallel for private(   input_index input_variable target_index target_variable)", "hash": "c17f6489b1c91860f1962eea345c0d36c0eb874c63a640ab2a7baae171200ccc"}
{"code": "for (i = 0; i < TESTITERS; i++) \n    check (accum (i), accum_ref (i));", "pragma": "target ", "hash": "f1ebb3a6c215224525a9839aee48636c17ae0271e77f21c5ba5e759a4490e909"}
{"code": "for (int i = 1; i < n; ++i) { \n    foo_ld(va_arg(ap, long double)); \n  }", "pragma": "target parallel ", "hash": "abc9d30fced82104d319bfc9a1c6c17bc6619ed53167cbf0278faeb27c1cda1c"}
{"code": "for (int i = 0, j = 0; i < 10; ++i) \n    c[i] = a[i];", "pragma": "target parallel for ", "hash": "f4aa9a7c042aaa8e17d7868ebb68fc995d3db869f156b614b9ac30571dacbb56"}
{"code": "for (long long i = 0; i < 10; ++i) { \n#pragma omp task in_reduction(+: argc, argv[0:10][0:argc]) \n    ; \n  }", "pragma": "target parallel for reduction(  +: argc argv[0:10][0:argc] task) ", "hash": "d90bb6d85250e77581f373648528adb1092941826751324c3d852cd7dbed7ab9"}
{"code": "for (int i = 0, j = 0; i < 10; ++i) \n    c[i] = a[i];", "pragma": "target parallel for simd ", "hash": "f4aa9a7c042aaa8e17d7868ebb68fc995d3db869f156b614b9ac30571dacbb56"}
{"code": "for (IT I = begin; I < end; I += TC<int, ST>::step()) { \n    ++I; \n  }", "pragma": "target simd ", "hash": "a61d315c5a47d514f226c88c0ce7cd05e668ce5f8a31eca179f2df1b9cd0910e"}
{"code": "for (int ib = 0; ib < 100; ib++) { \n    std::complex<T> partial_sum; \n    const int istart = ib * 4; \n    const int iend = (ib + 1) * 4; \n#pragma omp parallel for reduction(+                                     : partial_sum) \n    for (int i = istart; i < iend; i++) \n      partial_sum += std::complex<T>(i, i); \n  }", "pragma": "target teams distribute ", "hash": "e0cc30a7d224acb0ba9f98a78e938e755d73fc1c0c45741ed2a03a1dfcd7cd0c"}
{"code": "for (int i = 0, j = 0; i < 10; ++i) \n    c[i] = a[i];", "pragma": "target teams distribute parallel for ", "hash": "f4aa9a7c042aaa8e17d7868ebb68fc995d3db869f156b614b9ac30571dacbb56"}
{"code": "for (long long i = 0; i < 10; ++i) { \n#pragma omp task in_reduction(+: argc, argv[0:10][0:argc]) \n    ; \n  }", "pragma": "target teams distribute parallel for reduction(  +: argc argv[0:10][0:argc] task) ", "hash": "d90bb6d85250e77581f373648528adb1092941826751324c3d852cd7dbed7ab9"}
{"code": "for (IT I = begin; I < end; I += TC<int, ST>::step()) { \n    ++I; \n  }", "pragma": "target teams distribute parallel for simd ", "hash": "a61d315c5a47d514f226c88c0ce7cd05e668ce5f8a31eca179f2df1b9cd0910e"}
{"code": "for (IT I = begin; I < end; I += TC<int, ST>::step()) { \n    ++I; \n  }", "pragma": "target teams distribute simd ", "hash": "a61d315c5a47d514f226c88c0ce7cd05e668ce5f8a31eca179f2df1b9cd0910e"}
{"code": "for (i=0; i<nvec; i++) { \n    yd_dev = yd_dev_ptrs[i]; \n    sum = ZERO; \n#pragma omp parallel for reduction(+:sum) schedule(static, 1) \n    for (j=0; j<N; j++) \n      sum += xd_dev[j] * yd_dev[j]; \n    dotprods[i] += sum; \n  }", "pragma": "teams distribute ", "hash": "5fa6b9a96268643005de9fce26be15c3dc0f3a1ef86b22b141c3ebd78a328660"}
{"code": "for (int i = 0, j = 0; i < 10; ++i) \n    c[i] = a[i];", "pragma": "teams distribute parallel for ", "hash": "f4aa9a7c042aaa8e17d7868ebb68fc995d3db869f156b614b9ac30571dacbb56"}
{"code": "for (long long i = 0; i < 10; ++i) { \n#pragma omp task in_reduction(+: argc, argv[0:10][0:argc]) \n    ; \n  }", "pragma": "teams distribute parallel for reduction(  +: argc argv[0:10][0:argc] task) ", "hash": "d90bb6d85250e77581f373648528adb1092941826751324c3d852cd7dbed7ab9"}
{"code": "for (i = 0; i < N; i++) { \n      max = SUNMAX(SUNRabs(xd_dev[i]), max); \n    }", "pragma": "teams distribute parallel for reduction(max:max) ", "hash": "0afa0bfac29e4d5eea5b79fdb93e71ae446042c8e348969669c3165a3d9b4e43"}
{"code": "for (i = 0; i < N; i++) \n    if (dd_dev[i] != ZERO)  min = SUNMIN(nd_dev[i]/dd_dev[i], min);", "pragma": "teams distribute parallel for reduction(min:min) ", "hash": "90de4f64d9f32e3620e539b091b44a28fb9687aad316de4ab2d4307ae5b4db6c"}
{"code": "for (IT I = begin; I < end; I += TC<int, ST>::step()) {  \n \n    ++I; \n  }", "pragma": "teams distribute parallel for simd ", "hash": "264a3e591e922d65f97c04985037c90d7efe899722836bac0a755c7e48c21e62"}
{"code": "for (IT I = begin; I < end; I += TC<int, ST>::step()) {  \n \n    ++I; \n  }", "pragma": "teams distribute simd ", "hash": "264a3e591e922d65f97c04985037c90d7efe899722836bac0a755c7e48c21e62"}
{"code": "for (int tms = 1 ; tms <= 256 ; tms *= 2) {  \n \n      for (int ths = 32 ; ths <= 1024 ; ths *= 2) {  \n \n\tfor(int dssch = 128 ; dssch <= n ; dssch *= 1200) { \n\t  for(int sch = 100 ; sch <= n ; sch *= 3000) { \n\t    t++; \n#pragma omp target \n#pragma omp teams num_teams(tms) thread_limit(ths) \n\t    { \n#pragma omp distribute parallel for simd dist_schedule(static,dssch) schedule(static,sch) \n\t      for (int i = 0; i < n; ++i) { \n\t\ta[i] += b[i] + c[i]; \n\t      } \n\t    } \n\t  }  \n \n\t}  \n \n      }  \n \n    }", "pragma": "target ", "hash": "11c179ef24d03ce008c38b9056a49af297c4d099e8af7390788b29c591bcae7e"}
{"code": "for (int tms = 1 ; tms <= 256 ; tms *= 2) {  \n \n      for (int ths = 32 ; ths <= 1024 ; ths *= 2) {  \n \n\tfor(int dssch = 128 ; dssch <= n ; dssch *= 1200) { \n\t  for(int sch = 100 ; sch <= n ; sch *= 3000) { \n\t    t++; \n#pragma omp target teams distribute parallel for simd dist_schedule(static,dssch) schedule(static,sch) num_teams(tms) thread_limit(ths) \n\t      for (int i = 0; i < n; ++i) { \n\t\ta[i] += b[i] + c[i]; \n\t      } \n\t  }  \n \n\t}  \n \n      }  \n \n    }", "pragma": "target ", "hash": "aba4fbf1f1e117ce8488f7c1fedb433f65db6207b23a87ff749648b5f5e2792c"}
{"code": "for(i = 0; i < (int)training_instances_number; i++) \n    { \n        training_index = training_indices[i]; \n \n        inputs = data_set_pointer->get_instance(training_index, inputs_indices); \n \n        targets = data_set_pointer->get_instance(training_index, targets_indices); \n \n        first_order_forward_propagation = multilayer_perceptron_pointer->calculate_first_order_forward_propagation(inputs); \n \n        const Vector< Vector<double> >& layers_activation = first_order_forward_propagation[0]; \n        const Vector< Vector<double> >& layers_activation_derivative = first_order_forward_propagation[1]; \n \n        if(!has_conditions_layer) \n        { \n            const Vector<double>& outputs = first_order_forward_propagation[0][layers_number-1]; \n \n            term = (outputs-targets); \n \n            term_norm = term.calculate_norm(); \n \n            if(term_norm == 0.0) \n            { \n                output_gradient.set(outputs_number, 0.0); \n            } \n            else \n            { \n                output_gradient = term/term_norm; \n            } \n \n            layers_delta = calculate_layers_delta(layers_activation_derivative, output_gradient); \n        } \n        else \n        { \n            particular_solution = conditions_layer_pointer->calculate_particular_solution(inputs); \n            homogeneous_solution = conditions_layer_pointer->calculate_homogeneous_solution(inputs); \n \n            term = (particular_solution+homogeneous_solution*layers_activation[layers_number-1] - targets)/sqrt((double)training_instances_number); \n            term_norm = term.calculate_norm(); \n \n            if(term_norm == 0.0) \n            { \n                output_gradient.set(outputs_number, 0.0); \n            } \n            else \n            { \n                output_gradient = term/term_norm; \n            } \n \n            layers_delta = calculate_layers_delta(layers_activation_derivative, homogeneous_solution, output_gradient); \n        } \n \n        point_gradient = calculate_point_gradient(inputs, layers_activation, layers_delta); \n \n        terms_Jacobian.set_row(i, point_gradient); \n    }", "pragma": "parallel for private(                 first_order_forward_propagation homogeneous_solution i inputs layers_delta output_gradient particular_solution point_gradient targets term term_norm training_index)", "hash": "0bd87b8682a17ec253c16af33b4a14f8aaa89a495d9a35aeb3b450fc17b16901"}
{"code": "for(i = 0; i < (int)training_instances_number; i++) \n   { \n       training_index = training_indices[i]; \n \n      inputs = data_set_pointer->get_instance(training_index, inputs_indices); \n \n      targets = data_set_pointer->get_instance(training_index, targets_indices); \n \n      first_order_forward_propagation = multilayer_perceptron_pointer->calculate_first_order_forward_propagation(inputs); \n \n      const Vector< Vector<double> >& layers_activation = first_order_forward_propagation[0]; \n      const Vector< Vector<double> >& layers_activation_derivative = first_order_forward_propagation[1]; \n \n      if(!has_conditions_layer) \n      { \n         const Vector<double>& outputs = first_order_forward_propagation[0][layers_number-1];  \n \n         term = (outputs-targets); \n         term_norm = term.calculate_norm(); \n \n         if(term_norm == 0.0) \n         { \n             output_gradient.set(outputs_number, 0.0); \n         } \n         else \n         { \n            output_gradient = term/term_norm; \n         } \n \n         layers_delta = calculate_layers_delta(layers_activation_derivative, output_gradient); \n      } \n      else \n      { \n         particular_solution = conditions_layer_pointer->calculate_particular_solution(inputs); \n         homogeneous_solution = conditions_layer_pointer->calculate_homogeneous_solution(inputs); \n \n         term = (particular_solution+homogeneous_solution*layers_activation[layers_number-1] - targets)/sqrt((double)training_instances_number);               \n         term_norm = term.calculate_norm(); \n \n         if(term_norm == 0.0) \n         { \n             output_gradient.set(outputs_number, 0.0); \n         } \n         else \n         { \n            output_gradient = term/term_norm; \n         } \n \n         layers_delta = calculate_layers_delta(layers_activation_derivative, homogeneous_solution, output_gradient); \n\t  } \n \n      point_gradient = calculate_point_gradient(inputs, layers_activation, layers_delta); \n \n      terms_Jacobian.set_row(i, point_gradient); \n  }", "pragma": "parallel for private(              first_order_forward_propagation homogeneous_solution i inputs layers_delta output_gradient particular_solution point_gradient targets term term_norm training_index)", "hash": "c577f402d22dd0ad10447d2c2a8ebb36241a1ed6f5af8dc53cfdfae1591fc91c"}
{"code": "for(i = 0; i < (int)training_instances_number; i++) \n       { \n           training_index = training_indices[i]; \n \n           if(missing_values.has_missing_values(training_index)) \n           { \n               continue; \n           } \n \n          inputs = data_set_pointer->get_instance(training_index, inputs_indices); \n \n          targets = data_set_pointer->get_instance(training_index, targets_indices); \n \n          first_order_forward_propagation = multilayer_perceptron_pointer->calculate_first_order_forward_propagation(inputs); \n \n          const Vector< Vector<double> >& layers_activation = first_order_forward_propagation[0]; \n          const Vector< Vector<double> >& layers_activation_derivative = first_order_forward_propagation[1]; \n \n          if(!has_conditions_layer) \n          { \n             output_gradient = (layers_activation[layers_number-1]-targets)/(total_training_instances_number*loss); \n \n             layers_delta = calculate_layers_delta(layers_activation_derivative, output_gradient); \n          } \n          else \n          { \n             particular_solution = conditions_layer_pointer->calculate_particular_solution(inputs); \n             homogeneous_solution = conditions_layer_pointer->calculate_homogeneous_solution(inputs); \n \n             output_gradient = (particular_solution+homogeneous_solution*layers_activation[layers_number-1] - targets)/(total_training_instances_number*loss); \n \n             layers_delta = calculate_layers_delta(layers_activation_derivative, homogeneous_solution, output_gradient); \n          } \n \n          point_gradient = calculate_point_gradient(inputs, layers_activation, layers_delta); \n \n          #pragma omp critical \n \n          gradient += point_gradient; \n       }", "pragma": "parallel for private(                  first_order_forward_propagation homogeneous_solution i inputs layers_delta output_gradient particular_solution point_gradient targets training_index)", "hash": "dee83ac11344e685b17b9b7aa061185f7c902966c65a03d6f44de80c7e0f7f05"}
{"code": "for(i = 0; i < (int)training_instances_number; i++) \n       { \n           training_index = training_indices[i]; \n \n           if(missing_values.has_missing_values(training_index)) \n           { \n               continue; \n           } \n \n          inputs = data_set_pointer->get_instance(training_index, inputs_indices); \n \n          targets = data_set_pointer->get_instance(training_index, targets_indices); \n \n          first_order_forward_propagation = multilayer_perceptron_pointer->calculate_first_order_forward_propagation(inputs); \n \n          const Vector< Vector<double> >& layers_activation = first_order_forward_propagation[0]; \n          const Vector< Vector<double> >& layers_activation_derivative = first_order_forward_propagation[1]; \n \n          if(!has_conditions_layer) \n          { \n             output_gradient = (layers_activation[layers_number-1]-targets)/(training_instances_number*loss); \n \n             layers_delta = calculate_layers_delta(layers_activation_derivative, output_gradient); \n          } \n          else \n          { \n             particular_solution = conditions_layer_pointer->calculate_particular_solution(inputs); \n             homogeneous_solution = conditions_layer_pointer->calculate_homogeneous_solution(inputs); \n \n             output_gradient = (particular_solution+homogeneous_solution*layers_activation[layers_number-1] - targets)/(training_instances_number*loss); \n \n             layers_delta = calculate_layers_delta(layers_activation_derivative, homogeneous_solution, output_gradient); \n          } \n \n          point_gradient = calculate_point_gradient(inputs, layers_activation, layers_delta); \n \n          #pragma omp critical \n \n          gradient += point_gradient; \n       }", "pragma": "parallel for private(                  first_order_forward_propagation homogeneous_solution i inputs layers_delta output_gradient particular_solution point_gradient targets training_index)", "hash": "319766d612ec69737dbb56af62350f1c6b204fb8b3e9f33e8bd59a696a11f544"}
{"code": "for(i = 0; i < (int)training_instances_number; i++) \n    { \n        training_index = training_indices[i]; \n \n        inputs = data_set_pointer->get_instance(training_index, inputs_indices); \n \n        targets = data_set_pointer->get_instance(training_index, targets_indices); \n \n        first_order_forward_propagation = multilayer_perceptron_pointer->calculate_first_order_forward_propagation(inputs); \n \n        const Vector< Vector<double> >& layers_activation = first_order_forward_propagation[0]; \n        const Vector< Vector<double> >& layers_activation_derivative = first_order_forward_propagation[1]; \n \n        layers_inputs = multilayer_perceptron_pointer->arrange_layers_input(inputs, layers_activation); \n \n        layers_combination_parameters_Jacobian = multilayer_perceptron_pointer->calculate_layers_combination_parameters_Jacobian(layers_inputs); \n \n        if(!has_conditions_layer) \n        { \n            output_gradient = calculate_output_gradient_unnormalized(layers_activation[layers_number-1], targets); \n \n            layers_delta = calculate_layers_delta(layers_activation_derivative, output_gradient); \n        } \n        else \n        { \n            particular_solution = conditions_layer_pointer->calculate_particular_solution(inputs); \n            homogeneous_solution = conditions_layer_pointer->calculate_homogeneous_solution(inputs); \n \n            output_gradient = (particular_solution+homogeneous_solution*layers_activation[layers_number-1] - targets)*2.0; \n \n            layers_delta = calculate_layers_delta(layers_activation_derivative, homogeneous_solution, output_gradient); \n        } \n \n        point_gradient = calculate_point_gradient(layers_combination_parameters_Jacobian, layers_delta); \n \n#pragma omp critical \n        gradient += point_gradient; \n    }", "pragma": "parallel for private(               first_order_forward_propagation homogeneous_solution i inputs layers_combination_parameters_jacobian layers_delta layers_inputs output_gradient particular_solution point_gradient targets training_index)", "hash": "d1ce5134033a0f35decf06358118be6e26f96507812e6a8f56566ba6ab387d3d"}
{"code": "for(i = 0; i < (int)training_instances_number; i++) \n    { \n        training_index = training_indices[i]; \n \n       inputs = data_set_pointer->get_instance(training_index, inputs_indices); \n \n       targets = data_set_pointer->get_instance(training_index, targets_indices); \n \n       first_order_forward_propagation = multilayer_perceptron_pointer->calculate_first_order_forward_propagation(inputs); \n \n       const Vector< Vector<double> >& layers_activation = first_order_forward_propagation[0]; \n       const Vector< Vector<double> >& layers_activation_derivative = first_order_forward_propagation[1]; \n \n       layers_inputs = multilayer_perceptron_pointer->arrange_layers_input(inputs, layers_activation); \n \n       layers_combination_parameters_Jacobian = multilayer_perceptron_pointer->calculate_layers_combination_parameters_Jacobian(layers_inputs); \n \n       if(!has_conditions_layer) \n       { \n           output_gradient = calculate_output_gradient(layers_activation[layers_number-1], targets); \n \n           layers_delta = calculate_layers_delta(layers_activation_derivative, output_gradient); \n       } \n       else \n       { \n          particular_solution = conditions_layer_pointer->calculate_particular_solution(inputs); \n          homogeneous_solution = conditions_layer_pointer->calculate_homogeneous_solution(inputs); \n \n          output_gradient = (particular_solution+homogeneous_solution*layers_activation[layers_number-1] - targets)*2.0; \n \n          layers_delta = calculate_layers_delta(layers_activation_derivative, homogeneous_solution, output_gradient); \n       } \n \n       point_gradient = calculate_point_gradient(layers_combination_parameters_Jacobian, layers_delta); \n \n       #pragma omp critical \n       gradient += point_gradient; \n    }", "pragma": "parallel for private(                first_order_forward_propagation homogeneous_solution i inputs layers_combination_parameters_jacobian layers_delta layers_inputs output_gradient particular_solution point_gradient targets training_index)", "hash": "eca57b59ad2e10bdf01426aa0ce7de88ae09fd1b869a3f24ee21bf8a084c767f"}
{"code": "for(i = 0; i < (int)training_instances_number; i++) \n   { \n       training_index = training_indices[i]; \n \n      inputs = data_set_pointer->get_instance(training_index, inputs_indices); \n \n      targets = data_set_pointer->get_instance(training_index, targets_indices); \n \n      first_order_forward_propagation = multilayer_perceptron_pointer->calculate_first_order_forward_propagation(inputs); \n \n      const Vector< Vector<double> >& layers_activation = first_order_forward_propagation[0]; \n      const Vector< Vector<double> >& layers_activation_derivative = first_order_forward_propagation[1]; \n \n      layers_inputs = multilayer_perceptron_pointer->arrange_layers_input(inputs, layers_activation); \n \n      layers_combination_parameters_Jacobian = multilayer_perceptron_pointer->calculate_layers_combination_parameters_Jacobian(layers_inputs); \n \n      if(!has_conditions_layer) \n      { \n          output_gradient = calculate_output_gradient(layers_activation[layers_number-1], targets); \n \n          layers_delta = calculate_layers_delta(layers_activation_derivative, output_gradient); \n      } \n      else \n      { \n         particular_solution = conditions_layer_pointer->calculate_particular_solution(inputs); \n         homogeneous_solution = conditions_layer_pointer->calculate_homogeneous_solution(inputs); \n \n         output_gradient = (particular_solution+homogeneous_solution*layers_activation[layers_number-1] - targets)*2.0; \n \n         layers_delta = calculate_layers_delta(layers_activation_derivative, homogeneous_solution, output_gradient); \n      } \n \n      point_gradient = calculate_point_gradient(layers_combination_parameters_Jacobian, layers_delta); \n \n      #pragma omp critical \n      gradient += point_gradient; \n   }", "pragma": "parallel for private(               first_order_forward_propagation homogeneous_solution i inputs layers_combination_parameters_jacobian layers_delta layers_inputs output_gradient particular_solution point_gradient targets training_index)", "hash": "79ba3c627ecd1044ea7acb1a5eadc0945dcbca9a9903f51bfc2ae46e8b291b28"}
{"code": "for(i = 0; i < (int)training_instances_number; i++) \n   { \n       training_index = training_indices[i]; \n \n      inputs = data_set_pointer->get_instance(training_index, inputs_indices); \n \n      targets = data_set_pointer->get_instance(training_index, targets_indices); \n \n      first_order_forward_propagation = multilayer_perceptron_pointer->calculate_first_order_forward_propagation(inputs); \n \n      layers_inputs = multilayer_perceptron_pointer->arrange_layers_input(inputs, first_order_forward_propagation); \n \n       \n \n \n      layers_combination_parameters_Jacobian = multilayer_perceptron_pointer->calculate_layers_combination_parameters_Jacobian(layers_inputs); \n \n      if(!has_conditions_layer) \n      { \n          \n \n \n         term = first_order_forward_propagation[0][layers_number-1] - targets; \n         term_norm = term.calculate_norm(); \n \n         if(term_norm == 0.0) \n   \t     { \n             output_gradient.set(outputs_number, 0.0); \n\t     } \n         else \n\t     { \n            output_gradient = term/term_norm; \n\t     } \n \n         layers_delta = calculate_layers_delta(first_order_forward_propagation[1], output_gradient); \n      } \n      else \n      { \n \n         particular_solution = conditions_layer_pointer->calculate_particular_solution(inputs); \n         homogeneous_solution = conditions_layer_pointer->calculate_homogeneous_solution(inputs); \n \n          \n \n \n         term = (particular_solution+homogeneous_solution*first_order_forward_propagation[0][layers_number-1] - targets); \n         term_norm = term.calculate_norm(); \n \n         if(term_norm == 0.0) \n   \t     { \n             output_gradient.set(outputs_number, 0.0); \n         } \n\t     else \n\t     { \n            output_gradient = term/term_norm; \n\t     } \n \n         layers_delta = calculate_layers_delta(first_order_forward_propagation[1], homogeneous_solution, output_gradient); \n      } \n \n      point_gradient = calculate_point_gradient(layers_combination_parameters_Jacobian, layers_delta); \n \n      terms_Jacobian.set_row(i, point_gradient); \n  }", "pragma": "parallel for private(                  first_order_forward_propagation homogeneous_solution i inputs layers_combination_parameters_jacobian layers_delta layers_inputs output_gradient particular_solution point_gradient targets term term_norm training_index)", "hash": "40c961cbcac68e957a14f70a9426de4e5d54de624a942797d7e3b9e28b155009"}
{"code": "for(i = 0; i < (int)training_instances_number; i++) \n   { \n       training_index = training_indices[i]; \n \n       \n \n \n      inputs = data_set_pointer->get_instance(training_index, inputs_indices); \n \n       \n \n \n      outputs = multilayer_perceptron_pointer->calculate_outputs(inputs); \n \n       \n \n \n      targets = data_set_pointer->get_instance(training_index, targets_indices); \n \n       \n \n \n      error_terms[i] = outputs.calculate_distance(targets); \n   }", "pragma": "parallel for private(    i inputs outputs targets training_index)", "hash": "3a5e7204b01cdf98a53c6d0f6edc8630e4789ef8dade8b6081c5d2264c672b02"}
{"code": "for(i = 0; i < (int)training_instances_number; i++) \n   { \n       training_index = training_indices[i]; \n \n       \n \n \n      inputs = data_set_pointer->get_instance(training_index, inputs_indices); \n \n      targets = data_set_pointer->get_instance(training_index, targets_indices); \n \n       \n \n \n      first_order_forward_propagation = multilayer_perceptron_pointer->calculate_first_order_forward_propagation(inputs); \n \n      const Vector< Vector<double> >& layers_activation = first_order_forward_propagation[0]; \n      const Vector< Vector<double> >& layers_activation_derivative = first_order_forward_propagation[1]; \n \n      layers_inputs = multilayer_perceptron_pointer->arrange_layers_input(inputs, layers_activation); \n \n      layers_combination_parameters_Jacobian = multilayer_perceptron_pointer->calculate_layers_combination_parameters_Jacobian(layers_inputs); \n \n       \n \n \n      if(!has_conditions_layer) \n      { \n         output_gradient = (layers_activation[layers_number-1]-targets)*2.0; \n \n         layers_delta = calculate_layers_delta(layers_activation_derivative, output_gradient); \n      } \n      else \n      { \n         particular_solution = conditions_layer_pointer->calculate_particular_solution(inputs); \n         homogeneous_solution = conditions_layer_pointer->calculate_homogeneous_solution(inputs); \n \n         output_gradient = (particular_solution+homogeneous_solution*layers_activation[layers_number-1] - targets)*2.0; \n \n         layers_delta = calculate_layers_delta(layers_activation_derivative, homogeneous_solution, output_gradient); \n      } \n \n      point_gradient = calculate_point_gradient(layers_combination_parameters_Jacobian, layers_delta); \n \n      #pragma omp critical \n \n      gradient += point_gradient; \n \n      normalization_coefficient += targets.calculate_sum_squared_error(training_target_data_mean); \n   }", "pragma": "parallel for reduction(+ : normalization_coefficient) private(               first_order_forward_propagation homogeneous_solution i inputs layers_combination_parameters_jacobian layers_delta layers_inputs output_gradient particular_solution point_gradient targets training_index)", "hash": "c12a62efa9bef30da163aa92bf988a6a3cee416acbd423fefa4e7a06bd48e3b1"}
{"code": "for(i = 0; i < (int)training_instances_number; i++) \n   { \n       training_index = training_indices[i]; \n \n       \n \n \n      inputs = data_set_pointer->get_instance(training_index, inputs_indices); \n \n       \n \n \n      outputs = multilayer_perceptron_pointer->calculate_outputs(inputs); \n \n       \n \n \n      targets = data_set_pointer->get_instance(training_index, targets_indices); \n \n       \n \n \n      for(size_t j = 0; j < outputs_number; j++) \n      { \n          if(outputs[j] == 0.0) \n          { \n              outputs[j] = 1.0e-6; \n          } \n          else if(outputs[j] == 1.0) \n          { \n              outputs[j] = 0.999999; \n          } \n \n          cross_entropy_error -= (targets[j]*log(outputs[j]) + (1.0 - targets[j])*log(1.0 - outputs[j])); \n      } \n   }", "pragma": "parallel for reduction(+ : cross_entropy_error) private(    i inputs outputs targets training_index)", "hash": "23a272dfdf251bd2cecb676424ff3750638eb72bdde032b6f2bb2869cac8bb6d"}
{"code": "for(i = 0; i < (int)training_instances_number; i++) \n   { \n       training_index = training_indices[i]; \n \n       \n \n \n      inputs = data_set_pointer->get_instance(training_index, inputs_indices); \n \n       \n \n \n      outputs = multilayer_perceptron_pointer->calculate_outputs(inputs); \n \n       \n \n \n      targets = data_set_pointer->get_instance(training_index, targets_indices); \n \n       \n \n \n      for(size_t j = 0; j < outputs_number; j++) \n      { \n          if(outputs[j] == 0.0) \n          { \n              outputs[j] = 1.0e-6; \n          } \n          else if(outputs[j] == 1.0) \n          { \n              outputs[j] = 0.999999; \n          } \n \n          cross_entropy_error -= targets[j]*log(outputs[j]) + (1.0 - targets[j])*log(1.0 - outputs[j]); \n      } \n   }", "pragma": "parallel for reduction(+ : cross_entropy_error) private(    i inputs outputs targets training_index)", "hash": "fd5338b05ef9a5b532315b7b6b7930e28a5631df3b9b58d2e60fbdf4c28db142"}
{"code": "for(i = 0; i < (int)training_instances_number; i++) \n   { \n       training_index = training_indices[i]; \n \n       \n \n \n      inputs = data_set_pointer->get_instance(training_index, inputs_indices); \n \n       \n \n \n      outputs = multilayer_perceptron_pointer->calculate_outputs(inputs); \n \n       \n \n \n      targets = data_set_pointer->get_instance(training_index, targets_indices); \n \n       \n \n \n      for(size_t j = 0; j < outputs_number; j++) \n      { \n          if(outputs[j] == 0.0) \n          { \n              outputs[j] = 1.0e-6; \n          } \n          else if(outputs[j] == 1.0) \n          { \n              outputs[j] = 0.999999; \n          } \n \n          if(targets[j] == 0.0) \n          { \n              cross_entropy_error -= (1.0 - targets[j])*log(1.0 - outputs[j]); \n          } \n          else if(targets[j] == 1.0) \n          { \n              cross_entropy_error -= targets[j]*log(outputs[j]); \n \n          } \n      } \n   }", "pragma": "parallel for reduction(+ : cross_entropy_error) private(    i inputs outputs targets training_index)", "hash": "38bf9b01b35a5f4ce795bd019a4f5ab21895a68f45ab0986e10de2b5328906e3"}
{"code": "for(i = 0; i < (int)selection_instances_number; i++) \n    { \n        selection_index = selection_indices[i]; \n \n        \n \n \n       inputs = data_set_pointer->get_instance(selection_index, inputs_indices); \n \n        \n \n \n       outputs = multilayer_perceptron_pointer->calculate_outputs(inputs); \n \n        \n \n \n       targets = data_set_pointer->get_instance(selection_index, targets_indices); \n \n        \n \n \n       for(size_t j = 0; j < outputs_number; j++) \n       { \n           if(outputs[j] == 0.0) \n           { \n               outputs[j] = 1.0e-6; \n           } \n           else if(outputs[j] == 1.0) \n           { \n               outputs[j] = 0.999999; \n           } \n \n           if(targets[j] == 0.0) \n           { \n               targets[j] = 1.0e-6; \n           } \n           else if(targets[j] == 1.0) \n           { \n               targets[j] = 0.999999; \n           } \n \n           minimum_selection_loss -= (targets[j]*log(outputs[j]/targets[j]) + (1.0 - targets[j])*log((1.0 - outputs[j])/(1.0 - targets[j]))); \n       } \n    }", "pragma": "parallel for reduction(- : minimum_selection_loss) private(    i inputs outputs selection_index targets)", "hash": "170ef1cec2060e33cb906b6643f6081361dedbbc42c63ff6cf57d6c6b4d071fb"}
{"code": "for(i = 0; i < (int)selection_instances_number; i++) \n    { \n        selection_index = selection_indices[i]; \n \n        \n \n \n       inputs = data_set_pointer->get_instance(selection_index, inputs_indices); \n \n        \n \n \n       outputs = multilayer_perceptron_pointer->calculate_outputs(inputs); \n \n        \n \n \n       targets = data_set_pointer->get_instance(selection_index, targets_indices); \n \n        \n \n \n       for(size_t j = 0; j < outputs_number; j++) \n       { \n           if(outputs[j] == 0.0) \n           { \n               outputs[j] = 1.0e-6; \n           } \n           else if(outputs[j] == 1.0) \n           { \n               outputs[j] = 0.999999; \n           } \n \n           if(targets[j] == 0.0) \n           { \n               targets[j] = 1.0e-6; \n           } \n           else if(targets[j] == 1.0) \n           { \n               targets[j] = 0.999999; \n           } \n \n           minimum_selection_performance -= targets[j]*log(outputs[j]/targets[j]) + (1.0 - targets[j])*log((1.0 - outputs[j])/(1.0 - targets[j])); \n       } \n    }", "pragma": "parallel for reduction(- : minimum_selection_performance) private(    i inputs outputs selection_index targets)", "hash": "8de6c124ef260faaa16cbcf27d595a28b8363afb90454679120952c38ca0e0c1"}
{"code": "for(i = 0; i < (int)selection_instances_number; i++) \n   { \n       selection_index = selection_indices[i]; \n \n       \n \n \n      inputs = data_set_pointer->get_instance(selection_index, inputs_indices); \n \n       \n \n \n      outputs = multilayer_perceptron_pointer->calculate_outputs(inputs); \n \n       \n \n \n      targets = data_set_pointer->get_instance(selection_index, targets_indices); \n \n       \n \n \n      for(size_t j = 0; j < outputs_number; j++) \n      { \n          if(outputs[j] == 0.0) \n          { \n              outputs[j] = 1.0e-6; \n          } \n          else if(outputs[j] == 1.0) \n          { \n              outputs[j] = 0.999999; \n          } \n \n          selection_loss -= (targets[j]*log(outputs[j]) + (1.0 - targets[j])*log(1.0 - outputs[j])); \n      } \n   }", "pragma": "parallel for reduction(- : selection_loss) private(    i inputs outputs selection_index targets)", "hash": "24203ef36a52bc9367c8a3ed9d71ec6ca3f0970828337c3f826edca220e096fd"}
{"code": "for(i = 0; i < (int)selection_instances_number; i++) \n   { \n       selection_index = selection_indices[i]; \n \n       \n \n \n      inputs = data_set_pointer->get_instance(selection_index, inputs_indices); \n \n       \n \n \n      outputs = multilayer_perceptron_pointer->calculate_outputs(inputs); \n \n       \n \n \n      targets = data_set_pointer->get_instance(selection_index, targets_indices); \n \n       \n \n \n      for(size_t j = 0; j < outputs_number; j++) \n      { \n          if(outputs[j] == 0.0) \n          { \n              outputs[j] = 1.0e-6; \n          } \n          else if(outputs[j] == 1.0) \n          { \n              outputs[j] = 0.999999; \n          } \n \n          selection_performance -= targets[j]*log(outputs[j]) + (1.0 - targets[j])*log(1.0 - outputs[j]); \n      } \n   }", "pragma": "parallel for reduction(- : selection_performance) private(    i inputs outputs selection_index targets)", "hash": "b559c61bc2a9dfa21143e3c61ced4f14ebae88ca023b3473a4a5508b6127b0c0"}
{"code": "for(i = 0; i < (int)training_instances_number; i++) \n    { \n        training_index = training_indices[i]; \n \n         \n \n \n       inputs = data_set_pointer->get_instance(training_index, inputs_indices); \n \n        \n \n \n       outputs = multilayer_perceptron_pointer->calculate_outputs(inputs); \n \n        \n \n \n       targets = data_set_pointer->get_instance(training_index, targets_indices); \n \n        \n \n \n       for(size_t j = 0; j < outputs_number; j++) \n       { \n           if(outputs[j] == 0.0) \n           { \n               outputs[j] = 1.0e-6; \n           } \n           else if(outputs[j] == 1) \n           { \n               outputs[j] = 0.99999; \n           } \n \n           if(targets[j] == 0.0) \n           { \n               targets[j] = 1.0e-6; \n           } \n           else if(targets[j] == 1.0) \n           { \n               targets[j] = 0.999999; \n           } \n \n           minimum_cross_entropy_error -= (targets[j]*log(outputs[j]/targets[j]) + (1.0 - targets[j])*log((1.0 - outputs[j])/(1.0 - targets[j]))); \n       } \n    }", "pragma": "parallel for reduction(+ : minimum_cross_entropy_error) private(    i inputs outputs targets training_index)", "hash": "4d6bc8acd8e49d62e03971fb22661bd61a6283309bb02463611e1f321368318a"}
{"code": "for(i = 0; i < (int)training_instances_number; i++) \n    { \n        training_index = training_indices[i]; \n \n         \n \n \n       inputs = data_set_pointer->get_instance(training_index, inputs_indices); \n \n        \n \n \n       outputs = multilayer_perceptron_pointer->calculate_outputs(inputs); \n \n        \n \n \n       targets = data_set_pointer->get_instance(training_index, targets_indices); \n \n        \n \n \n       for(size_t j = 0; j < outputs_number; j++) \n       { \n           if(outputs[j] == 0.0) \n           { \n               outputs[j] = 1.0e-6; \n           } \n           else if(outputs[j] == 1) \n           { \n               outputs[j] = 0.99999; \n           } \n \n           if(targets[j] == 0.0) \n           { \n               targets[j] = 1.0e-6; \n           } \n           else if(targets[j] == 1.0) \n           { \n               targets[j] = 0.999999; \n           } \n \n           minimum_cross_entropy_error -= targets[j]*log(outputs[j]/targets[j]) + (1.0 - targets[j])*log((1.0 - outputs[j])/(1.0 - targets[j])); \n       } \n    }", "pragma": "parallel for reduction(+ : minimum_cross_entropy_error) private(    i inputs outputs targets training_index)", "hash": "665be350c0219d579123d50bd0fe696ebe2a344857ce6df436412dad9d57efa2"}
{"code": "for(i = 0; i < (int)training_instances_number; i++) \n    { \n        training_index = training_indices[i]; \n \n         \n \n \n        inputs = data_set_pointer->get_instance(training_index, inputs_indices); \n \n         \n \n \n        outputs = multilayer_perceptron_pointer->calculate_outputs(inputs); \n \n         \n \n \n        targets = data_set_pointer->get_instance(training_index, targets_indices); \n \n         \n \n \n        if(targets[0] == 1.0) \n        { \n            error = positives_w*outputs.calculate_sum_squared_error(targets); \n        } \n        else if(targets[0] == 0.0) \n        { \n            error = negatives_w*outputs.calculate_sum_squared_error(targets); \n        } \n        else \n        { \n            std::ostringstream buffer; \n \n            buffer << \"OpenNN Exception: WeightedSquaredError class.\\n\" \n                   << \"double calculate_error(void) const method.\\n\" \n                   << \"Target is neither a positive nor a negative.\\n\"; \n \n            throw std::logic_error(buffer.str()); \n        } \n \n        sum_squared_error += error; \n    }", "pragma": "parallel for reduction(+:sum_squared_error) private(     error i inputs outputs targets training_index)", "hash": "dca5581286141b63be0f73c3f83eabd175a84fb80dfcca76f6851cd935b44674"}
{"code": "for(i = 0; i < (int)training_instances_number; i++) \n    { \n        training_index = training_indices[i]; \n \n         \n \n \n        inputs = data_set_pointer->get_instance(training_index, inputs_indices); \n \n         \n \n \n        outputs = multilayer_perceptron_pointer->calculate_outputs(inputs); \n \n         \n \n \n        targets = data_set_pointer->get_instance(training_index, targets_indices); \n \n         \n \n \n        if(targets[0] == 1.0) \n        { \n            error_terms[i] = positives_w*outputs.calculate_distance(targets); \n        } \n        else if(targets[0] == 0.0) \n        { \n            error_terms[i] = negatives_w*outputs.calculate_distance(targets); \n        } \n        else \n        { \n            std::ostringstream buffer; \n \n            buffer << \"OpenNN Exception: WeightedSquaredError class.\\n\" \n                   << \"Vector<double> WeightedSquaredError::calculate_terms(void) const.\\n\" \n                   << \"Target is neither a positive nor a negative.\\n\"; \n \n            throw std::logic_error(buffer.str()); \n        } \n    }", "pragma": "parallel for private(    i inputs outputs targets training_index)", "hash": "2f8c10b9cf496e234b543336c5291341b4af0185368ac1e155620d60efde707f"}
{"code": "for(i = 0; i < (int)selection_instances_number; i++) \n    { \n        selection_index = selection_indices[i]; \n \n         \n \n \n        inputs = data_set_pointer->get_instance(selection_index, inputs_indices); \n \n         \n \n \n        outputs = multilayer_perceptron_pointer->calculate_outputs(inputs); \n \n         \n \n \n        targets = data_set_pointer->get_instance(selection_index, targets_indices); \n \n         \n \n \n        if(targets[0] == 1.0) \n        { \n            loss = positives_w*outputs.calculate_sum_squared_error(targets); \n        } \n        else if(targets[0] == 0.0) \n        { \n            loss = negatives_w*outputs.calculate_sum_squared_error(targets); \n        } \n        else \n        { \n            std::ostringstream buffer; \n \n            buffer << \"OpenNN Exception: WeightedSquaredError class.\\n\" \n                   << \"double calculate_error(const Vector<double>&) const method.\\n\" \n                   << \"Target is neither a positive nor a negative.\\n\"; \n \n            throw std::logic_error(buffer.str()); \n        } \n \n        selection_loss += loss; \n    }", "pragma": "parallel for reduction(+:selection_loss) private(     i inputs loss outputs selection_index targets)", "hash": "449817219f647245ee6694ec30325d2cce347e770fd00669880d0408dffb27e6"}
{"code": "for(i = 0; i < (int)selection_instances_number; i++) \n    { \n        selection_index = selection_indices[i]; \n \n         \n \n \n        inputs = data_set_pointer->get_instance(selection_index, inputs_indices); \n \n         \n \n \n        outputs = multilayer_perceptron_pointer->calculate_outputs(inputs); \n \n         \n \n \n        targets = data_set_pointer->get_instance(selection_index, targets_indices); \n \n         \n \n \n        if(targets[0] == 1.0) \n        { \n            performance = positives_w*outputs.calculate_sum_squared_error(targets); \n        } \n        else if(targets[0] == 0.0) \n        { \n            performance = negatives_w*outputs.calculate_sum_squared_error(targets); \n        } \n        else \n        { \n            std::ostringstream buffer; \n \n            buffer << \"OpenNN Exception: WeightedSquaredError class.\\n\" \n                   << \"double calculate_error(const Vector<double>&) const method.\\n\" \n                   << \"Target is neither a positive nor a negative.\\n\"; \n \n            throw std::logic_error(buffer.str()); \n        } \n \n        selection_performance += performance; \n    }", "pragma": "parallel for reduction(+:selection_performance) private(     i inputs outputs performance selection_index targets)", "hash": "3a7a4fecec856bb32d216c853cf808e1ecf9e68e42cbb97032f4c0198f2494b9"}
{"code": "for(i = 0; i < (int)training_instances_number; i++) \n    { \n        training_index = training_indices[i]; \n \n         \n \n \n        inputs = data_set_pointer->get_instance(training_index, inputs_indices); \n \n         \n \n \n        outputs = multilayer_perceptron_pointer->calculate_outputs(inputs); \n \n         \n \n \n        targets = data_set_pointer->get_instance(training_index, targets_indices); \n \n         \n \n \n        if(targets[0] == 1.0) \n        { \n            performance_terms[i] = positives_w*outputs.calculate_distance(targets); \n        } \n        else if(targets[0] == 0.0) \n        { \n            performance_terms[i] = negatives_w*outputs.calculate_distance(targets); \n        } \n        else \n        { \n            std::ostringstream buffer; \n \n            buffer << \"OpenNN Exception: WeightedSquaredError class.\\n\" \n                   << \"Vector<double> WeightedSquaredError::calculate_terms(void) const.\\n\" \n                   << \"Target is neither a positive nor a negative.\\n\"; \n \n            throw std::logic_error(buffer.str()); \n        } \n    }", "pragma": "parallel for private(    i inputs outputs targets training_index)", "hash": "2f337ddb1583693db7e174434b4644f1f01ab9783d8d3fc5e4b507b3afead3ca"}
{"code": "for(i = 0; i < (int)training_instances_number; i++) \n   {        \n       training_index = training_indices[i]; \n \n       \n \n \n      inputs = data_set_pointer->get_instance(training_index, inputs_indices); \n \n       \n \n \n      outputs = multilayer_perceptron_pointer->calculate_outputs(inputs); \n \n       \n \n \n      targets = data_set_pointer->get_instance(training_index, targets_indices); \n \n       \n \n \n      Minkowski_error += (outputs-targets).calculate_p_norm(Minkowski_parameter); \n   }", "pragma": "parallel for reduction(+ : minkowski_error) private(    i inputs outputs targets training_index)", "hash": "1e4b4361d1499aa946ef4a49d6382c1b2bf236e0b47f9e7d85bc42becc7a8a23"}
{"code": "for(i = 0; i < (int)training_instances_number; i++) \n    { \n        training_index = training_indices[i]; \n \n         \n \n \n        inputs = data_set_pointer->get_instance(training_index, inputs_indices); \n \n         \n \n \n        outputs = multilayer_perceptron_pointer->calculate_outputs(inputs); \n \n         \n \n \n        targets = data_set_pointer->get_instance(training_index, targets_indices); \n \n         \n \n \n        outputs[0] = 0.5; \n \n        if(targets[0] == 0.0) \n        { \n            sum_squared_error += negatives*outputs.calculate_sum_squared_error(targets); \n        } \n    }", "pragma": "parallel for reduction(+:sum_squared_error) private(     i inputs negatives outputs targets training_index)", "hash": "c2219456b34cd02de6e861a6a54d2eb03f8c58d70b2b94ec7d51c64a4313c94f"}
{"code": "for(i = 0; i < (int)training_instances_number; i++) \n    { \n        training_index = training_indices[i]; \n \n         \n \n \n        inputs = data_set_pointer->get_instance(training_index, inputs_indices); \n \n         \n \n \n        outputs = multilayer_perceptron_pointer->calculate_outputs(inputs); \n \n         \n \n \n        targets = data_set_pointer->get_instance(training_index, targets_indices); \n \n         \n \n \n        outputs[0] = 0.5; \n \n        if(targets[0] == 1.0) \n        { \n            sum_squared_error += positives*outputs.calculate_sum_squared_error(targets); \n        } \n    }", "pragma": "parallel for reduction(+:sum_squared_error) private(     i inputs outputs positives targets training_index)", "hash": "1f8270a96a52e6565f6c6cf5075315575ae25d72e2e5369ea1a9504054a3c3cc"}
{"code": "for(i = 0; i < (int)training_instances_number; i++) \n   { \n       training_index = training_indices[i]; \n \n       \n \n \n      inputs = data_set_pointer->get_instance(training_index, inputs_indices); \n \n       \n \n \n      targets = data_set_pointer->get_instance(training_index, targets_indices); \n \n       \n \n \n      outputs = multilayer_perceptron_pointer->calculate_outputs(inputs); \n \n       \n \n \n      sum_squared_error += outputs.calculate_sum_squared_error(targets); \n   }", "pragma": "parallel for reduction(+ : sum_squared_error) private(    i inputs outputs targets training_index)", "hash": "9020cff2893edaf0f33f41e2ee601f42e9263b605cb731ec71ea1b4d8756c5b5"}
{"code": "for(i = 0; i < (int)training_instances_number; i++) \n   { \n       training_index = training_indices[i]; \n \n       \n \n \n      inputs = data_set_pointer->get_instance(training_index, inputs_indices); \n \n       \n \n \n      outputs = multilayer_perceptron_pointer->calculate_outputs(inputs); \n \n       \n \n \n      targets = data_set_pointer->get_instance(training_index, targets_indices); \n \n       \n \n \n      performance_terms[i] = outputs.calculate_distance(targets); \n   }", "pragma": "parallel for private(    i inputs outputs targets training_index)", "hash": "32f943684dedfd2d959f6f1fc481a82a6d6c2f262abcc21146e3d7dd10db1ad6"}
{"code": "for(i = 0; i < (int)selection_instances_number; i++) \n      { \n          selection_index = selection_indices[i]; \n \n          \n \n \n         inputs = data_set_pointer->get_instance(selection_index, inputs_indices); \n \n          \n \n \n         outputs = multilayer_perceptron_pointer->calculate_outputs(inputs); \n \n          \n \n \n         targets = data_set_pointer->get_instance(selection_index, targets_indices); \n \n          \n \n \n         selection_loss += outputs.calculate_sum_squared_error(targets); \n      }", "pragma": "parallel for reduction(+:selection_loss) private(    i inputs outputs selection_index targets)", "hash": "5cbb3195353cb1e6a28683618125fc08a2b32599992737b80e59568801360aa4"}
{"code": "for(i = 0; i < (int)selection_instances_number; i++) \n   { \n       selection_index = selection_indices[i]; \n \n       \n \n \n      inputs = data_set_pointer->get_instance(selection_index, inputs_indices); \n \n       \n \n \n      outputs = multilayer_perceptron_pointer->calculate_outputs(inputs); \n \n       \n \n \n      targets = data_set_pointer->get_instance(selection_index, targets_indices); \n \n       \n \n \n      selection_loss += outputs.calculate_sum_squared_error(targets); \n   }", "pragma": "parallel for reduction(+ : selection_loss) private(    i inputs outputs selection_index targets)", "hash": "94dbc4bfbef9000226686f19f02188576244175823049f828dae18aa36d681b8"}
{"code": "for(i = 0; i < (int)selection_instances_number; i++) \n   { \n       selection_index = selection_indices[i]; \n \n       \n \n \n      inputs = data_set_pointer->get_instance(selection_index, inputs_indices); \n \n       \n \n \n      outputs = multilayer_perceptron_pointer->calculate_outputs(inputs); \n \n       \n \n \n      targets = data_set_pointer->get_instance(selection_index, targets_indices); \n \n       \n \n \n      selection_loss += (outputs-targets).calculate_p_norm(Minkowski_parameter); \n   }", "pragma": "parallel for reduction(+ : selection_loss) private(    i inputs outputs selection_index targets)", "hash": "edf10f63a95a8cf91d0bb54bde98c2a67fb0f389457fb7a554b6ed21ec0902a2"}
{"code": "for(i = 0; i < (int)selection_instances_number; i++) \n   { \n       selection_index = selection_indices[i]; \n \n       \n \n \n      inputs = data_set_pointer->get_instance(selection_index, inputs_indices); \n \n       \n \n \n      outputs = multilayer_perceptron_pointer->calculate_outputs(inputs); \n \n       \n \n \n      targets = data_set_pointer->get_instance(selection_index, targets_indices); \n \n       \n \n \n      selection_objective += outputs.calculate_sum_squared_error(targets);            \n   }", "pragma": "parallel for reduction(+ : selection_objective) private(    i inputs outputs selection_index targets)", "hash": "b2a56a73660143afb043938122017908300dee2cb3ce3d8d92a10deacd2d8d49"}
{"code": "for(i = 0; i < (int)selection_instances_number; i++) \n      { \n          selection_index = selection_indices[i]; \n \n          \n \n \n         inputs = data_set_pointer->get_instance(selection_index, inputs_indices); \n \n          \n \n \n         outputs = multilayer_perceptron_pointer->calculate_outputs(inputs); \n \n          \n \n \n         targets = data_set_pointer->get_instance(selection_index, targets_indices); \n \n          \n \n \n         selection_performance += outputs.calculate_sum_squared_error(targets); \n      }", "pragma": "parallel for reduction(+:selection_performance) private(    i inputs outputs selection_index targets)", "hash": "2ff0306e3ee9814bd2dc133f9d4826c7458597b9438d886b6691dff67c176a04"}
{"code": "for(i = 0; i < (int)selection_instances_number; i++) \n   { \n       selection_index = selection_indices[i]; \n \n       \n \n \n      inputs = data_set_pointer->get_instance(selection_index, inputs_indices); \n \n       \n \n \n      outputs = multilayer_perceptron_pointer->calculate_outputs(inputs); \n \n       \n \n \n      targets = data_set_pointer->get_instance(selection_index, targets_indices); \n \n       \n \n \n      selection_performance += outputs.calculate_sum_squared_error(targets); \n   }", "pragma": "parallel for reduction(+ : selection_performance) private(    i inputs outputs selection_index targets)", "hash": "2703157b4facf1e02d12b67bd6484a7b3cc1ec6f55f9a36ec779b8bb48071e69"}
{"code": "for(i = 0; i < (int)selection_instances_number; i++) \n   { \n       selection_index = selection_indices[i]; \n \n       \n \n \n      inputs = data_set_pointer->get_instance(selection_index, inputs_indices); \n \n       \n \n \n      outputs = multilayer_perceptron_pointer->calculate_outputs(inputs); \n \n       \n \n \n      targets = data_set_pointer->get_instance(selection_index, targets_indices); \n \n       \n \n \n      selection_performance += (outputs-targets).calculate_p_norm(Minkowski_parameter); \n   }", "pragma": "parallel for reduction(+ : selection_performance) private(    i inputs outputs selection_index targets)", "hash": "4c3cd5b17caf69d6075c0d4ef1f5ce215907e16bba8eb9167d7edf792e435a5a"}
{"code": "for(i = 0; i < (int)training_instances_number; i++) \n   { \n       training_index = training_indices[i]; \n \n        \n \n \n      inputs = data_set_pointer->get_instance(training_index, inputs_indices); \n \n       \n \n \n      outputs = multilayer_perceptron_pointer->calculate_outputs(inputs); \n \n       \n \n \n      targets = data_set_pointer->get_instance(training_index, targets_indices); \n \n       \n \n \n      squared_errors[i] = outputs.calculate_sum_squared_error(targets); \n   }", "pragma": "parallel for private(    i inputs outputs targets training_index)", "hash": "379d8bf38c3902fe2db396f03c78d1dc1bd1bb8bb9c2340bfe416b52c1244a5e"}
{"code": "for(i = 0; i < (int)selection_instances_number; i++) \n   { \n       selection_index = selection_indices[i]; \n \n       \n \n \n      inputs = data_set_pointer->get_instance(selection_index, inputs_indices); \n \n       \n \n \n      outputs = multilayer_perceptron_pointer->calculate_outputs(inputs); \n \n       \n \n \n      targets = data_set_pointer->get_instance(selection_index, targets_indices); \n \n       \n \n \n      sum_squared_error += outputs.calculate_sum_squared_error(targets); \n \n       \n \n \n      normalization_coefficient += targets.calculate_sum_squared_error(selection_target_data_mean); \n   }", "pragma": "parallel for reduction( + : normalization_coefficient sum_squared_error) private(    i inputs outputs selection_index targets)", "hash": "cc6fcbf135917724a0227503e706eb752eae332d8063ebaabbc34f4403ed85f9"}
{"code": "for(i = 0; i < (int)training_instances_number; i++) \n   { \n       training_index = training_indices[i]; \n \n       \n \n \n      inputs = data_set_pointer->get_instance(training_index, inputs_indices); \n \n       \n \n \n      outputs = multilayer_perceptron_pointer->calculate_outputs(inputs); \n \n       \n \n \n      targets = data_set_pointer->get_instance(training_index, targets_indices); \n \n       \n \n \n      sum_squared_error += outputs.calculate_sum_squared_error(targets); \n \n       \n \n \n      normalization_coefficient += targets.calculate_sum_squared_error(training_target_data_mean); \n   }", "pragma": "parallel for reduction( + : normalization_coefficient sum_squared_error) private(    i inputs outputs targets training_index)", "hash": "717238fa00de6bbdfd71204e64df3b4ee7a8e1312db1c5d83fa973dad1a517d2"}
{"code": "for(i = 0; i < (int)training_instances_number; i++) \n   { \n       training_index = training_indices[i]; \n \n       \n \n \n      inputs = data_set_pointer->get_instance(training_index, inputs_indices); \n \n       \n \n \n      outputs = multilayer_perceptron_pointer->calculate_outputs(inputs); \n \n       \n \n \n      targets = data_set_pointer->get_instance(training_index, targets_indices); \n \n       \n \n \n      sum_squared_error += outputs.calculate_sum_squared_error(targets); \n \n\t   \n \n \n\t  normalization_coefficient += targets.calculate_sum_squared_error(training_target_data_mean); \n   }", "pragma": "parallel for reduction( + : normalization_coefficient sum_squared_error) private(    i inputs outputs targets training_index)", "hash": "16614eefd35b2cad48879095139411c366cfbabe35c91283ebbe71cae53d02b2"}
{"code": "for(i = 0; i < (int)training_instances_number; i++) \n   { \n       training_index = training_indices[i]; \n \n        \n \n \n      inputs = data_set_pointer->get_instance(training_index, inputs_indices); \n \n       \n \n \n      outputs = multilayer_perceptron_pointer->calculate_outputs(inputs); \n \n       \n \n \n      targets = data_set_pointer->get_instance(training_index, targets_indices); \n \n       \n \n \n\t  error_terms[i] = outputs.calculate_distance(targets); \n \n\t   \n \n \n\t  normalization_coefficient += targets.calculate_sum_squared_error(training_target_data_mean); \n   }", "pragma": "parallel for reduction(+ : normalization_coefficient) private(    i inputs outputs targets training_index)", "hash": "17adf93b70084f182b1c0fadb319f14b50abf5e028a2be72793196081f6bab69"}
{"code": "for(i = 0; i < (int)training_instances_number; i++) \n   { \n       training_index = training_indices[i]; \n \n        \n \n \n      inputs = data_set_pointer->get_instance(training_index, inputs_indices); \n \n       \n \n \n      outputs = multilayer_perceptron_pointer->calculate_outputs(inputs); \n \n       \n \n \n      targets = data_set_pointer->get_instance(training_index, targets_indices); \n \n       \n \n \n\t  performance_terms[i] = outputs.calculate_distance(targets); \n \n\t   \n \n \n\t  normalization_coefficient += targets.calculate_sum_squared_error(training_target_data_mean); \n   }", "pragma": "parallel for reduction(+ : normalization_coefficient) private(    i inputs outputs targets training_index)", "hash": "ba794f708e309d5a38d93bc1e9690aa06707216daf4f8c4fff30db44710ffae0"}
{"code": "for(i = 0; i < (int)training_instances_number; i++) \n   { \n       training_index = training_indices[i]; \n \n       \n \n \n      inputs = data_set_pointer->get_instance(training_index, inputs_indices); \n \n       \n \n \n      outputs = multilayer_perceptron_pointer->calculate_outputs(inputs); \n \n       \n \n \n      targets = data_set_pointer->get_instance(training_index, targets_indices); \n \n       \n \n \n\t  sum_squared_error += outputs.calculate_sum_squared_error(targets); \n   }", "pragma": "parallel for reduction(+:sum_squared_error) private(    i inputs outputs targets training_index)", "hash": "ca63d216330fd7ffb1c5b67e2cba0ec4de55abd052991b9b7e3fd7f1893f35b2"}
{"code": "for(i = 0; i < (int)selection_instances_number; i++) \n   { \n       selection_index = selection_indices[i]; \n \n       \n \n \n      inputs = data_set_pointer->get_instance(selection_index, inputs_indices); \n \n       \n \n \n      outputs = multilayer_perceptron_pointer->calculate_outputs(inputs); \n \n       \n \n \n      targets = data_set_pointer->get_instance(selection_index, targets_indices); \n \n       \n \n \n\t  sum_squared_error += outputs.calculate_sum_squared_error(targets); \n \n\t   \n \n \n\t  normalization_coefficient += targets.calculate_sum_squared_error(selection_target_data_mean); \n   }", "pragma": "parallel for reduction( + : normalization_coefficient sum_squared_error) private(    i inputs outputs selection_index targets)", "hash": "b139b89f52b99e5e429de27841d25c668dab6f255ed70088eda460aec6f9e7ce"}
{"code": "for(i = 0; i < (int)training_instances_number; i++) \n   { \n       training_index = training_indices[i]; \n \n        \n \n \n      inputs = data_set_pointer->get_instance(training_index, inputs_indices); \n \n      targets = data_set_pointer->get_instance(training_index, targets_indices); \n \n\t   \n \n \n      first_order_forward_propagation = multilayer_perceptron_pointer->calculate_first_order_forward_propagation(inputs); \n \n      const Vector< Vector<double> >& layers_activation = first_order_forward_propagation[0]; \n      const Vector< Vector<double> >& layers_activation_derivative = first_order_forward_propagation[1]; \n \n      layers_inputs = multilayer_perceptron_pointer->arrange_layers_input(inputs, layers_activation); \n \n\t  layers_combination_parameters_Jacobian = multilayer_perceptron_pointer->calculate_layers_combination_parameters_Jacobian(layers_inputs); \n\t   \n\t   \n \n \n      if(!has_conditions_layer)  \n \n      { \n         const Vector<double>& outputs = layers_activation[layers_number-1];  \n \n         term = outputs-targets; \n         term_norm = term.calculate_norm(); \n \n         if(term_norm == 0.0) \n   \t     { \n            output_gradient.initialize(0.0); \n\t     } \n         else \n\t     { \n            output_gradient = term/term_norm; \n\t     } \n \n         layers_delta = calculate_layers_delta(layers_activation_derivative, output_gradient); \n      } \n      else  \n \n      {         \n         particular_solution = conditions_layer_pointer->calculate_particular_solution(inputs); \n         homogeneous_solution = conditions_layer_pointer->calculate_homogeneous_solution(inputs); \n \n         const Vector<double>& output_layer_activation = layers_activation[layers_number-1];  \n \n         term = (particular_solution+homogeneous_solution*output_layer_activation - targets);               \n         term_norm = term.calculate_norm(); \n \n         if(term_norm == 0.0) \n   \t     { \n            output_gradient.initialize(0.0); \n\t     } \n\t     else \n\t     { \n            output_gradient = term/term_norm; \n\t     } \n \n         layers_delta = calculate_layers_delta(layers_activation_derivative, homogeneous_solution, output_gradient); \n\t  } \n \n\t  normalization_coefficient += targets.calculate_sum_squared_error(training_target_data_mean); \n \n      point_gradient = calculate_point_gradient(layers_combination_parameters_Jacobian, layers_delta); \n \n      terms_Jacobian.set_row(i, point_gradient); \n \n  }", "pragma": "parallel for private(                  first_order_forward_propagation homogeneous_solution i inputs layers_combination_parameters_jacobian layers_delta layers_inputs output_gradient particular_solution point_gradient targets term term_norm training_index)", "hash": "db4eaaeb938a053c375628f9b168faad477712fcf5d6a0bcb2a804fb2c2bc033"}
{"code": "for(int i=0; i<N; i++) { \n    if (i==0) { \n      teams1 = omp_get_num_teams(); \n      threads1 = omp_get_num_threads(); \n      if (DEBUG) printf(\"  num teams %d, num thread %d\\n\", teams1, threads1); \n    } \n    A[i] = 2*i; \n  }", "pragma": "target teams distribute parallel for ", "hash": "a05ab1b86516ed0b80e9d4a2d1138f8f97f0264f7832a6596a2ef73df1e5fd17"}
{"code": "for(int i=0; i<N; i++) { \n    if (i==0) { \n      teams2 = omp_get_num_teams(); \n      threads2 = omp_get_num_threads(); \n      if (DEBUG) printf(\"  num teams %d, num thread %d\\n\", teams2, threads2); \n    } \n    A[i] += 2*i; \n  }", "pragma": "target teams distribute parallel for ", "hash": "30e5e916338df1dd36098d19a1a836230d2957a9be4c39f7b1fc9e37f302c54d"}
{"code": "for(int i=0; i<N; i++) { \n    if (i==0) { \n      teams3 = omp_get_num_teams(); \n      threads3 = omp_get_num_threads(); \n      if (DEBUG) printf(\"  num teams %d, num thread %d\\n\", teams3, threads3); \n    } \n    A[i] += 2*i; \n  }", "pragma": "target teams distribute parallel for ", "hash": "c6717d9e095a968b70f6106c1e2339d05e226d8b68412c4692262681b41da570"}
{"code": "for(i = 0; i < (int)training_instances_number; i++) \n   { \n       training_index = training_indices[i]; \n \n       \n \n \n      inputs = data_set_pointer->get_instance(training_index, inputs_indices); \n \n       \n \n \n      outputs = multilayer_perceptron_pointer->calculate_outputs(inputs); \n \n       \n \n \n \n \n \n       \n \n \n \n \n      sum_squared_error += outputs.calculate_sum_squared_error(data, training_index, targets_indices); \n \n   }", "pragma": "parallel for reduction(+:sum_squared_error) private(    i inputs outputs targets training_index)", "hash": "f14793b7a1853a4ef02d0f1b8eb663ab89b307106cc9ad3c5fea6c832575400e"}
